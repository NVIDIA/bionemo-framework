{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![ Click here to deploy.](https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdeploynavy.svg)](https://console.brev.dev/launchable/deploy?launchableID=env-2rPWpPzzJIxq7SMRJIQehCxBymV)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> It takes about 10 minutes to deploy this notebook as a Launchable. As of this writing, we are working on a free tier so a credit card may be required. You can reach out to your NVIDIA rep for credits. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESM-2 Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The [ESM-2](https://www.science.org/doi/abs/10.1126/science.ade2574) model is a transformer-based protein language model that has achieved state-of-the-art results in various protein-related tasks. When fine-tuning ESM2, the task-head plays a crucial role. A task head refers to the additional layer or set of layers added on top of a pre-trained model, like the ESM-2 transformer-based protein language model, to adapt it for a specific downstream task. As a part of transfer learning, a pre-trained model is often utilized to learn generic features from a large-scale dataset. However, these features might not be directly applicable to the specific task at hand. By incorporating a task head, which consists of learnable parameters, the model can adapt and specialize to the target task. The task head serves as a flexible and adaptable component that learns task-specific representations by leveraging the pre-trained features as a foundation. Through fine-tuning, the task head enables the model to learn and extract task-specific patterns, improving performance and addressing the nuances of the downstream task. It acts as a critical bridge between the pre-trained model and the specific task, enabling efficient and effective transfer of knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> We divided the fine-tuning use cases to *sequence-level* and *token-level* tasks where a target value is expected per each protein sequence and each token respectively. The first part of this tutorial will guide you through the steps for creating a sequence-level regression fine-tuning task for simplicity. The techniques demonstrated here can be adapted for sequence-level classification and token-level tasks.\n",
    "\n",
    "The utilities described in this tutorial are available in:\n",
    "\n",
    "<pre>bionemo.esm2.model.finetune</pre> \n",
    "\n",
    "In the second part of the tutorial, we will cover loading a pre-trained model, fine-tuning it sequence-level regression/classification and token-level classification, and using the fine-tuned models for inference. For instructions on pre-training the ESM-2 model, please refer to the [ESM-2 Pretraining](./pretrain.md) tutorial.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Regression Fine-tune Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define some key classes to successfully build a fine-tuning module in BioNeMo framework: \n",
    "\n",
    "1. **Loss Reduction Class** - To compute the supervised fine-tuning loss.\n",
    "2. **Fine-Tuned Model Head** - Downstream task head model.\n",
    "3. **Fine-Tuned Model** - Model that combines ESM-2 with the task head model.\n",
    "4. **Fine-Tuning Config** - Configures the fine-tuning model and loss to use in the training and inference framework.\n",
    "5. **Dataset** - Training and inference datasets for ESM2 fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Loss Reduction Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A class for calculating the supervised loss of the fine-tune model from targets. We inherit from Megatron Bert Masked Language Model Loss (`BERTMLMLossWithReduction`) and override the `forward()` pass to compute MSE loss of the regression head within a micro-batch. The `reduce()` method is used for computing the average over the micro-batches and is only used for logging.\n",
    "\n",
    "```python\n",
    "class RegressorLossReduction(BERTMLMLossWithReduction):\n",
    "    def forward(\n",
    "        self, batch: Dict[str, torch.Tensor], forward_out: Dict[str, torch.Tensor]\n",
    "    ) -> Tuple[torch.Tensor, Union[PerTokenLossDict, SameSizeLossDict]]:\n",
    "\n",
    "        regression_output = forward_out[\"regression_output\"]\n",
    "        targets = batch[\"labels\"].to(dtype=regression_output.dtype)  # [b, 1]\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(regression_output, targets)\n",
    "        return loss, {\"avg\": loss}\n",
    "\n",
    "    def reduce(self, losses_reduced_per_micro_batch: Sequence[ReductionT]) -> torch.Tensor:\n",
    "        losses = torch.stack([loss[\"avg\"] for loss in losses_reduced_per_micro_batch])\n",
    "        return losses.mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Fine-Tuned Model Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An MLP class for sequence-level regression. This class inherits `MegatronModule` and uses the fine-tune config (`TransformerConfig`) to configure the regression head for the fine-tuned ESM-2 model.\n",
    "\n",
    "```python\n",
    "class MegatronMLPHead(MegatronModule):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__(config)\n",
    "        layer_sizes = [config.hidden_size, 256, 1]\n",
    "        self.linear_layers = torch.nn.ModuleList(\n",
    "            [torch.nn.Linear(i, o) for i, o in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "        )\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(p=config.ft_dropout)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> List[torch.Tensor]:\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fine-tuned ESM-2 model class for token classification tasks. This class inherits from the `ESM2Model` class and adds the custom regression head `MegatronMLPHead` the we created in the previous step. Optionally one can freeze all or parts of the encoder by parsing through the model parameters in the model constructor.\n",
    "\n",
    "```python\n",
    "class ESM2FineTuneSeqModel(ESM2Model):\n",
    "    def __init__(self, config, *args, post_process: bool = True, include_embeddings: bool = False, **kwargs):\n",
    "        super().__init__(config, *args, post_process=post_process, include_embeddings=True, **kwargs)\n",
    "\n",
    "        # freeze encoder parameters\n",
    "        if config.encoder_frozen:\n",
    "            for _, param in self.named_parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if post_process:\n",
    "            self.regression_head = MegatronMLPHead(config)\n",
    "\n",
    "    def forward(self, *args, **kwargs,):\n",
    "        output = super().forward(*args, **kwargs)\n",
    "        ...\n",
    "        output[\"regression_output\"] = self.regression_head(embeddings)\n",
    "        return output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Fine-Tuning Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `dataclass` that configures the fine-tuned ESM-2 model. In this example `ESM2FineTuneSeqConfig` inherits from `ESM2GenericConfig` and adds custom arguments to setup the fine-tuned model. The `configure_model()` method of this `dataclass` is called within the `Lightning` module to call the model constructor with the `dataclass` arguments.\n",
    "\n",
    "The common arguments among different fine-tuning tasks are\n",
    "\n",
    "- `model_cls`: The fine-tune model class defined in previous step (`ESM2FineTuneSeqModel`)\n",
    "- `initial_ckpt_path`: BioNeMo 2.0 ESM-2 pre-trained checkpoint\n",
    "- `initial_ckpt_skip_keys_with_these_prefixes`: skips keys when loading parameters from a checkpoint. For example, we should not look for `regression_head` in the pre-trained checkpoint.\n",
    "- `get_loss_reduction_class()`: Implements selection of the appropriate `MegatronLossReduction` class that we defined in the first step of this tutorial.\n",
    "\n",
    "```python\n",
    "\n",
    "@dataclass\n",
    "class ESM2FineTuneSeqConfig(\n",
    "    ESM2GenericConfig[ESM2FineTuneSeqModel, RegressorLossReduction], iom.IOMixinWithGettersSetters\n",
    "):\n",
    "    model_cls: Type[ESM2FineTuneSeqModel] = ESM2FineTuneSeqModel\n",
    "    # The following checkpoint path is for nemo2 checkpoints. Config parameters not present in\n",
    "    # self.override_parent_fields will be loaded from the checkpoint and override those values here.\n",
    "    initial_ckpt_path: str | None = None\n",
    "    # typical case is fine-tune the base biobert that doesn't have this head. If you are instead loading a checkpoint\n",
    "    # that has this new head and want to keep using these weights, please drop this next line or set to []\n",
    "    initial_ckpt_skip_keys_with_these_prefixes: List[str] = field(default_factory=lambda: [\"regression_head\"])\n",
    "\n",
    "    encoder_frozen: bool = True  # freeze encoder parameters\n",
    "    ft_dropout: float = 0.25  # MLP layer dropout\n",
    "\n",
    "    def get_loss_reduction_class(self) -> Type[MegatronLossReduction]:\n",
    "        return RegressorLossReduction\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will use a sample dataset for demonstration purposes. Create a dataset class by extending ```bionemo.esm2.model.finetune.dataset.InMemoryProteinDataset```. The `InMemoryProteinDataset` has a `classmethod` (`from_csv`) that reads data from a CSV file that has `sequences` and optionally `labels` columns. It is important to override the `transform_label()` method that returns a `torch.Tensor` containing the label in correct format. As an example we can use this method to add custom tokenization if `label` is a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The custom dataset class will be appropriate (found in ```bionemo.esm2.model.finetune.dataset.InMemorySingleValueDataset```) as it facilitates predicting on a single value. An excerpt from the class is shown below. This example dataset has a class method `from_csv()` that expects a `data_path` to a CSV file that has `sequences`, and `labels` columns.\n",
    "\n",
    "```python\n",
    "class InMemorySingleValueDataset(InMemoryProteinDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        labels: pd.Series,\n",
    "        task_type: str = \"regression\",\n",
    "        tokenizer: tokenizer.BioNeMoESMTokenizer = tokenizer.get_tokenizer(),\n",
    "        seed: int = np.random.SeedSequence().entropy,\n",
    "    ):\n",
    "        super().__init__(sequences, labels, task_type, tokenizer, seed)\n",
    "\n",
    "    def transform_label(self, label: float) -> Tensor:\n",
    "        return torch.tensor([label], dtype=torch.float)\n",
    "```\n",
    "\n",
    "The `transform_label` method allows for custom transformation of raw labels by casting or tokenization and need to be adjusted based on the data. Here we use this method to create a `float` tensor of the regression value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To coordinate the creation of training, validation and testing datasets from your data, we need to use a `datamodule` class. To do this we can directly use or extend the ```ESM2FineTuneDataModule``` class (located at ```bionemo.esm2.model.finetune.datamodule.ESM2FineTuneDataModule```) which defines helpful abstract methods that use your dataset class.\n",
    "\n",
    "```python\n",
    "dataset = InMemorySingleValueDataset.from_csv(data_path)\n",
    "data_module = ESM2FineTuneDataModule(\n",
    "    train_dataset=dataset,\n",
    "    valid_dataset=dataset\n",
    "    micro_batch_size=4,   # size of a batch to be processed in a device\n",
    "    global_batch_size=8,  # size of batch across all devices. Should be multiple of micro_batch_size\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part of this tutorial we will prepare the input needed to run sequence-level regression/classification and token-level classification fine-tuning examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All commands should be executed inside the BioNeMo docker container, which has all ESM-2 dependencies pre-installed. For more information on how to build or pull the BioNeMo2 container, refer to the [Initialization Guide](../../getting-started/initialization-guide.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> Some of the cells below generate long text output. We're using <pre>%%capture --no-display --no-stderr cell_output</pre> to suppress this output. Comment or delete this line in the cells below to restore full output.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the work directory to store data and results:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> We set the following to clean up the work directory created by this notebook  <pre>cleanup : bool = True</pre></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/workspace/bionemo2/esm2_finetune_tutorial' created.\n"
     ]
    }
   ],
   "source": [
    "work_dir = \"/workspace/bionemo2/esm2_finetune_tutorial\"\n",
    "\n",
    "if cleanup and os.path.exists(work_dir):\n",
    "    shutil.rmtree(work_dir)\n",
    "\n",
    "if not os.path.exists(work_dir):\n",
    "    os.makedirs(work_dir)\n",
    "    print(f\"Directory '{work_dir}' created.\")\n",
    "else:\n",
    "    print(f\"Directory '{work_dir}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Pre-trained Model Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will download the internally pre-trained model, `esm2/8m:2.0`, from the NGC registry. Please refer to [ESM-2 Model Overview](../../../models/ESM-2/index.md) for a list of available checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.cache/bionemo/2957b2c36d5978d0f595d6f1b72104b312621cf0329209086537b613c1c96d16-esm2_hf_converted_8m_checkpoint.tar.gz.untar\n"
     ]
    }
   ],
   "source": [
    "from bionemo.core.data.load import load\n",
    "\n",
    "\n",
    "pretrain_checkpoint_path = load(\"esm2/8m:2.0\")\n",
    "print(pretrain_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example is downloading an internally trained 8M ESM-2 model. The pre-trained checkpoints can be downloaded from NGC resources using either the following bash command or the `load` function in `bionemo.core.data.load` as shown above.\n",
    "\n",
    "```bash\n",
    "download_bionemo_data esm2/650m:2.0\n",
    "```\n",
    "\n",
    "which returns the checkpoint path (e.g. `.../.cache/bionemo/975d29ee980fcb08c97401bbdfdcf8ce-esm2_650M_nemo2.tar.gz.untar`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take advantage of the ESM2 fine-tuning script in ```bionemo.esm2.scripts.finetune_esm2``` or use the ```finetune_esm2``` executable the fine-tuning process given:\n",
    "\n",
    "- Pre-trained checkpoint of ESM2\n",
    "- Finetune config class name that configures the finetune model and loss reduction\n",
    "- Path to train and validation CSV data files\n",
    "- Dataset class name\n",
    "\n",
    "To get the full list of arguments to tune a finetuning run use:\n",
    "```bash\n",
    "finetune_esm2 --help \n",
    "```\n",
    "\n",
    "For a detailed description of training loop and the arguments please refer to the [ESM-2 Pretraining](./pretrain.md) tutorial.\n",
    "\n",
    "#### Scaled LR for fine-tune head parameters \n",
    "We can assign a different LR for specific layers (e.g. task head) during fine-tuning by making it possible to specify the name of the target layer as well as the LR multiplier.\n",
    "\n",
    "- `--lr-multiplier`: is a float that scales `--lr`\n",
    "- `--sclae-lr-layer`: is the name of the layers for which we scale the LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b>\n",
    "\n",
    "Due to Megatron limitations, the log produced by the training run iterates on steps/iterations and not epochs. Therefore, `Training epoch` counter stays at value zero while `iteration` and `global_step` increase during the course of training (example in the following).\n",
    "\n",
    "<pre>\n",
    "Training epoch 0, iteration <x/max_steps> | ... | global_step: <x> | reduced_train_loss: ... | val_loss: ...\n",
    "</pre>\n",
    "\n",
    "to achieve the same epoch-based effect while training, please choose the number of training steps (`num_steps`) so that:\n",
    "\n",
    "<pre>\n",
    "num_steps * global_batch_size = len(dataset) * desired_num_epochs\n",
    "</pre></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence-level Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this demo, we'll assume dataset consists of small set of protein sequences with a target value of `len(sequence) / 100.0` as their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_sequence_data = [\n",
    "    \"TLILGWSDKLGSLLNQLAIANESLGGGTIAVMAERDKEDMELDIGKMEFDFKGTSVI\",\n",
    "    \"LYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n",
    "    \"GRFNVWLGGNESKIRQVLKAVKEIGVSPTLFAVYEKN\",\n",
    "    \"DELTALGGLLHDIGKPVQRAGLYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n",
    "    \"KLGSLLNQLAIANESLGGGTIAVMAERDKEDMELDIGKMEFDFKGTSVI\",\n",
    "    \"LFGAIGNAISAIHGQSAVEELVDAFVGGARISSAFPYSGDTYYLPKP\",\n",
    "    \"LGGLLHDIGKPVQRAGLYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n",
    "    \"LYSGDHSTQGARFLRDLAENTGRAEYELLSLF\",\n",
    "    \"ISAIHGQSAVEELVDAFVGGARISSAFPYSGDTYYLPKP\",\n",
    "    \"SGSKASSDSQDANQCCTSCEDNAPATSYCVECSEPLCETCVEAHQRVKYTKDHTVRSTGPAKT\",\n",
    "]\n",
    "\n",
    "data = [(seq, len(seq) / 100.0) for seq in artificial_sequence_data]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"sequences\", \"labels\"])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "data_path = os.path.join(work_dir, \"regression_data.csv\")\n",
    "df.to_csv(data_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the sequence-level fine-tune model config `ESM2FineTuneSeqConfig` and single-value dataset `InMemorySingleValueDataset` and set the task-type to `regression`. In addition to model and dataset configuration we can define the MLP task head and specify the number of hidden parameters (`mlp-hidden-size`), output layer size (`mlp-target-size`) and dropout (`mlp-ft-dropout`) in the following CLI call.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "\n",
    "! finetune_esm2 \\\n",
    "    --restore-from-checkpoint-path {pretrain_checkpoint_path} \\\n",
    "    --train-data-path {data_path} \\\n",
    "    --valid-data-path {data_path} \\\n",
    "    --config-class ESM2FineTuneSeqConfig \\\n",
    "    --dataset-class InMemorySingleValueDataset \\\n",
    "    --task-type \"regression\" \\\n",
    "    --mlp-ft-dropout 0.25 \\\n",
    "    --mlp-hidden-size 256 \\\n",
    "    --mlp-target-size 1 \\\n",
    "    --experiment-name \"sequence-level-regression\" \\\n",
    "    --num-steps 50 \\\n",
    "    --num-gpus 1 \\\n",
    "    --val-check-interval 10 \\\n",
    "    --log-every-n-steps 10 \\\n",
    "    --encoder-frozen \\\n",
    "    --lr 5e-3 \\\n",
    "    --lr-multiplier 1e2 \\\n",
    "    --scale-lr-layer \"regression_head\" \\\n",
    "    --result-dir {work_dir}  \\\n",
    "    --micro-batch-size 2 \\\n",
    "    --num-gpus 1 \\\n",
    "    --precision \"bf16-mixed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous cell executes the finetuning and saves the checkpoints at the end of the run. The checkpoint path is logged at the end of the finetuning log file: \n",
    "\n",
    "```\n",
    "[NeMo I $$$$-$$-$$ 22:04:28 nemo_logging:393] Async checkpoint save for step 50 (/workspace/bionemo2/esm2_finetune_tutorial/sequence-level-regression/dev/checkpoints/checkpoint-step=49-consumed_samples=100.0-last-v1.ckpt) finalized successfully.\n",
    "```\n",
    "\n",
    "To avoid long text output from the previous cell, the log is captured and stored into the `cell_output` variable. To visualize the log file uncomment and execute the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cell_output.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the checkpoint stored in the previous step to run inference. We will drop the `.ckpt` from the checkpoint path and provide that to the `--checkpoint-path` argument of `infer_esm2` executable.\n",
    "\n",
    "The input `--data-path` for inference is a CSV file with `sequences` column. It is also required to provide the appropriate `--config-class` name to load the model from the checkpoint. For a detailed description of inference arguments please refer to the [ESM-2 Inference](./inference.ipynb) tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "df = pd.DataFrame(artificial_sequence_data, columns=[\"sequences\"])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "data_path = os.path.join(work_dir, \"sequences.csv\")\n",
    "df.to_csv(data_path, index=False)\n",
    "\n",
    "checkpoint_path = (\n",
    "    f\"{work_dir}/sequence-level-regression/dev/checkpoints/checkpoint-step=49-consumed_samples=100.0-last\"\n",
    ")\n",
    "results_path = f\"{work_dir}/sequence-level-regression/infer/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "\n",
    "! infer_esm2 --checkpoint-path {checkpoint_path} \\\n",
    "             --config-class ESM2FineTuneSeqConfig \\\n",
    "             --data-path {data_path} \\\n",
    "             --results-path {results_path} \\\n",
    "             --micro-batch-size 3 \\\n",
    "             --num-gpus 1 \\\n",
    "             --precision \"bf16-mixed\" \\\n",
    "             --include-embeddings \\\n",
    "             --include-input-ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference results are written into a `.pt` file which can be loaded using PyTorch library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\ttorch.Size([10, 1024])\n",
      "embeddings\ttorch.Size([10, 320])\n",
      "regression_output\ttorch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "results = torch.load(f\"{results_path}/predictions__rank_0.pt\")\n",
    "\n",
    "for key, val in results.items():\n",
    "    if val is not None:\n",
    "        print(f\"{key}\\t{val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence-level Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for a sequence-level classification task we can create a dataset by labeling our sequences with arbitrary class names and take advantage of `Label2IDTokenizer` in the `transform_label()` method of `InMemorySingleValueDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [\n",
    "    \"E_class\",\n",
    "    \"C_class\",\n",
    "    \"H_class\",\n",
    "    \"H_class\",\n",
    "    \"C_class\",\n",
    "    \"H_class\",\n",
    "    \"H_class\",\n",
    "    \"C_class\",\n",
    "    \"H_class\",\n",
    "    \"C_class\",\n",
    "]\n",
    "\n",
    "data = [(seq, label) for seq, label in zip(artificial_sequence_data, class_labels)]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"sequences\", \"labels\"])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "data_path = os.path.join(work_dir, \"classification_data.csv\")\n",
    "df.to_csv(data_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this task is also a sequence-level fine-tuning, we will use `ESM2FineTuneSeqConfig` and single-value dataset `InMemorySingleValueDataset` but set the task-type to `classification`. For this classification task the MLP output layer size (`mlp-target-size`) should be set to number of classes in the dataset (3 in this example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "\n",
    "! finetune_esm2 \\\n",
    "    --restore-from-checkpoint-path {pretrain_checkpoint_path} \\\n",
    "    --train-data-path {data_path} \\\n",
    "    --valid-data-path {data_path} \\\n",
    "    --config-class ESM2FineTuneSeqConfig \\\n",
    "    --dataset-class InMemorySingleValueDataset \\\n",
    "    --task-type \"classification\" \\\n",
    "    --mlp-ft-dropout 0.25 \\\n",
    "    --mlp-hidden-size 256 \\\n",
    "    --mlp-target-size 3 \\\n",
    "    --experiment-name \"sequence-level-classification\" \\\n",
    "    --num-steps 50 \\\n",
    "    --num-gpus 1 \\\n",
    "    --val-check-interval 10 \\\n",
    "    --log-every-n-steps 10 \\\n",
    "    --encoder-frozen \\\n",
    "    --lr 5e-3 \\\n",
    "    --lr-multiplier 1e2 \\\n",
    "    --scale-lr-layer \"classification_head\" \\\n",
    "    --result-dir {work_dir}  \\\n",
    "    --micro-batch-size 2 \\\n",
    "    --num-gpus 1 \\\n",
    "    --precision \"bf16-mixed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toke-level Classification data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task we assign secondary structure label to each token in the sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_structure_labels = [\n",
    "    \"EEEECCCCCHHHHHHHHHHHHHHHCCCEEEEEECCCHHHHHHHHHCCCCCCCCCEEE\",\n",
    "    \"CCCCCHHHHHHHHHHHHHHCCCCCHHHHHHCC\",\n",
    "    \"HHHHHCCCCCHHHHHHHHHHHHHHCCCHHHHHHHHHH\",\n",
    "    \"HHHHHHHHHHCCCHHHHHCCCCCCCCHHHHHHHHHHHHHHCCCCCHHHHHHCC\",\n",
    "    \"CHHHHHHHHHHHHHHHCCCEEEEEECCCHHHHHHHHHCCCCCCCCCEEE\",\n",
    "    \"HHHHHHHHHHHHHCHHHHHHHHHHHHCCCEECCCEEEECCEEEEECC\",\n",
    "    \"HHHHHCCCHHHHHCCCCCCCCHHHHHHHHHHHHHHCCCCCHHHHHHCC\",\n",
    "    \"CCCCCHHHHHHHHHHHHHHCCCCCHHHHHHCC\",\n",
    "    \"HHHHHCHHHHHHHHHHHHCCCEECCCEEEECCEEEEECC\",\n",
    "    \"CCCCCCCCCCCCCCCCCCCCCCCCCCEEECCCCEEECHHHHHHHHHCCCCCCCCEEECCCCCC\",\n",
    "]\n",
    "\n",
    "data = [(seq, label) for (seq, label) in zip(artificial_sequence_data, secondary_structure_labels)]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"sequences\", \"labels\"])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "data_path = os.path.join(work_dir, \"token_classification_data.csv\")\n",
    "df.to_csv(data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "\n",
    "! finetune_esm2 \\\n",
    "    --restore-from-checkpoint-path {pretrain_checkpoint_path} \\\n",
    "    --train-data-path {data_path} \\\n",
    "    --valid-data-path {data_path} \\\n",
    "    --config-class ESM2FineTuneTokenConfig \\\n",
    "    --dataset-class InMemoryPerTokenValueDataset \\\n",
    "    --task-type \"classification\" \\\n",
    "    --cnn-dropout 0.25 \\\n",
    "    --cnn-hidden-size 32 \\\n",
    "    --cnn-num-classes 3 \\\n",
    "    --experiment-name \"token-level-classification\" \\\n",
    "    --num-steps 50 \\\n",
    "    --num-gpus 1 \\\n",
    "    --val-check-interval 10 \\\n",
    "    --log-every-n-steps 10 \\\n",
    "    --encoder-frozen \\\n",
    "    --lr 5e-3 \\\n",
    "    --lr-multiplier 1e2 \\\n",
    "    --scale-lr-layer \"classification_head\" \\\n",
    "    --result-dir {work_dir}  \\\n",
    "    --micro-batch-size 2 \\\n",
    "    --num-gpus 1 \\\n",
    "    --precision \"bf16-mixed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous cell executes the finetuning and saves the checkpoints at the end of the run. The checkpoint path is logged at the end of the finetuning log file: \n",
    "\n",
    "```\n",
    "[NeMo I $$$$-$$-$$ 22:16:46 nemo_logging:393] Async checkpoint save for step 50 (/workspace/bionemo2/esm2_finetune_tutorial/token-level-classification/dev/checkpoints/checkpoint-step=49-consumed_samples=100.0-last.ckpt) finalized successfully.\n",
    "```\n",
    "\n",
    "To avoid long text output from the previous cell, the log is captured and stored into the `cell_output` variable. To visualize the log file uncomment and execute the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cell_output.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the checkpoint stored in the previous step to run inference. We will drop the `.ckpt` from the checkpoint path and provide that to the `--checkpoint-path` argument of `infer_esm2` executable.\n",
    "\n",
    "The input `--data-path` for inference is a CSV file with `sequences` column. It is also required to provide the appropriate `--config-class` name to load the model from the checkpoint. For a detailed description of inference arguments please refer to the [ESM-2 Inference](./inference.ipynb) tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "df = pd.DataFrame(artificial_sequence_data, columns=[\"sequences\"])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "data_path = os.path.join(work_dir, \"sequences.csv\")\n",
    "df.to_csv(data_path, index=False)\n",
    "\n",
    "checkpoint_path = (\n",
    "    f\"{work_dir}/token-level-classification/dev/checkpoints/checkpoint-step=49-consumed_samples=100.0-last\"\n",
    ")\n",
    "results_path = f\"{work_dir}/token-level-classification/infer/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "! infer_esm2 --checkpoint-path {checkpoint_path} \\\n",
    "             --config-class ESM2FineTuneTokenConfig \\\n",
    "             --data-path {data_path} \\\n",
    "             --results-path {results_path} \\\n",
    "             --micro-batch-size 3 \\\n",
    "             --num-gpus 1 \\\n",
    "             --precision \"bf16-mixed\" \\\n",
    "             --include-embeddings \\\n",
    "             --include-hiddens \\\n",
    "             --include-input-ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cell_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference results are written into a `.pt` file which can be loaded using PyTorch library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_states\ttorch.Size([10, 1024, 320])\n",
      "input_ids\ttorch.Size([10, 1024])\n",
      "embeddings\ttorch.Size([10, 320])\n",
      "classification_output\ttorch.Size([10, 1024, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "results = torch.load(f\"{results_path}/predictions__rank_0.pt\")\n",
    "\n",
    "for key, val in results.items():\n",
    "    if val is not None:\n",
    "        print(f\"{key}\\t{val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the label tokenizer to convert the classification output to class names. Note that for demonstration purposes we are using a small dataset of artificial sequences in this example. You may experience over-fitting and observe no change in the validation metrics. This amount of data and the short training run does not result in accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bionemo.esm2.data.tokenizer import get_tokenizer\n",
    "\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "tokens = tokenizer.all_tokens\n",
    "aa_tokens = [\"L\", \"A\", \"G\", \"V\", \"S\", \"E\", \"R\", \"T\", \"I\", \"D\", \"P\", \"K\", \"Q\", \"N\", \"F\", \"Y\", \"M\", \"H\", \"W\", \"C\"]\n",
    "aa_indices = [i for i, token in enumerate(tokens) if token in aa_tokens]\n",
    "extra_indices = [i for i, token in enumerate(tokens) if token not in aa_tokens]\n",
    "\n",
    "input_ids = results[\"input_ids\"]  # b, s\n",
    "# mask where non-amino acid tokens are True\n",
    "mask = ~torch.isin(input_ids, torch.tensor(extra_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bionemo.llm.data.label2id_tokenizer import Label2IDTokenizer\n",
    "\n",
    "\n",
    "label_tokenizer = Label2IDTokenizer()\n",
    "label_tokenizer = label_tokenizer.build_vocab(secondary_structure_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Secondary Structures:\n",
      "EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
      "EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
      "EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
      "EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
      "EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
      "EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
      "EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
      "EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
      "EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
      "EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n"
     ]
    }
   ],
   "source": [
    "output_ids = torch.argmax(results[\"classification_output\"], dim=-1)\n",
    "\n",
    "print(\"Predicted Secondary Structures:\")\n",
    "for i in range(output_ids.shape[0]):\n",
    "    ss_ids = output_ids[i][mask[i]]\n",
    "    print(label_tokenizer.ids_to_text(ss_ids.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning with LoRA\n",
    "Finte with LoRA is supported. In this regime, the encoder and the embedding layers are frozen, and LoRA weights are added to those layers. The classification and regression heads are not frozen. LoRA fine-tuning is supported for any of the classification types above. The outputted weights in the results directory only contain the LoRA weights and the classification and regression heads. For further inference and training, both the original model weights and fine-tuned weights are necessary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO     | pytorch_lightning.utilities.rank_zero]: Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: GPU available: True (cuda), used: True\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: TPU available: False, using: 0 TPU cores\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2025-04-25 04:50:48 nemo_logging:405] WandB is currently turned off.\n",
      "[NeMo W 2025-04-25 04:50:48 nemo_logging:405] User-set tensorboard is currently turned off. Internally one may still be set by NeMo2.\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Experiments will be logged at /workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev\n",
      "[NeMo W 2025-04-25 04:50:48 nemo_logging:405] The Trainer already contains a ModelCheckpoint callback. This will be overwritten.\n",
      "[NeMo W 2025-04-25 04:50:48 nemo_logging:405] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 50. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] All embedding group ranks: [[0]]\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Rank 0 has embedding rank: 0\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: ----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Setting up ModelTransform for stage: TrainerFn.FITTING\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Found model_transform attribute on pl_module\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Set model_transform to: <function _call_counter.<locals>.wrapper at 0x770fd71d2840>\n",
      "[WARNING  | /workspaces/bionemo-framework/sub-packages/bionemo-llm/src/bionemo/llm/model/config.py]: Loading /home/ubuntu/.cache/bionemo/2957b2c36d5978d0f595d6f1b72104b312621cf0329209086537b613c1c96d16-esm2_hf_converted_8m_checkpoint.tar.gz.untar\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Padded vocab_size: 128, original vocab_size: 33, dummy tokens: 95.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo W 2025-04-25 04:50:48 nemo_logging:405] Could not copy Trainer's 'max_steps' to LR scheduler's 'max_steps'. If you are not using an LR scheduler, this warning can safely be ignored.\n",
      "[NeMo I 2025-04-25 04:50:48 num_microbatches_calculator:228] setting number of microbatches to constant 1\n",
      "┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
      "┃\u001b[1;35m \u001b[0m\u001b[1;35m  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName                                       \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
      "┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
      "│\u001b[2m \u001b[0m\u001b[2m0 \u001b[0m\u001b[2m \u001b[0m│ valid_metric                                │ Multicl… │      0 │ train │\n",
      "│\u001b[2m \u001b[0m\u001b[2m1 \u001b[0m\u001b[2m \u001b[0m│ module                                      │ ESM2Fin… │  7.6 M │ train │\n",
      "│\u001b[2m \u001b[0m\u001b[2m2 \u001b[0m\u001b[2m \u001b[0m│ module.embedding                            │ ESM2Emb… │ 41.0 K │ train │\n",
      "│\u001b[2m \u001b[0m\u001b[2m3 \u001b[0m\u001b[2m \u001b[0m│ module.embedding.word_embeddings            │ VocabPa… │ 41.0 K │ train │\n",
      "│\u001b[2m \u001b[0m\u001b[2m4 \u001b[0m\u001b[2m \u001b[0m│ module.embedding.embedding_dropout          │ Dropout  │      0 │ train │\n",
      "│\u001b[2m \u001b[0m\u001b[2m5 \u001b[0m\u001b[2m \u001b[0m│ module.rotary_pos_emb                       │ RotaryE… │      0 │ train │\n",
      "│\u001b[2m \u001b[0m\u001b[2m6 \u001b[0m\u001b[2m \u001b[0m│ module.encoder                              │ Transfo… │  7.4 M │ train │\n",
      "│\u001b[2m \u001b[0m\u001b[2m7 \u001b[0m\u001b[2m \u001b[0m│ module.encoder.layers                       │ ModuleL… │  7.4 M │ train │\n",
      "│\u001b[2m \u001b[0m\u001b[2m8 \u001b[0m\u001b[2m \u001b[0m│ module.encoder.layers.0                     │ Transfo… │  1.2 M │ train │\n",
      "│\u001b[2m \u001b[0m\u001b[2m9 \u001b[0m\u001b[2m \u001b[0m│ module.encoder.layers.1                     │ Transfo… │  1.2 M │ train │\n",
      "│\u001b[2m \u001b[0m\u001b[2m10\u001b[0m\u001b[2m \u001b[0m│ module.encoder.layers.2                     │ Transfo… │  1.2 M │ train │\n",
      "│\u001b[2m \u001b[0m\u001b[2m11\u001b[0m\u001b[2m \u001b[0m│ module.encoder.layers.3                     │ Transfo… │  1.2 M │ train │\n",
      "│\u001b[2m \u001b[0m\u001b[2m12\u001b[0m\u001b[2m \u001b[0m│ module.encoder.layers.4                     │ Transfo… │  1.2 M │ train │\n",
      "│\u001b[2m \u001b[0m\u001b[2m13\u001b[0m\u001b[2m \u001b[0m│ module.encoder.layers.5                     │ Transfo… │  1.2 M │ train │\n",
      "│\u001b[2m \u001b[0m\u001b[2m14\u001b[0m\u001b[2m \u001b[0m│ module.encoder.final_layernorm              │ LayerNo… │    640 │ train │\n",
      "│\u001b[2m \u001b[0m\u001b[2m15\u001b[0m\u001b[2m \u001b[0m│ module.lm_head                              │ BertLMH… │  103 K │ train │\n",
      "│\u001b[2m \u001b[0m\u001b[2m16\u001b[0m\u001b[2m \u001b[0m│ module.lm_head.dense                        │ Linear   │  102 K │ train │\n",
      "│\u001b[2m \u001b[0m\u001b[2m17\u001b[0m\u001b[2m \u001b[0m│ module.lm_head.layer_norm                   │ FusedLa… │    640 │ train │\n",
      "│\u001b[2m \u001b[0m\u001b[2m18\u001b[0m\u001b[2m \u001b[0m│ module.output_layer                         │ ColumnP… │    128 │ train │\n",
      "│\u001b[2m \u001b[0m\u001b[2m19\u001b[0m\u001b[2m \u001b[0m│ module.classification_head                  │ Megatro… │ 72.4 K │ train │\n",
      "│\u001b[2m \u001b[0m\u001b[2m20\u001b[0m\u001b[2m \u001b[0m│ module.classification_head.finetune_model   │ Sequent… │ 71.7 K │ train │\n",
      "│\u001b[2m \u001b[0m\u001b[2m21\u001b[0m\u001b[2m \u001b[0m│ module.classification_head.finetune_model.0 │ Conv2d   │ 71.7 K │ train │\n",
      "│\u001b[2m \u001b[0m\u001b[2m22\u001b[0m\u001b[2m \u001b[0m│ module.classification_head.finetune_model.1 │ ReLU     │      0 │ train │\n",
      "│\u001b[2m \u001b[0m\u001b[2m23\u001b[0m\u001b[2m \u001b[0m│ module.classification_head.finetune_model.2 │ Dropout  │      0 │ train │\n",
      "│\u001b[2m \u001b[0m\u001b[2m24\u001b[0m\u001b[2m \u001b[0m│ module.classification_head.class_heads      │ Conv2d   │    675 │ train │\n",
      "└────┴─────────────────────────────────────────────┴──────────┴────────┴───────┘\n",
      "\u001b[1mTrainable params\u001b[0m: 72.4 K                                                        \n",
      "\u001b[1mNon-trainable params\u001b[0m: 7.5 M                                                     \n",
      "\u001b[1mTotal params\u001b[0m: 7.6 M                                                             \n",
      "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 30                                      \n",
      "\u001b[1mModules in train mode\u001b[0m: 139                                                      \n",
      "\u001b[1mModules in eval mode\u001b[0m: 0                                                         \n",
      "[NeMo W 2025-04-25 04:50:48 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Adding lora to: 0.module.encoder.layers.0.self_attention.linear_proj\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Adding lora to: 0.module.encoder.layers.0.self_attention.linear_qkv\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Adding lora to: 0.module.encoder.layers.0.mlp.linear_fc1\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Adding lora to: 0.module.encoder.layers.0.mlp.linear_fc2\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Adding lora to: 0.module.encoder.layers.1.self_attention.linear_proj\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Adding lora to: 0.module.encoder.layers.1.self_attention.linear_qkv\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Adding lora to: 0.module.encoder.layers.1.mlp.linear_fc1\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Adding lora to: 0.module.encoder.layers.1.mlp.linear_fc2\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Adding lora to: 0.module.encoder.layers.2.self_attention.linear_proj\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Adding lora to: 0.module.encoder.layers.2.self_attention.linear_qkv\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Adding lora to: 0.module.encoder.layers.2.mlp.linear_fc1\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Adding lora to: 0.module.encoder.layers.2.mlp.linear_fc2\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Adding lora to: 0.module.encoder.layers.3.self_attention.linear_proj\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Adding lora to: 0.module.encoder.layers.3.self_attention.linear_qkv\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Adding lora to: 0.module.encoder.layers.3.mlp.linear_fc1\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Adding lora to: 0.module.encoder.layers.3.mlp.linear_fc2\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Adding lora to: 0.module.encoder.layers.4.self_attention.linear_proj\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Adding lora to: 0.module.encoder.layers.4.self_attention.linear_qkv\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Adding lora to: 0.module.encoder.layers.4.mlp.linear_fc1\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Adding lora to: 0.module.encoder.layers.4.mlp.linear_fc2\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Adding lora to: 0.module.encoder.layers.5.self_attention.linear_proj\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Adding lora to: 0.module.encoder.layers.5.self_attention.linear_qkv\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Adding lora to: 0.module.encoder.layers.5.mlp.linear_fc1\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Adding lora to: 0.module.encoder.layers.5.mlp.linear_fc2\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] After applying model_transform:\n",
      "    \n",
      "  | Name         | Type                   | Params | Mode\n",
      "---------------------------------------------------------------\n",
      "0 | valid_metric | MulticlassAccuracy     | 0      | eval\n",
      "1 | module       | ESM2FineTuneTokenModel | 8.6 M  | eval\n",
      "---------------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "7.5 M     Non-trainable params\n",
      "8.6 M     Total params\n",
      "34.393    Total estimated model params size (MB)\n",
      "122       Modules in train mode\n",
      "137       Modules in eval mode\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Initializing model parallel\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0 ,0): 8598275\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393]  > number of trainable parameters: 1055427 (12.27% of total)\n",
      "[NeMo I 2025-04-25 04:50:48 utils:302] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=False, overlap_grad_reduce=False, overlap_param_gather=True, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=True, check_for_large_grads=False, bucket_size=None, average_in_collective=True, fp8_param_gather=False)\n",
      "[NeMo I 2025-04-25 04:50:48 utils:323] Number of buckets for gradient all-reduce / reduce-scatter: 1\n",
      "    Params for bucket 1 (1055427 elements):\n",
      "    \tmodule.encoder.layers.5.self_attention.linear_proj.adapter.linear_in.weight\n",
      "    \tmodule.encoder.layers.3.mlp.linear_fc1.adapter.linear_out.weight\n",
      "    \tmodule.encoder.layers.3.mlp.linear_fc1.adapter.linear_in.weight\n",
      "    \tmodule.encoder.layers.1.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "    \tmodule.encoder.layers.0.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "    \tmodule.encoder.layers.0.self_attention.linear_proj.adapter.linear_in.weight\n",
      "    \tmodule.classification_head.class_heads.bias\n",
      "    \tmodule.encoder.layers.5.mlp.linear_fc2.adapter.linear_in.weight\n",
      "    \tmodule.encoder.layers.4.mlp.linear_fc1.adapter.linear_out.weight\n",
      "    \tmodule.encoder.layers.3.self_attention.linear_proj.adapter.linear_out.weight\n",
      "    \tmodule.encoder.layers.1.mlp.linear_fc1.adapter.linear_in.weight\n",
      "    \tmodule.encoder.layers.0.mlp.linear_fc2.adapter.linear_out.weight\n",
      "    \tmodule.encoder.layers.0.mlp.linear_fc1.adapter.linear_in.weight\n",
      "    \tmodule.encoder.layers.0.self_attention.linear_proj.adapter.linear_out.weight\n",
      "    \tmodule.encoder.layers.5.mlp.linear_fc1.adapter.linear_out.weight\n",
      "    \tmodule.encoder.layers.5.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "    \tmodule.encoder.layers.2.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "    \tmodule.encoder.layers.1.mlp.linear_fc2.adapter.linear_out.weight\n",
      "    \tmodule.encoder.layers.4.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "    \tmodule.encoder.layers.5.mlp.linear_fc2.adapter.linear_out.weight\n",
      "    \tmodule.classification_head.finetune_model.0.bias\n",
      "    \tmodule.encoder.layers.4.self_attention.linear_proj.adapter.linear_in.weight\n",
      "    \tmodule.encoder.layers.3.mlp.linear_fc2.adapter.linear_out.weight\n",
      "    \tmodule.encoder.layers.2.mlp.linear_fc2.adapter.linear_out.weight\n",
      "    \tmodule.encoder.layers.2.self_attention.linear_proj.adapter.linear_out.weight\n",
      "    \tmodule.encoder.layers.1.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "    \tmodule.encoder.layers.1.self_attention.linear_proj.adapter.linear_in.weight\n",
      "    \tmodule.classification_head.class_heads.weight\n",
      "    \tmodule.encoder.layers.4.mlp.linear_fc1.adapter.linear_in.weight\n",
      "    \tmodule.encoder.layers.4.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "    \tmodule.encoder.layers.3.mlp.linear_fc2.adapter.linear_in.weight\n",
      "    \tmodule.encoder.layers.2.mlp.linear_fc1.adapter.linear_in.weight\n",
      "    \tmodule.encoder.layers.1.self_attention.linear_proj.adapter.linear_out.weight\n",
      "    \tmodule.encoder.layers.0.mlp.linear_fc2.adapter.linear_in.weight\n",
      "    \tmodule.classification_head.finetune_model.0.weight\n",
      "    \tmodule.encoder.layers.5.mlp.linear_fc1.adapter.linear_in.weight\n",
      "    \tmodule.encoder.layers.4.mlp.linear_fc2.adapter.linear_out.weight\n",
      "    \tmodule.encoder.layers.3.self_attention.linear_proj.adapter.linear_in.weight\n",
      "    \tmodule.encoder.layers.2.mlp.linear_fc1.adapter.linear_out.weight\n",
      "    \tmodule.encoder.layers.1.mlp.linear_fc1.adapter.linear_out.weight\n",
      "    \tmodule.encoder.layers.4.self_attention.linear_proj.adapter.linear_out.weight\n",
      "    \tmodule.encoder.layers.3.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "    \tmodule.encoder.layers.2.mlp.linear_fc2.adapter.linear_in.weight\n",
      "    \tmodule.encoder.layers.2.self_attention.linear_proj.adapter.linear_in.weight\n",
      "    \tmodule.encoder.layers.1.mlp.linear_fc2.adapter.linear_in.weight\n",
      "    \tmodule.encoder.layers.0.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "    \tmodule.encoder.layers.5.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "    \tmodule.encoder.layers.5.self_attention.linear_proj.adapter.linear_out.weight\n",
      "    \tmodule.encoder.layers.4.mlp.linear_fc2.adapter.linear_in.weight\n",
      "    \tmodule.encoder.layers.3.self_attention.linear_qkv.adapter.linear_out.weight\n",
      "    \tmodule.encoder.layers.2.self_attention.linear_qkv.adapter.linear_in.weight\n",
      "    \tmodule.encoder.layers.0.mlp.linear_fc1.adapter.linear_out.weight\n",
      "[NeMo I 2025-04-25 04:50:48 nemo_logging:393] Setting up optimizers\n",
      "[NeMo I 2025-04-25 04:50:48 utils:302] Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.005, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=True, params_dtype=torch.bfloat16, use_precision_aware_optimizer=False, main_grads_dtype=torch.float32, main_params_dtype=torch.float32, exp_avg_dtype=torch.float32, exp_avg_sq_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.98, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather_with_optimizer_step=False, optimizer_cpu_offload=False, optimizer_offload_fraction=0.0, use_torch_optimizer_for_cpu_offload=False, overlap_cpu_optimizer_d2h_h2d=False, pin_cpu_grads=True, pin_cpu_params=True, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')\n",
      "Sanity checking Validation: iteration 1/2\n",
      "Sanity checking Validation: iteration 2/2\n",
      "[NeMo W 2025-04-25 04:50:51 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('global_batch_size', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2025-04-25 04:50:51 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2025-04-25 04:50:51 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2025-04-25 04:50:52 rerun_state_machine:1264] Implicit initialization of Rerun State Machine!\n",
      "[NeMo W 2025-04-25 04:50:52 rerun_state_machine:239] RerunStateMachine initialized in mode RerunMode.DISABLED\n",
      "Training epoch 0, iteration 0/49 | lr: 0.005 | global_batch_size: 2 | global_step: 0 | reduced_train_loss: 1.094\n",
      "Training epoch 0, iteration 1/49 | lr: 0.005 | global_batch_size: 2 | global_step: 1 | reduced_train_loss: 3376 | consumed_samples: 4\n",
      "Training epoch 0, iteration 2/49 | lr: 0.005 | global_batch_size: 2 | global_step: 2 | reduced_train_loss: 1216 | consumed_samples: 6\n",
      "Training epoch 0, iteration 3/49 | lr: 0.005 | global_batch_size: 2 | global_step: 3 | reduced_train_loss: 8704 | consumed_samples: 8\n",
      "Training epoch 0, iteration 4/49 | lr: 0.005 | global_batch_size: 2 | global_step: 4 | reduced_train_loss: 2064 | consumed_samples: 10\n",
      "Training epoch 0, iteration 5/49 | lr: 0.005 | global_batch_size: 2 | global_step: 5 | reduced_train_loss: 7776 | consumed_samples: 12\n",
      "Training epoch 0, iteration 6/49 | lr: 0.005 | global_batch_size: 2 | global_step: 6 | reduced_train_loss: 5952 | consumed_samples: 14\n",
      "Training epoch 0, iteration 7/49 | lr: 0.005 | global_batch_size: 2 | global_step: 7 | reduced_train_loss: 8448 | consumed_samples: 16\n",
      "Training epoch 0, iteration 8/49 | lr: 0.005 | global_batch_size: 2 | global_step: 8 | reduced_train_loss: 3152 | consumed_samples: 18\n",
      "Training epoch 0, iteration 9/49 | lr: 0.005 | global_batch_size: 2 | global_step: 9 | reduced_train_loss: 5952 | consumed_samples: 20\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: Epoch 0, global step 9: 'val_loss' was not in top 2\n",
      "[NeMo I 2025-04-25 04:50:53 nemo_logging:393] Using FullyParallelSaveStrategyWrapper(torch_dist, 1) dist-ckpt save strategy.\n",
      "[NeMo I 2025-04-25 04:51:03 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 9 : Start time: 1745556653.726s : Save duration: 9.420s\n",
      "[NeMo I 2025-04-25 04:51:03 nemo_logging:393] Scheduled async checkpoint save for /workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=9-consumed_samples=20.0-last.ckpt\n",
      "[NeMo I 2025-04-25 04:51:03 nemo_logging:393] Async finalization time took 0.001 s\n",
      "Validation: iteration 1/2\n",
      "Validation: iteration 2/2\n",
      "Training epoch 0, iteration 10/49 | lr: 0.005 | global_batch_size: 2 | global_step: 10 | reduced_train_loss: 5152 | consumed_samples: 22 | val_loss: 4.981e+03 | val_acc: 0.136\n",
      "[NeMo I 2025-04-25 04:51:03 nemo_logging:393] Async finalization time took 0.000 s\n",
      "Training epoch 0, iteration 11/49 | lr: 0.005 | global_batch_size: 2 | global_step: 11 | reduced_train_loss: 1104 | consumed_samples: 24 | val_loss: 4.981e+03 | val_acc: 0.136\n",
      "[NeMo I 2025-04-25 04:51:03 nemo_logging:393] Successfully saved checkpoint from iteration       9 to /workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=9-consumed_samples=20.0-last.ckpt\n",
      "[NeMo I 2025-04-25 04:51:03 nemo_logging:393] Async checkpoint save for step 10 (/workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=9-consumed_samples=20.0-last.ckpt) finalized successfully.\n",
      "[NeMo I 2025-04-25 04:51:03 nemo_logging:393] Async finalization time took 0.016 s\n",
      "Training epoch 0, iteration 12/49 | lr: 0.005 | global_batch_size: 2 | global_step: 12 | reduced_train_loss: 15808 | consumed_samples: 26 | val_loss: 4.981e+03 | val_acc: 0.136\n",
      "Training epoch 0, iteration 13/49 | lr: 0.005 | global_batch_size: 2 | global_step: 13 | reduced_train_loss: 8832 | consumed_samples: 28 | val_loss: 4.981e+03 | val_acc: 0.136\n",
      "Training epoch 0, iteration 14/49 | lr: 0.005 | global_batch_size: 2 | global_step: 14 | reduced_train_loss: 6208 | consumed_samples: 30 | val_loss: 4.981e+03 | val_acc: 0.136\n",
      "Training epoch 0, iteration 15/49 | lr: 0.005 | global_batch_size: 2 | global_step: 15 | reduced_train_loss: 494 | consumed_samples: 32 | val_loss: 4.981e+03 | val_acc: 0.136\n",
      "Training epoch 0, iteration 16/49 | lr: 0.005 | global_batch_size: 2 | global_step: 16 | reduced_train_loss: 1256 | consumed_samples: 34 | val_loss: 4.981e+03 | val_acc: 0.136\n",
      "Training epoch 0, iteration 17/49 | lr: 0.005 | global_batch_size: 2 | global_step: 17 | reduced_train_loss: 6624 | consumed_samples: 36 | val_loss: 4.981e+03 | val_acc: 0.136\n",
      "Training epoch 0, iteration 18/49 | lr: 0.005 | global_batch_size: 2 | global_step: 18 | reduced_train_loss: 4320 | consumed_samples: 38 | val_loss: 4.981e+03 | val_acc: 0.136\n",
      "Training epoch 0, iteration 19/49 | lr: 0.005 | global_batch_size: 2 | global_step: 19 | reduced_train_loss: 2784 | consumed_samples: 40 | val_loss: 4.981e+03 | val_acc: 0.136\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: Epoch 0, global step 19: 'val_loss' reached 4981.33350 (best 4981.33350), saving model to '/workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=19-consumed_samples=40.0.ckpt' as top 2\n",
      "[NeMo I 2025-04-25 04:51:03 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 19 : Start time: 1745556663.872s : Save duration: 0.037s\n",
      "[NeMo I 2025-04-25 04:51:03 nemo_logging:393] Scheduled async checkpoint save for /workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=19-consumed_samples=40.0.ckpt\n",
      "[NeMo I 2025-04-25 04:51:04 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 19 : Start time: 1745556663.992s : Save duration: 0.038s\n",
      "[NeMo I 2025-04-25 04:51:04 nemo_logging:393] Scheduled async checkpoint save for /workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=19-consumed_samples=40.0-last.ckpt\n",
      "[NeMo I 2025-04-25 04:51:04 nemo_logging:393] Async finalization time took 0.001 s\n",
      "Validation: iteration 1/2\n",
      "Validation: iteration 2/2\n",
      "Training epoch 0, iteration 20/49 | lr: 0.005 | global_batch_size: 2 | global_step: 20 | reduced_train_loss: 7968 | consumed_samples: 42 | val_loss: 7.211e+03 | val_acc: 0.2105\n",
      "[NeMo I 2025-04-25 04:51:04 nemo_logging:393] Successfully saved checkpoint from iteration      19 to /workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=19-consumed_samples=40.0.ckpt\n",
      "[NeMo I 2025-04-25 04:51:04 nemo_logging:393] Async checkpoint save for step 20 (/workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=19-consumed_samples=40.0.ckpt) finalized successfully.\n",
      "[NeMo I 2025-04-25 04:51:04 nemo_logging:393] Async finalization time took 0.013 s\n",
      "Training epoch 0, iteration 21/49 | lr: 0.005 | global_batch_size: 2 | global_step: 21 | reduced_train_loss: 4640 | consumed_samples: 44 | val_loss: 7.211e+03 | val_acc: 0.2105\n",
      "[NeMo I 2025-04-25 04:51:04 nemo_logging:393] Successfully saved checkpoint from iteration      19 to /workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=19-consumed_samples=40.0-last.ckpt\n",
      "[NeMo I 2025-04-25 04:51:04 nemo_logging:393] Async checkpoint save for step 20 (/workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=19-consumed_samples=40.0-last.ckpt) finalized successfully.\n",
      "[NeMo I 2025-04-25 04:51:04 nemo_logging:393] Async finalization time took 0.015 s\n",
      "Training epoch 0, iteration 22/49 | lr: 0.005 | global_batch_size: 2 | global_step: 22 | reduced_train_loss: 4128 | consumed_samples: 46 | val_loss: 7.211e+03 | val_acc: 0.2105\n",
      "Training epoch 0, iteration 23/49 | lr: 0.005 | global_batch_size: 2 | global_step: 23 | reduced_train_loss: 1048 | consumed_samples: 48 | val_loss: 7.211e+03 | val_acc: 0.2105\n",
      "Training epoch 0, iteration 24/49 | lr: 0.005 | global_batch_size: 2 | global_step: 24 | reduced_train_loss: 510 | consumed_samples: 50 | val_loss: 7.211e+03 | val_acc: 0.2105\n",
      "Training epoch 0, iteration 25/49 | lr: 0.005 | global_batch_size: 2 | global_step: 25 | reduced_train_loss: 13248 | consumed_samples: 52 | val_loss: 7.211e+03 | val_acc: 0.2105\n",
      "Training epoch 0, iteration 26/49 | lr: 0.005 | global_batch_size: 2 | global_step: 26 | reduced_train_loss: 10432 | consumed_samples: 54 | val_loss: 7.211e+03 | val_acc: 0.2105\n",
      "Training epoch 0, iteration 27/49 | lr: 0.005 | global_batch_size: 2 | global_step: 27 | reduced_train_loss: 1704 | consumed_samples: 56 | val_loss: 7.211e+03 | val_acc: 0.2105\n",
      "Training epoch 0, iteration 28/49 | lr: 0.005 | global_batch_size: 2 | global_step: 28 | reduced_train_loss: 1760 | consumed_samples: 58 | val_loss: 7.211e+03 | val_acc: 0.2105\n",
      "Training epoch 0, iteration 29/49 | lr: 0.005 | global_batch_size: 2 | global_step: 29 | reduced_train_loss: 162 | consumed_samples: 60 | val_loss: 7.211e+03 | val_acc: 0.2105\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: Epoch 0, global step 29: 'val_loss' reached 7210.66650 (best 4981.33350), saving model to '/workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=29-consumed_samples=60.0.ckpt' as top 2\n",
      "[NeMo I 2025-04-25 04:51:04 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 29 : Start time: 1745556664.742s : Save duration: 0.036s\n",
      "[NeMo I 2025-04-25 04:51:04 nemo_logging:393] Scheduled async checkpoint save for /workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=29-consumed_samples=60.0.ckpt\n",
      "[NeMo I 2025-04-25 04:51:04 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 29 : Start time: 1745556664.867s : Save duration: 0.039s\n",
      "[NeMo I 2025-04-25 04:51:04 nemo_logging:393] Scheduled async checkpoint save for /workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=29-consumed_samples=60.0-last.ckpt\n",
      "[NeMo I 2025-04-25 04:51:04 nemo_logging:393] Async finalization time took 0.000 s\n",
      "Validation: iteration 1/2\n",
      "Validation: iteration 2/2\n",
      "Training epoch 0, iteration 30/49 | lr: 0.005 | global_batch_size: 2 | global_step: 30 | reduced_train_loss: 4.5 | consumed_samples: 62 | val_loss: 6.073 | val_acc: 0.3289\n",
      "[NeMo I 2025-04-25 04:51:05 nemo_logging:393] Successfully saved checkpoint from iteration      29 to /workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=29-consumed_samples=60.0.ckpt\n",
      "[NeMo I 2025-04-25 04:51:05 nemo_logging:393] Async checkpoint save for step 30 (/workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=29-consumed_samples=60.0.ckpt) finalized successfully.\n",
      "[NeMo I 2025-04-25 04:51:05 nemo_logging:393] Async finalization time took 0.015 s\n",
      "Training epoch 0, iteration 31/49 | lr: 0.005 | global_batch_size: 2 | global_step: 31 | reduced_train_loss: 2.781 | consumed_samples: 64 | val_loss: 6.073 | val_acc: 0.3289\n",
      "[NeMo I 2025-04-25 04:51:05 nemo_logging:393] Successfully saved checkpoint from iteration      29 to /workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=29-consumed_samples=60.0-last.ckpt\n",
      "[NeMo I 2025-04-25 04:51:05 nemo_logging:393] Async checkpoint save for step 30 (/workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=29-consumed_samples=60.0-last.ckpt) finalized successfully.\n",
      "[NeMo I 2025-04-25 04:51:05 nemo_logging:393] Async finalization time took 0.016 s\n",
      "Training epoch 0, iteration 32/49 | lr: 0.005 | global_batch_size: 2 | global_step: 32 | reduced_train_loss: 3.531 | consumed_samples: 66 | val_loss: 6.073 | val_acc: 0.3289\n",
      "Training epoch 0, iteration 33/49 | lr: 0.005 | global_batch_size: 2 | global_step: 33 | reduced_train_loss: 3.344 | consumed_samples: 68 | val_loss: 6.073 | val_acc: 0.3289\n",
      "Training epoch 0, iteration 34/49 | lr: 0.005 | global_batch_size: 2 | global_step: 34 | reduced_train_loss: 2.672 | consumed_samples: 70 | val_loss: 6.073 | val_acc: 0.3289\n",
      "Training epoch 0, iteration 35/49 | lr: 0.005 | global_batch_size: 2 | global_step: 35 | reduced_train_loss: 2.016 | consumed_samples: 72 | val_loss: 6.073 | val_acc: 0.3289\n",
      "Training epoch 0, iteration 36/49 | lr: 0.005 | global_batch_size: 2 | global_step: 36 | reduced_train_loss: 1.508 | consumed_samples: 74 | val_loss: 6.073 | val_acc: 0.3289\n",
      "Training epoch 0, iteration 37/49 | lr: 0.005 | global_batch_size: 2 | global_step: 37 | reduced_train_loss: 1.18 | consumed_samples: 76 | val_loss: 6.073 | val_acc: 0.3289\n",
      "Training epoch 0, iteration 38/49 | lr: 0.005 | global_batch_size: 2 | global_step: 38 | reduced_train_loss: 1.047 | consumed_samples: 78 | val_loss: 6.073 | val_acc: 0.3289\n",
      "Training epoch 0, iteration 39/49 | lr: 0.005 | global_batch_size: 2 | global_step: 39 | reduced_train_loss: 0.9531 | consumed_samples: 80 | val_loss: 6.073 | val_acc: 0.3289\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: Epoch 0, global step 39: 'val_loss' reached 6.07292 (best 6.07292), saving model to '/workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=39-consumed_samples=80.0.ckpt' as top 2\n",
      "[NeMo I 2025-04-25 04:51:05 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 39 : Start time: 1745556665.620s : Save duration: 0.037s\n",
      "[NeMo I 2025-04-25 04:51:05 nemo_logging:393] Scheduled async checkpoint save for /workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=39-consumed_samples=80.0.ckpt\n",
      "[NeMo I 2025-04-25 04:51:05 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 39 : Start time: 1745556665.741s : Save duration: 0.039s\n",
      "[NeMo I 2025-04-25 04:51:05 nemo_logging:393] Scheduled async checkpoint save for /workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=39-consumed_samples=80.0-last.ckpt\n",
      "[NeMo I 2025-04-25 04:51:05 nemo_logging:393] Async finalization time took 0.001 s\n",
      "Validation: iteration 1/2\n",
      "Validation: iteration 2/2\n",
      "Training epoch 0, iteration 40/49 | lr: 0.005 | global_batch_size: 2 | global_step: 40 | reduced_train_loss: 1 | consumed_samples: 82 | val_loss: 1.234 | val_acc: 0.3114\n",
      "[NeMo I 2025-04-25 04:51:05 nemo_logging:393] Successfully saved checkpoint from iteration      39 to /workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=39-consumed_samples=80.0.ckpt\n",
      "[NeMo I 2025-04-25 04:51:05 nemo_logging:393] Async checkpoint save for step 40 (/workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=39-consumed_samples=80.0.ckpt) finalized successfully.\n",
      "[NeMo I 2025-04-25 04:51:05 nemo_logging:393] Async finalization time took 0.017 s\n",
      "Training epoch 0, iteration 41/49 | lr: 0.005 | global_batch_size: 2 | global_step: 41 | reduced_train_loss: 1.445 | consumed_samples: 84 | val_loss: 1.234 | val_acc: 0.3114\n",
      "[NeMo I 2025-04-25 04:51:06 nemo_logging:393] Async finalization time took 0.000 s\n",
      "Training epoch 0, iteration 42/49 | lr: 0.005 | global_batch_size: 2 | global_step: 42 | reduced_train_loss: 1.273 | consumed_samples: 86 | val_loss: 1.234 | val_acc: 0.3114\n",
      "[NeMo I 2025-04-25 04:51:06 nemo_logging:393] Successfully saved checkpoint from iteration      39 to /workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=39-consumed_samples=80.0-last.ckpt\n",
      "[NeMo I 2025-04-25 04:51:06 nemo_logging:393] Async checkpoint save for step 40 (/workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=39-consumed_samples=80.0-last.ckpt) finalized successfully.\n",
      "[NeMo I 2025-04-25 04:51:06 nemo_logging:393] Async finalization time took 0.017 s\n",
      "Training epoch 0, iteration 43/49 | lr: 0.005 | global_batch_size: 2 | global_step: 43 | reduced_train_loss: 2.266 | consumed_samples: 88 | val_loss: 1.234 | val_acc: 0.3114\n",
      "Training epoch 0, iteration 44/49 | lr: 0.005 | global_batch_size: 2 | global_step: 44 | reduced_train_loss: 1.18 | consumed_samples: 90 | val_loss: 1.234 | val_acc: 0.3114\n",
      "Training epoch 0, iteration 45/49 | lr: 0.005 | global_batch_size: 2 | global_step: 45 | reduced_train_loss: 1.391 | consumed_samples: 92 | val_loss: 1.234 | val_acc: 0.3114\n",
      "Training epoch 0, iteration 46/49 | lr: 0.005 | global_batch_size: 2 | global_step: 46 | reduced_train_loss: 1.242 | consumed_samples: 94 | val_loss: 1.234 | val_acc: 0.3114\n",
      "Training epoch 0, iteration 47/49 | lr: 0.005 | global_batch_size: 2 | global_step: 47 | reduced_train_loss: 1.578 | consumed_samples: 96 | val_loss: 1.234 | val_acc: 0.3114\n",
      "Training epoch 0, iteration 48/49 | lr: 0.005 | global_batch_size: 2 | global_step: 48 | reduced_train_loss: 1.945 | consumed_samples: 98 | val_loss: 1.234 | val_acc: 0.3114\n",
      "Training epoch 0, iteration 49/49 | lr: 0.005 | global_batch_size: 2 | global_step: 49 | reduced_train_loss: 1.07 | consumed_samples: 100 | val_loss: 1.234 | val_acc: 0.3114\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: Epoch 0, global step 49: 'val_loss' reached 1.23438 (best 1.23438), saving model to '/workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=49-consumed_samples=100.0.ckpt' as top 2\n",
      "[NeMo I 2025-04-25 04:51:06 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 49 : Start time: 1745556666.484s : Save duration: 0.037s\n",
      "[NeMo I 2025-04-25 04:51:06 nemo_logging:393] Scheduled async checkpoint save for /workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=49-consumed_samples=100.0.ckpt\n",
      "[NeMo I 2025-04-25 04:51:06 nemo_logging:393] Global Checkpoint Save : Rank: 0 : Iteration: 49 : Start time: 1745556666.605s : Save duration: 0.041s\n",
      "[NeMo I 2025-04-25 04:51:06 nemo_logging:393] Scheduled async checkpoint save for /workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=49-consumed_samples=100.0-last.ckpt\n",
      "[NeMo I 2025-04-25 04:51:06 nemo_logging:393] Async finalization time took 0.000 s\n",
      "Validation: iteration 1/2\n",
      "Validation: iteration 2/2\n",
      "[NeMo I 2025-04-25 04:51:06 nemo_logging:393] Successfully saved checkpoint from iteration      49 to /workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=49-consumed_samples=100.0.ckpt\n",
      "[NeMo I 2025-04-25 04:51:06 nemo_logging:393] Async checkpoint save for step 50 (/workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=49-consumed_samples=100.0.ckpt) finalized successfully.\n",
      "[NeMo I 2025-04-25 04:51:06 nemo_logging:393] Async finalization time took 0.017 s\n",
      "[INFO     | pytorch_lightning.utilities.rank_zero]: `Trainer.fit` stopped: `max_steps=50` reached.\n",
      "[NeMo I 2025-04-25 04:51:06 nemo_logging:393] Pending async checkpoint saves. Finalizing them synchronously now\n",
      "[NeMo I 2025-04-25 04:51:06 nemo_logging:393] Successfully saved checkpoint from iteration      49 to /workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=49-consumed_samples=100.0-last.ckpt\n",
      "[NeMo I 2025-04-25 04:51:06 nemo_logging:393] Async checkpoint save for step 50 (/workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/dev/checkpoints/checkpoint-step=49-consumed_samples=100.0-last.ckpt) finalized successfully.\n",
      "[NeMo I 2025-04-25 04:51:06 nemo_logging:393] Async finalization time took 0.120 s\n"
     ]
    }
   ],
   "source": [
    "# %%capture --no-display --no-stderr cell_output\n",
    "\n",
    "data_path = os.path.join(work_dir, \"token_classification_data.csv\")\n",
    "\n",
    "! finetune_esm2 \\\n",
    "    --restore-from-checkpoint-path {pretrain_checkpoint_path} \\\n",
    "    --train-data-path {data_path} \\\n",
    "    --valid-data-path {data_path} \\\n",
    "    --config-class ESM2FineTuneTokenConfig \\\n",
    "    --dataset-class InMemoryPerTokenValueDataset \\\n",
    "    --task-type \"classification\" \\\n",
    "    --cnn-dropout 0.25 \\\n",
    "    --cnn-hidden-size 32 \\\n",
    "    --cnn-num-classes 3 \\\n",
    "    --experiment-name \"lora-token-level-classification\" \\\n",
    "    --num-steps 50 \\\n",
    "    --num-gpus 1 \\\n",
    "    --val-check-interval 10 \\\n",
    "    --log-every-n-steps 10 \\\n",
    "    --encoder-frozen \\\n",
    "    --lr 5e-3 \\\n",
    "    --lr-multiplier 1e2 \\\n",
    "    --scale-lr-layer \"classification_head\" \\\n",
    "    --result-dir {work_dir}  \\\n",
    "    --micro-batch-size 2 \\\n",
    "    --num-gpus 1 \\\n",
    "    --precision \"bf16-mixed\" \\\n",
    "    --lora-finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_checkpoint_path = f\"{work_dir}/lora-token-level-classification/dev/checkpoints/checkpoint-step=49-consumed_samples=100.0-last/weights\"\n",
    "results_path = f\"{work_dir}/lora-token-level-classification/infer/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/bionemo2/esm2_finetune_tutorial/lora-token-level-classification/infer/\n"
     ]
    }
   ],
   "source": [
    "print(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "data_path = os.path.join(work_dir, \"sequences.csv\")\n",
    "\n",
    "! infer_esm2 --checkpoint-path {pretrain_checkpoint_path} \\\n",
    "             --config-class ESM2FineTuneTokenConfig \\\n",
    "             --data-path {data_path} \\\n",
    "             --results-path {results_path} \\\n",
    "             --micro-batch-size 3 \\\n",
    "             --num-gpus 1 \\\n",
    "             --precision \"bf16-mixed\" \\\n",
    "             --include-embeddings \\\n",
    "             --include-hiddens \\\n",
    "             --include-input-ids \\\n",
    "             --lora-checkpoint-path {lora_checkpoint_path}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
