{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c23e69",
   "metadata": {},
   "source": [
    "# Adding the OAS Dataset: Modifying the Dataset Class\n",
    "\n",
    "This tutorial is the second part of a series focused on adding a new dataset to BioNeMo using the [Observed Antibody Space (OAS)](https://opig.stats.ox.ac.uk/webapps/oas/) database. There are three steps to this task:\n",
    "\n",
    "1. Preprocessing includes download of the raw data and any additional preparation steps, such as extracting the files. It also includes dividing the data into train, validation, and test splits. The preprocessing step can make use of two BioNeMo base classes, `RemoteResource` and `ResourcePreprocessor`, from `bionemo.utils.remote` and `bionemo.data.preprocess.dna.preprocess`, respectively. Their use is optional but they provide some basic functionality which can accelerate development. This step is covered by this tutorial. This objective was accomplished by the previous tutorial, <a href=\"custom-dataset-preprocessing-fw.html\">Downloading and Preprocessing</a>. </br></br>\n",
    "2. Development of the new dataset class. Here, the NeMo dataset class [CSVMemMapDataset](https://github.com/NVIDIA/NeMo/blob/b0e5bf3627dbcfb3f4a72d73d3c5e92184d8b1f6/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py#L286) will be used. This step will be completed during the current tutorial. </br></br>\n",
    "3. Modification of the dataloader classes. This task will be covered by the third tutorial, <a href=\"custom-dataloader-fw.html\">Adding a Custom Dataloader</a>.  TODO FIX LINK WHEN TUTORIAL FINISHED </br></br>\n",
    "\n",
    "This tutorial assumes the first step has been completed successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7616f026",
   "metadata": {},
   "source": [
    "## Setup and Assumptions\n",
    "\n",
    "This tutorial assumes that a copy of the BioNeMo framework repo exists on workstation or server and has been mounted inside the container at `/workspace/bionemo` as described in the <a href=\"../quickstart-fw.html#code-development\">Code Development section of the Quickstart Guide</a>. This path will be referred to with the variable `BIONEMO_WORKSPACE` in the tutorial. \n",
    "\n",
    "All commands should be executed inside the BioNeMo docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1ce6b1e",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "BIONEMO_WORKSPACE = '/workspace/bionemo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3eccafb",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "### Utility functions \n",
    "\n",
    "from IPython.display import Code\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def stage_files(tag: str,\n",
    "                source_directory: str = f'{BIONEMO_WORKSPACE}/examples/oas_dataset'):\n",
    "    \"\"\"Stage files for each step of the tutorial\"\"\"\n",
    "    source_path = os.path.join(source_directory, tag)\n",
    "    \n",
    "    data_path = os.path.join(BIONEMO_WORKSPACE, 'bionemo/data/preprocess/protein')\n",
    "    shutil.copyfile(os.path.join(source_path, 'oas_paired_subset_download.sh'), \n",
    "                    os.path.join(data_path, 'oas_paired_subset_download.sh'))\n",
    "    \n",
    "    preprocess_path = os.path.join(BIONEMO_WORKSPACE, 'bionemo/data/preprocess/protein')\n",
    "    shutil.copyfile(os.path.join(source_path, 'oas_preprocess.py'), \n",
    "                    os.path.join(preprocess_path, 'oas_preprocess.py'))\n",
    "    \n",
    "    config_path = os.path.join(BIONEMO_WORKSPACE, 'examples/protein/esm1nv/conf')\n",
    "    shutil.copyfile(os.path.join(source_path, 'pretrain_oas.yaml'), \n",
    "                    os.path.join(config_path, 'pretrain_oas.yaml'))\n",
    "    \n",
    "    pretrain_path = os.path.join(BIONEMO_WORKSPACE, 'examples/protein/esm1nv')\n",
    "    shutil.copyfile(os.path.join(source_path, 'pretrain_oas.py'), \n",
    "                    os.path.join(pretrain_path, 'pretrain_oas.py'))\n",
    "\n",
    "def show_code(filename: str,\n",
    "              language: str,\n",
    "              start_line = None,\n",
    "              end_line = None,\n",
    "              end_column = None):\n",
    "    \"\"\"Display syntax highlighted section of code\"\"\"\n",
    "    \n",
    "    with open(filename, 'r') as fh:\n",
    "        code = fh.readlines()\n",
    "\n",
    "    if end_line:\n",
    "        code = code[:end_line]\n",
    "        code.append('...\\n')\n",
    "    if start_line:\n",
    "        code = code[start_line:]\n",
    "        code.insert(0, '...\\n')\n",
    "    if end_column:\n",
    "        for line in code:\n",
    "            line = line[:end_column] + '...\\n'\n",
    "        \n",
    "    code = ''.join(code)\n",
    "    return Code(data=code, language=language)\n",
    "\n",
    "\n",
    "def filter_log(logfile_list, regex):\n",
    "    \"\"\"Filter a list of log output until a regex match is found\"\"\"\n",
    "\n",
    "    reg = re.compile(regex)\n",
    "    string_matches = filter(reg.search, logfile_list)\n",
    "    position_matches = list(map(lambda x: logfile_list.index(x), string_matches))\n",
    "    logfile_list = logfile_list[position_matches[0]:]\n",
    "    return '\\n'.join(logfile_list)\n",
    "\n",
    "def clean_progress_bar(logfile_list):\n",
    "    \"\"\"Remove incremental progress bar entries. Must also prune empty lines.\"\"\"\n",
    "\n",
    "    progress_reg = re.compile(r\"\"\"\\d+\\%|.+?\\| (?P<cur_iter>\\d+)\\/(?P<max_iter>\\d+)\"\"\")\n",
    "    clean_list = []\n",
    "\n",
    "    for row in logfile_list:\n",
    "        keep_line = True\n",
    "        row = row.strip()\n",
    "\n",
    "        progress_match = re.search(progress_reg, row)\n",
    "        if progress_match:\n",
    "            if progress_match.group('cur_iter') != progress_match.group('max_iter'):\n",
    "                keep_line = False\n",
    "\n",
    "        if keep_line:\n",
    "            clean_list.append(row)\n",
    "\n",
    "    return '\\n'.join(clean_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6893f04b",
   "metadata": {},
   "source": [
    "## Configuring the CSV Memory Mapped Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "571280bb",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "TUTORIAL_FILE_VERSION = 'step_040_dataset'\n",
    "stage_files(TUTORIAL_FILE_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35c287e",
   "metadata": {},
   "source": [
    "### Custom YAML Config\n",
    "\n",
    "BioNeMo uses memory mapping to enable the flexibility of text based data formats, such as CSV, while also minimizing memory usage. The key elements of the [CSVMemMapDataset](https://github.com/NVIDIA/NeMo/blob/b0e5bf3627dbcfb3f4a72d73d3c5e92184d8b1f6/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py#L286) dataset that must be changed in the `model.data` section of the YAML configuration file are:\n",
    "\n",
    "* `dataset_paths`: a list of the paths to all data files for a given split, which contains the `train`, `val`, `test` columns. For the OAS heavy chain data, the path is `/data/OASpaired/processed/heavy`.\n",
    "* `data_col`: the zero-based integer number of the column containing the pretraining data. This will be set to `1` to select the column `sequence_heavy`.\n",
    "* `data_sep`: the delimiter for the CSV dataset, defaults to '`,`'. This will not need to be changed.\n",
    "* `header_lines`: the number of header lines in the data files, defaults to `1`. This will not need to be changed.\n",
    "\n",
    "The range of exsting datafiles must also be updated to reflect that there are six files (named `x000.csv` through `x005.csv` for training and two (`x000.csv` and `x001.csv`) for validation and test data, respectively. `do_training` will also be set to `True` since a pretraining run is required to test the dataset class.\n",
    "\n",
    "The YAML configuration file below demonstrates these changes. Config files are located in ``{BIONEMO_WORKSPACE}/examples/protein/esm1nv/conf/``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "596ed262",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"nt\">defaults</span><span class=\"p\">:</span>\n",
       "<span class=\"w\">  </span><span class=\"p p-Indicator\">-</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">base_config</span>\n",
       "\n",
       "<span class=\"nt\">name</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">esm1nv-oas</span>\n",
       "<span class=\"nt\">do_training</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">True</span><span class=\"w\"> </span><span class=\"c1\">### Set to True to run training</span>\n",
       "\n",
       "<span class=\"c1\">###### Begin OAS Related Addtions ######</span>\n",
       "\n",
       "<span class=\"nt\">trainer</span><span class=\"p\">:</span>\n",
       "<span class=\"w\">  </span><span class=\"nt\">devices</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1</span><span class=\"w\"> </span>\n",
       "<span class=\"w\">  </span><span class=\"nt\">max_steps</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">200</span><span class=\"w\"> </span><span class=\"c1\"># consumed_samples = global_step * micro_batch_size * data_parallel_size * accumulate_grad_batches</span>\n",
       "<span class=\"w\">  </span><span class=\"nt\">val_check_interval</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">100</span>\n",
       "<span class=\"w\">  </span><span class=\"nt\">limit_val_batches</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">10</span><span class=\"w\"> </span><span class=\"c1\"># number of batches in validation step, use fraction for fraction of data, 0 to disable</span>\n",
       "\n",
       "<span class=\"c1\">###### End OAS Related Addtions ######</span>\n",
       "\n",
       "<span class=\"nt\">exp_manager</span><span class=\"p\">:</span><span class=\"w\"> </span>\n",
       "<span class=\"w\">  </span><span class=\"nt\">create_wandb_logger</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">False</span>\n",
       "\n",
       "<span class=\"nt\">restore_from_path</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">null</span><span class=\"w\"> </span><span class=\"c1\"># used when starting from a .nemo file</span>\n",
       "\n",
       "<span class=\"nt\">model</span><span class=\"p\">:</span>\n",
       "<span class=\"w\">  </span><span class=\"nt\">tokenizer</span><span class=\"p\">:</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">library</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s\">&#39;sentencepiece&#39;</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">type</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">null</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">model</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">/tokenizers/protein/esm1nv/vocab/protein_sequence_sentencepiece.model</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">vocab_file</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">/tokenizers/vocab/protein_sequence_sentencepiece.vocab</span>\n",
       "<span class=\"w\">  </span><span class=\"nt\">data</span><span class=\"p\">:</span>\n",
       "<span class=\"w\">  </span>\n",
       "<span class=\"w\">    </span><span class=\"c1\">###### Begin OAS Related Addtions ######</span>\n",
       "<span class=\"w\">  </span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">dataset_path</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">/data/OASpaired/processed/heavy</span><span class=\"w\"> </span><span class=\"c1\"># parent directory for data, contains train / val / test folders. Needs to be writeable for index creation.</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">dataset</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"c1\"># inclusive range of data files to load x[000..049] or can a single file, e.g. x000</span>\n",
       "<span class=\"w\">      </span><span class=\"nt\">train</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">x[000..005]</span>\n",
       "<span class=\"w\">      </span><span class=\"nt\">test</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">x[000..001]</span>\n",
       "<span class=\"w\">      </span><span class=\"nt\">val</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">x[000..001]</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">data_impl_kwargs</span><span class=\"p\">:</span>\n",
       "<span class=\"w\">      </span><span class=\"nt\">csv_mmap</span><span class=\"p\">:</span>\n",
       "<span class=\"w\">        </span><span class=\"nt\">data_col</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">1</span><span class=\"w\"> </span><span class=\"c1\"># 0-based</span>\n",
       "<span class=\"w\">  </span>\n",
       "<span class=\"w\">    </span><span class=\"c1\">###### End OAS Related Addtions ######</span>\n",
       "<span class=\"w\">  </span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">micro_batch_size</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">${model.micro_batch_size}</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">num_workers</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">10</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">modify_percent</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">0.1</span><span class=\"w\"> </span><span class=\"c1\"># Percentage of characters in a protein sequence to modify. (Modification means replacing with another amino acid or with a mask token)</span>\n",
       "<span class=\"w\">    </span><span class=\"nt\">perturb_percent</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"l l-Scalar l-Scalar-Plain\">0.5</span><span class=\"w\"> </span><span class=\"c1\"># Of the modify_percent, what percentage of characters are to be replaced with another amino acid.</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{n+nt}{defaults}\\PY{p}{:}\n",
       "\\PY{+w}{  }\\PY{p+pIndicator}{\\PYZhy{}}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{base\\PYZus{}config}\n",
       "\n",
       "\\PY{n+nt}{name}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{esm1nv\\PYZhy{}oas}\n",
       "\\PY{n+nt}{do\\PYZus{}training}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{True}\\PY{+w}{ }\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{} Set to True to run training}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{} Begin OAS Related Addtions \\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "\n",
       "\\PY{n+nt}{trainer}\\PY{p}{:}\n",
       "\\PY{+w}{  }\\PY{n+nt}{devices}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{1}\\PY{+w}{ }\n",
       "\\PY{+w}{  }\\PY{n+nt}{max\\PYZus{}steps}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{200}\\PY{+w}{ }\\PY{c+c1}{\\PYZsh{} consumed\\PYZus{}samples = global\\PYZus{}step * micro\\PYZus{}batch\\PYZus{}size * data\\PYZus{}parallel\\PYZus{}size * accumulate\\PYZus{}grad\\PYZus{}batches}\n",
       "\\PY{+w}{  }\\PY{n+nt}{val\\PYZus{}check\\PYZus{}interval}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{100}\n",
       "\\PY{+w}{  }\\PY{n+nt}{limit\\PYZus{}val\\PYZus{}batches}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{10}\\PY{+w}{ }\\PY{c+c1}{\\PYZsh{} number of batches in validation step, use fraction for fraction of data, 0 to disable}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{} End OAS Related Addtions \\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "\n",
       "\\PY{n+nt}{exp\\PYZus{}manager}\\PY{p}{:}\\PY{+w}{ }\n",
       "\\PY{+w}{  }\\PY{n+nt}{create\\PYZus{}wandb\\PYZus{}logger}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{False}\n",
       "\n",
       "\\PY{n+nt}{restore\\PYZus{}from\\PYZus{}path}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{null}\\PY{+w}{ }\\PY{c+c1}{\\PYZsh{} used when starting from a .nemo file}\n",
       "\n",
       "\\PY{n+nt}{model}\\PY{p}{:}\n",
       "\\PY{+w}{  }\\PY{n+nt}{tokenizer}\\PY{p}{:}\n",
       "\\PY{+w}{    }\\PY{n+nt}{library}\\PY{p}{:}\\PY{+w}{ }\\PY{l+s}{\\PYZsq{}}\\PY{l+s}{sentencepiece}\\PY{l+s}{\\PYZsq{}}\n",
       "\\PY{+w}{    }\\PY{n+nt}{type}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{null}\n",
       "\\PY{+w}{    }\\PY{n+nt}{model}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{/tokenizers/protein/esm1nv/vocab/protein\\PYZus{}sequence\\PYZus{}sentencepiece.model}\n",
       "\\PY{+w}{    }\\PY{n+nt}{vocab\\PYZus{}file}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{/tokenizers/vocab/protein\\PYZus{}sequence\\PYZus{}sentencepiece.vocab}\n",
       "\\PY{+w}{  }\\PY{n+nt}{data}\\PY{p}{:}\n",
       "\\PY{+w}{  }\n",
       "\\PY{+w}{    }\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{} Begin OAS Related Addtions \\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "\\PY{+w}{  }\n",
       "\\PY{+w}{    }\\PY{n+nt}{dataset\\PYZus{}path}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{/data/OASpaired/processed/heavy}\\PY{+w}{ }\\PY{c+c1}{\\PYZsh{} parent directory for data, contains train / val / test folders. Needs to be writeable for index creation.}\n",
       "\\PY{+w}{    }\\PY{n+nt}{dataset}\\PY{p}{:}\\PY{+w}{ }\\PY{c+c1}{\\PYZsh{} inclusive range of data files to load x[000..049] or can a single file, e.g. x000}\n",
       "\\PY{+w}{      }\\PY{n+nt}{train}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{x[000..005]}\n",
       "\\PY{+w}{      }\\PY{n+nt}{test}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{x[000..001]}\n",
       "\\PY{+w}{      }\\PY{n+nt}{val}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{x[000..001]}\n",
       "\\PY{+w}{    }\\PY{n+nt}{data\\PYZus{}impl\\PYZus{}kwargs}\\PY{p}{:}\n",
       "\\PY{+w}{      }\\PY{n+nt}{csv\\PYZus{}mmap}\\PY{p}{:}\n",
       "\\PY{+w}{        }\\PY{n+nt}{data\\PYZus{}col}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{1}\\PY{+w}{ }\\PY{c+c1}{\\PYZsh{} 0\\PYZhy{}based}\n",
       "\\PY{+w}{  }\n",
       "\\PY{+w}{    }\\PY{c+c1}{\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{} End OAS Related Addtions \\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}\\PYZsh{}}\n",
       "\\PY{+w}{  }\n",
       "\\PY{+w}{    }\\PY{n+nt}{micro\\PYZus{}batch\\PYZus{}size}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{\\PYZdl{}\\PYZob{}model.micro\\PYZus{}batch\\PYZus{}size\\PYZcb{}}\n",
       "\\PY{+w}{    }\\PY{n+nt}{num\\PYZus{}workers}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{10}\n",
       "\\PY{+w}{    }\\PY{n+nt}{modify\\PYZus{}percent}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{0.1}\\PY{+w}{ }\\PY{c+c1}{\\PYZsh{} Percentage of characters in a protein sequence to modify. (Modification means replacing with another amino acid or with a mask token)}\n",
       "\\PY{+w}{    }\\PY{n+nt}{perturb\\PYZus{}percent}\\PY{p}{:}\\PY{+w}{ }\\PY{l+lScalar+lScalarPlain}{0.5}\\PY{+w}{ }\\PY{c+c1}{\\PYZsh{} Of the modify\\PYZus{}percent, what percentage of characters are to be replaced with another amino acid.}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "defaults:\n",
       "  - base_config\n",
       "\n",
       "name: esm1nv-oas\n",
       "do_training: True ### Set to True to run training\n",
       "\n",
       "###### Begin OAS Related Addtions ######\n",
       "\n",
       "trainer:\n",
       "  devices: 1 \n",
       "  max_steps: 200 # consumed_samples = global_step * micro_batch_size * data_parallel_size * accumulate_grad_batches\n",
       "  val_check_interval: 100\n",
       "  limit_val_batches: 10 # number of batches in validation step, use fraction for fraction of data, 0 to disable\n",
       "\n",
       "###### End OAS Related Addtions ######\n",
       "\n",
       "exp_manager: \n",
       "  create_wandb_logger: False\n",
       "\n",
       "restore_from_path: null # used when starting from a .nemo file\n",
       "\n",
       "model:\n",
       "  tokenizer:\n",
       "    library: 'sentencepiece'\n",
       "    type: null\n",
       "    model: /tokenizers/protein/esm1nv/vocab/protein_sequence_sentencepiece.model\n",
       "    vocab_file: /tokenizers/vocab/protein_sequence_sentencepiece.vocab\n",
       "  data:\n",
       "  \n",
       "    ###### Begin OAS Related Addtions ######\n",
       "  \n",
       "    dataset_path: /data/OASpaired/processed/heavy # parent directory for data, contains train / val / test folders. Needs to be writeable for index creation.\n",
       "    dataset: # inclusive range of data files to load x[000..049] or can a single file, e.g. x000\n",
       "      train: x[000..005]\n",
       "      test: x[000..001]\n",
       "      val: x[000..001]\n",
       "    data_impl_kwargs:\n",
       "      csv_mmap:\n",
       "        data_col: 1 # 0-based\n",
       "  \n",
       "    ###### End OAS Related Addtions ######\n",
       "  \n",
       "    micro_batch_size: ${model.micro_batch_size}\n",
       "    num_workers: 10\n",
       "    modify_percent: 0.1 # Percentage of characters in a protein sequence to modify. (Modification means replacing with another amino acid or with a mask token)\n",
       "    perturb_percent: 0.5 # Of the modify_percent, what percentage of characters are to be replaced with another amino acid."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = f'{BIONEMO_WORKSPACE}/examples/protein/esm1nv/conf/pretrain_oas.yaml'\n",
    "show_code(filename, language='yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debb0d59",
   "metadata": {},
   "source": [
    "### Testing \n",
    "\n",
    "No additional changes should need to be made to other files before testing.\n",
    "\n",
    "As before, execute the pretrain script:\n",
    "\n",
    "```shell\n",
    "cd examples/protein/esm1nv\n",
    "python pretrain_oas.py\n",
    "```\n",
    "\n",
    "The entire log is shown this time for completeness, but the sections associated with loading data can be found by searching for the text \"Loading data from\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90efcbb4",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-08-17 16:22:29 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-08-17 16:22:29 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-08-17 16:22:30 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'pretrain_oas': Defaults list is missing `_self_`. See https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "      warnings.warn(msg, UserWarning)\n",
      "    \n",
      "[NeMo W 2023-08-17 16:22:30 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n",
      "[NeMo I 2023-08-17 16:22:30 pretrain_oas:14] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2023-08-17 16:22:30 pretrain_oas:15] \n",
      "    name: esm1nv-oas\n",
      "    do_training: true\n",
      "    do_testing: false\n",
      "    restore_from_path: null\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      num_nodes: 1\n",
      "      accelerator: gpu\n",
      "      precision: 16-mixed\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: null\n",
      "      max_steps: 200\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 100\n",
      "      limit_val_batches: 10\n",
      "      limit_test_batches: 500\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "      benchmark: false\n",
      "    exp_manager:\n",
      "      name: ${name}\n",
      "      exp_dir: /result/nemo_experiments/${.name}/${.wandb_logger_kwargs.name}\n",
      "      explicit_log_dir: ${.exp_dir}\n",
      "      create_wandb_logger: false\n",
      "      create_tensorboard_logger: true\n",
      "      wandb_logger_kwargs:\n",
      "        project: ${name}_pretraining\n",
      "        name: ${name}_pretraining\n",
      "        group: ${name}\n",
      "        job_type: Localhost_nodes_${trainer.num_nodes}_gpus_${trainer.devices}\n",
      "        notes: 'date: ${now:%y%m%d-%H%M%S}'\n",
      "        tags:\n",
      "        - ${name}\n",
      "        offline: false\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: val_loss\n",
      "        save_top_k: 10\n",
      "        mode: min\n",
      "        always_save_nemo: false\n",
      "        filename: megatron_bert--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "    model:\n",
      "      micro_batch_size: 8\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      seq_length: 512\n",
      "      max_position_embeddings: ${.seq_length}\n",
      "      encoder_seq_length: ${.seq_length}\n",
      "      num_layers: 6\n",
      "      hidden_size: 768\n",
      "      ffn_hidden_size: 3072\n",
      "      num_attention_heads: 12\n",
      "      init_method_std: 0.02\n",
      "      hidden_dropout: 0.1\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: true\n",
      "      bert_binary_head: false\n",
      "      resume_from_checkpoint: null\n",
      "      masked_softmax_fusion: true\n",
      "      tokenizer:\n",
      "        library: sentencepiece\n",
      "        type: null\n",
      "        model: /tokenizers/protein/esm1nv/vocab/protein_sequence_sentencepiece.model\n",
      "        vocab_file: /tokenizers/vocab/protein_sequence_sentencepiece.vocab\n",
      "        merge_file: null\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      seed: 1234\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: 1\n",
      "      data:\n",
      "        ngc_registry_target: uniref50_2022_05\n",
      "        ngc_registry_version: v23.06\n",
      "        data_prefix: ''\n",
      "        num_workers: 10\n",
      "        dataloader_type: single\n",
      "        reset_position_ids: false\n",
      "        reset_attention_mask: false\n",
      "        eod_mask_loss: false\n",
      "        masked_lm_prob: 0.15\n",
      "        short_seq_prob: 0.1\n",
      "        skip_lines: 0\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "        data_impl: csv_mmap\n",
      "        data_impl_kwargs:\n",
      "          csv_mmap:\n",
      "            header_lines: 1\n",
      "            newline_int: 10\n",
      "            workers: ${model.data.num_workers}\n",
      "            sort_dataset_paths: true\n",
      "            data_sep: ','\n",
      "            data_col: 1\n",
      "        use_upsampling: true\n",
      "        seed: ${model.seed}\n",
      "        max_seq_length: ${model.seq_length}\n",
      "        dataset_path: /data/OASpaired/processed/heavy\n",
      "        dataset:\n",
      "          train: x[000..005]\n",
      "          test: x[000..001]\n",
      "          val: x[000..001]\n",
      "        micro_batch_size: ${model.micro_batch_size}\n",
      "        modify_percent: 0.1\n",
      "        perturb_percent: 0.5\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0002\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 500\n",
      "          constant_steps: 50000\n",
      "          min_lr: 2.0e-05\n",
      "    \n",
      "[NeMo W 2023-08-17 16:22:30 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/plugins/precision/native_amp.py:131: LightningDeprecationWarning: The `NativeMixedPrecisionPlugin` class has been renamed in v1.9.0 and will be removed in v2.0.0. Please use `pytorch_lightning.plugins.MixedPrecisionPlugin` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo I 2023-08-17 16:22:30 utils:168] Selected Callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo E 2023-08-17 16:22:30 exp_manager:646] exp_manager received explicit_log_dir: /result/nemo_experiments/esm1nv-oas/esm1nv-oas_pretraining and at least one of exp_dir: /result/nemo_experiments/esm1nv-oas/esm1nv-oas_pretraining, or version: None. Please note that exp_dir, name, and version will be ignored.\n",
      "[NeMo W 2023-08-17 16:22:30 exp_manager:568] There was no checkpoint folder at checkpoint_dir :/result/nemo_experiments/esm1nv-oas/esm1nv-oas_pretraining/checkpoints. Training from scratch.\n",
      "[NeMo I 2023-08-17 16:22:30 exp_manager:374] Experiments will be logged at /result/nemo_experiments/esm1nv-oas/esm1nv-oas_pretraining\n",
      "[NeMo I 2023-08-17 16:22:30 exp_manager:797] TensorboardLogger has been set up\n",
      "[NeMo W 2023-08-17 16:22:30 exp_manager:893] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 200. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo I 2023-08-17 16:22:30 utils:191] Resuming training from checkpoint: None\n",
      "[NeMo I 2023-08-17 16:22:30 utils:234] \n",
      "    \n",
      "    ************** Trainer configuration ***********\n",
      "[NeMo I 2023-08-17 16:22:30 utils:235] \n",
      "    name: esm1nv-oas\n",
      "    do_training: true\n",
      "    do_testing: false\n",
      "    restore_from_path: null\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      num_nodes: 1\n",
      "      accelerator: gpu\n",
      "      precision: 16-mixed\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: null\n",
      "      max_steps: 200\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 100\n",
      "      limit_val_batches: 10\n",
      "      limit_test_batches: 500\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "      benchmark: false\n",
      "    exp_manager:\n",
      "      name: ${name}\n",
      "      exp_dir: /result/nemo_experiments/${.name}/${.wandb_logger_kwargs.name}\n",
      "      explicit_log_dir: ${.exp_dir}\n",
      "      create_wandb_logger: false\n",
      "      create_tensorboard_logger: true\n",
      "      wandb_logger_kwargs:\n",
      "        project: ${name}_pretraining\n",
      "        name: ${name}_pretraining\n",
      "        group: ${name}\n",
      "        job_type: Localhost_nodes_${trainer.num_nodes}_gpus_${trainer.devices}\n",
      "        notes: 'date: ${now:%y%m%d-%H%M%S}'\n",
      "        tags:\n",
      "        - ${name}\n",
      "        offline: false\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: val_loss\n",
      "        save_top_k: 10\n",
      "        mode: min\n",
      "        always_save_nemo: false\n",
      "        filename: megatron_bert--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "    model:\n",
      "      micro_batch_size: 8\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      seq_length: 512\n",
      "      max_position_embeddings: ${.seq_length}\n",
      "      encoder_seq_length: ${.seq_length}\n",
      "      num_layers: 6\n",
      "      hidden_size: 768\n",
      "      ffn_hidden_size: 3072\n",
      "      num_attention_heads: 12\n",
      "      init_method_std: 0.02\n",
      "      hidden_dropout: 0.1\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: true\n",
      "      bert_binary_head: false\n",
      "      resume_from_checkpoint: null\n",
      "      masked_softmax_fusion: true\n",
      "      tokenizer:\n",
      "        library: sentencepiece\n",
      "        type: null\n",
      "        model: /tokenizers/protein/esm1nv/vocab/protein_sequence_sentencepiece.model\n",
      "        vocab_file: /tokenizers/vocab/protein_sequence_sentencepiece.vocab\n",
      "        merge_file: null\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      seed: 1234\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: 1\n",
      "      data:\n",
      "        ngc_registry_target: uniref50_2022_05\n",
      "        ngc_registry_version: v23.06\n",
      "        data_prefix: ''\n",
      "        num_workers: 10\n",
      "        dataloader_type: single\n",
      "        reset_position_ids: false\n",
      "        reset_attention_mask: false\n",
      "        eod_mask_loss: false\n",
      "        masked_lm_prob: 0.15\n",
      "        short_seq_prob: 0.1\n",
      "        skip_lines: 0\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "        data_impl: csv_mmap\n",
      "        data_impl_kwargs:\n",
      "          csv_mmap:\n",
      "            header_lines: 1\n",
      "            newline_int: 10\n",
      "            workers: ${model.data.num_workers}\n",
      "            sort_dataset_paths: true\n",
      "            data_sep: ','\n",
      "            data_col: 1\n",
      "        use_upsampling: true\n",
      "        seed: ${model.seed}\n",
      "        max_seq_length: ${model.seq_length}\n",
      "        dataset_path: /data/OASpaired/processed/heavy\n",
      "        dataset:\n",
      "          train: x[000..005]\n",
      "          test: x[000..001]\n",
      "          val: x[000..001]\n",
      "        micro_batch_size: ${model.micro_batch_size}\n",
      "        modify_percent: 0.1\n",
      "        perturb_percent: 0.5\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0002\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 500\n",
      "          constant_steps: 50000\n",
      "          min_lr: 2.0e-05\n",
      "      global_batch_size: 8\n",
      "      precision: 16-mixed\n",
      "    \n",
      "[NeMo I 2023-08-17 16:22:30 pretrain_oas:21] ************** Starting Training ***********\n",
      "[NeMo I 2023-08-17 16:22:30 megatron_init:231] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2023-08-17 16:22:30 megatron_init:234] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2023-08-17 16:22:30 megatron_init:235] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-08-17 16:22:30 megatron_init:243] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-08-17 16:22:30 megatron_init:244] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-08-17 16:22:30 megatron_init:254] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-08-17 16:22:30 megatron_init:258] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-08-17 16:22:30 megatron_init:259] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-08-17 16:22:30 megatron_init:273] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-08-17 16:22:30 megatron_init:285] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-08-17 16:22:30 megatron_init:291] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-08-17 16:22:30 megatron_init:292] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-08-17 16:22:30 megatron_init:293] All embedding group ranks: [[0]]\n",
      "[NeMo I 2023-08-17 16:22:30 megatron_init:294] Rank 0 has embedding rank: 0\n",
      "23-08-17 16:22:30 - PID:335 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 1\n",
      "[NeMo I 2023-08-17 16:22:30 tokenizer_utils:191] Getting SentencePiece with model: /tokenizers/protein/esm1nv/vocab/protein_sequence_sentencepiece.model\n",
      "[NeMo I 2023-08-17 16:22:30 megatron_base_model:229] Padded vocab_size: 128, original vocab_size: 30, dummy tokens: 98.\n",
      "[NeMo W 2023-08-17 16:22:30 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/configuration_validator.py:175: UserWarning: The `batch_idx` argument in `ESM1nvModel.on_train_batch_start` hook may not match with the actual batch index when using a `dataloader_iter` argument in your `training_step`.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2023-08-17 16:22:30 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/configuration_validator.py:175: UserWarning: The `batch_idx` argument in `ESM1nvModel.on_train_batch_end` hook may not match with the actual batch index when using a `dataloader_iter` argument in your `training_step`.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.\n",
      "Added key: store_based_barrier_key:3 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 1 nodes.\n",
      "Added key: store_based_barrier_key:4 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 1 nodes.\n",
      "Added key: store_based_barrier_key:5 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 1 nodes.\n",
      "Added key: store_based_barrier_key:6 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 1 nodes.\n",
      "Added key: store_based_barrier_key:7 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 1 nodes.\n",
      "[NeMo I 2023-08-17 16:22:31 megatron_bert_model:563] Pipeline model parallel rank: 0, Tensor model parallel rank: 0, Number of model parameters on device: 4.36e+07. Total number of model parameters: 4.36e+07.\n",
      "[NeMo I 2023-08-17 16:22:31 esm1nv_model:96] Building Bert datasets.\n",
      "train:1600\n",
      "Loading data from /data/OASpaired/processed/heavy/train/x000.csv, /data/OASpaired/processed/heavy/train/x001.csv, /data/OASpaired/processed/heavy/train/x002.csv, /data/OASpaired/processed/heavy/train/x003.csv, /data/OASpaired/processed/heavy/train/x004.csv, /data/OASpaired/processed/heavy/train/x005.csv\n",
      "[NeMo I 2023-08-17 16:22:31 text_memmap_dataset:104] Building data files\n",
      "[NeMo I 2023-08-17 16:22:31 text_memmap_dataset:343] Processing 6 data files using 10 workers\n",
      "[NeMo I 2023-08-17 16:22:31 text_memmap_dataset:349] Time building 0 / 6 mem-mapped files: 0:00:00.148196\n",
      "[NeMo I 2023-08-17 16:22:31 text_memmap_dataset:114] Loading data files\n",
      "[NeMo I 2023-08-17 16:22:31 text_memmap_dataset:205] Loading /data/OASpaired/processed/heavy/train/x000.csv\n",
      "[NeMo I 2023-08-17 16:22:31 text_memmap_dataset:205] Loading /data/OASpaired/processed/heavy/train/x001.csv\n",
      "[NeMo I 2023-08-17 16:22:31 text_memmap_dataset:205] Loading /data/OASpaired/processed/heavy/train/x002.csv\n",
      "[NeMo I 2023-08-17 16:22:31 text_memmap_dataset:205] Loading /data/OASpaired/processed/heavy/train/x003.csv\n",
      "[NeMo I 2023-08-17 16:22:31 text_memmap_dataset:205] Loading /data/OASpaired/processed/heavy/train/x004.csv\n",
      "[NeMo I 2023-08-17 16:22:31 text_memmap_dataset:205] Loading /data/OASpaired/processed/heavy/train/x005.csv\n",
      "[NeMo I 2023-08-17 16:22:31 text_memmap_dataset:117] Time loading 6 mem-mapped files: 0:00:00.003227\n",
      "[NeMo I 2023-08-17 16:22:31 text_memmap_dataset:121] Computing global indices\n",
      "[NeMo I 2023-08-17 16:22:31 dataset_utils:1341]  > loading indexed mapping from /data/OASpaired/processed/heavy/train/__indexmap_1600mns_512msl_0.00ssp_1234s.npy\n",
      "[NeMo I 2023-08-17 16:22:31 dataset_utils:1344]     loaded indexed file in 0.001 seconds\n",
      "[NeMo I 2023-08-17 16:22:31 dataset_utils:1345]     total number of samples: 21110\n",
      "val:240\n",
      "Loading data from /data/OASpaired/processed/heavy/val/x000.csv, /data/OASpaired/processed/heavy/val/x001.csv\n",
      "[NeMo I 2023-08-17 16:22:31 text_memmap_dataset:104] Building data files\n",
      "[NeMo I 2023-08-17 16:22:31 text_memmap_dataset:343] Processing 2 data files using 10 workers\n",
      "[NeMo I 2023-08-17 16:22:31 text_memmap_dataset:349] Time building 0 / 2 mem-mapped files: 0:00:00.137560\n",
      "[NeMo I 2023-08-17 16:22:31 text_memmap_dataset:114] Loading data files\n",
      "[NeMo I 2023-08-17 16:22:31 text_memmap_dataset:205] Loading /data/OASpaired/processed/heavy/val/x000.csv\n",
      "[NeMo I 2023-08-17 16:22:31 text_memmap_dataset:205] Loading /data/OASpaired/processed/heavy/val/x001.csv\n",
      "[NeMo I 2023-08-17 16:22:31 text_memmap_dataset:117] Time loading 2 mem-mapped files: 0:00:00.001477\n",
      "[NeMo I 2023-08-17 16:22:31 text_memmap_dataset:121] Computing global indices\n",
      "[NeMo I 2023-08-17 16:22:31 dataset_utils:1341]  > loading indexed mapping from /data/OASpaired/processed/heavy/val/__indexmap_240mns_512msl_0.00ssp_1234s.npy\n",
      "[NeMo I 2023-08-17 16:22:31 dataset_utils:1344]     loaded indexed file in 0.000 seconds\n",
      "[NeMo I 2023-08-17 16:22:31 dataset_utils:1345]     total number of samples: 3754\n",
      "test:4000\n",
      "Loading data from /data/OASpaired/processed/heavy/test/x000.csv, /data/OASpaired/processed/heavy/test/x001.csv\n",
      "[NeMo I 2023-08-17 16:22:31 text_memmap_dataset:104] Building data files\n",
      "[NeMo I 2023-08-17 16:22:31 text_memmap_dataset:343] Processing 2 data files using 10 workers\n",
      "[NeMo I 2023-08-17 16:22:32 text_memmap_dataset:349] Time building 0 / 2 mem-mapped files: 0:00:00.165497\n",
      "[NeMo I 2023-08-17 16:22:32 text_memmap_dataset:114] Loading data files\n",
      "[NeMo I 2023-08-17 16:22:32 text_memmap_dataset:205] Loading /data/OASpaired/processed/heavy/test/x000.csv\n",
      "[NeMo I 2023-08-17 16:22:32 text_memmap_dataset:205] Loading /data/OASpaired/processed/heavy/test/x001.csv\n",
      "[NeMo I 2023-08-17 16:22:32 text_memmap_dataset:117] Time loading 2 mem-mapped files: 0:00:00.001370\n",
      "[NeMo I 2023-08-17 16:22:32 text_memmap_dataset:121] Computing global indices\n",
      "[NeMo I 2023-08-17 16:22:32 dataset_utils:1341]  > loading indexed mapping from /data/OASpaired/processed/heavy/test/__indexmap_4000mns_512msl_0.00ssp_1234s.npy\n",
      "[NeMo I 2023-08-17 16:22:32 dataset_utils:1344]     loaded indexed file in 0.000 seconds\n",
      "[NeMo I 2023-08-17 16:22:32 dataset_utils:1345]     total number of samples: 5822\n",
      "[NeMo I 2023-08-17 16:22:32 esm1nv_model:114] Length of train dataset: 1600\n",
      "[NeMo I 2023-08-17 16:22:32 esm1nv_model:115] Length of val dataset: 240\n",
      "[NeMo I 2023-08-17 16:22:32 esm1nv_model:116] Length of test dataset: 4000\n",
      "[NeMo I 2023-08-17 16:22:32 esm1nv_model:117] Finished building Bert datasets.\n",
      "[NeMo I 2023-08-17 16:22:32 megatron_bert_model:662] Setting up train dataloader with len(len(self._train_ds)): 1600 and consumed samples: 0\n",
      "[NeMo I 2023-08-17 16:22:32 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 1600 and consumed_samples: 0\n",
      "[NeMo I 2023-08-17 16:22:32 megatron_bert_model:670] Setting up validation dataloader with len(len(self._validation_ds)): 240 and consumed samples: 0\n",
      "[NeMo I 2023-08-17 16:22:32 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 240 and consumed_samples: 0\n",
      "[NeMo I 2023-08-17 16:22:32 megatron_bert_model:678] Setting up test dataloader with len(len(self._test_ds)): 4000 and consumed samples: 0\n",
      "[NeMo I 2023-08-17 16:22:32 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 4000 and consumed_samples: 0\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[NeMo I 2023-08-17 16:22:32 nlp_overrides:124] Configuring DDP for model parallelism.\n",
      "[NeMo I 2023-08-17 16:22:32 modelPT:722] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0002\n",
      "        weight_decay: 0.01\n",
      "    \n",
      "    Parameter Group 1\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0002\n",
      "        weight_decay: 0.0\n",
      "    )\n",
      "[NeMo I 2023-08-17 16:22:32 lr_scheduler:910] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7fc96434f790>\" \n",
      "    will be used during training (effective maximum steps = 200) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 500\n",
      "    constant_steps: 50000\n",
      "    min_lr: 2.0e-05\n",
      "    max_steps: 200\n",
      "    )\n",
      "\n",
      "  | Name                           | Type                     | Params\n",
      "----------------------------------------------------------------------------\n",
      "0 | model                          | BertModel                | 43.6 M\n",
      "1 | model.language_model           | TransformerLanguageModel | 43.0 M\n",
      "2 | model.language_model.embedding | Embedding                | 491 K \n",
      "3 | model.language_model.encoder   | ParallelTransformer      | 42.5 M\n",
      "4 | model.lm_head                  | BertLMHead               | 592 K \n",
      "5 | model.lm_head.dense            | Linear                   | 590 K \n",
      "6 | model.lm_head.layernorm        | MixedFusedLayerNorm      | 1.5 K \n",
      "----------------------------------------------------------------------------\n",
      "43.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "43.6 M    Total params\n",
      "87.225    Total estimated model params size (MB)\n",
      "\n",
      "Sanity Checking: 0it [00:00, ?it/s][NeMo W 2023-08-17 16:22:32 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:401: UserWarning: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "\n",
      "Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:01<00:01,  1.44s/it]\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:01<00:00,  1.36it/s][NeMo W 2023-08-17 16:22:33 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:232: UserWarning: You called `self.log('consumed_samples', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-08-17 16:22:33 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-08-17 16:22:33 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val_loss_ECE', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-08-17 16:22:33 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('consumed_samples', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "\n",
      "                                                                           [NeMo W 2023-08-17 16:22:33 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/fit_loop.py:344: UserWarning: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "\n",
      "\n",
      "Training: 0it [00:00, ?it/s]\n",
      "Training:   0%|          | 0/220 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/220 [00:00<?, ?it/s] [NeMo W 2023-08-17 16:22:35 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:232: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-08-17 16:22:35 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:232: UserWarning: You called `self.log('consumed_samples', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-08-17 16:22:35 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "      warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "    \n",
      "\n",
      "Epoch 0:   0%|          | 1/220 [00:01<06:47,  1.86s/it]\n",
      "Epoch 0:   0%|          | 1/220 [00:01<06:47,  1.86s/it, loss=4.58, v_num=, reduced_train_loss=4.580, global_step=0.000, consumed_samples=0.000]\n",
      "Epoch 0:   1%|          | 2/220 [00:01<03:29,  1.04it/s, loss=4.58, v_num=, reduced_train_loss=4.580, global_step=0.000, consumed_samples=0.000]\n",
      "Epoch 0:   1%|          | 2/220 [00:01<03:29,  1.04it/s, loss=4.59, v_num=, reduced_train_loss=4.600, global_step=1.000, consumed_samples=8.000]\n",
      "Epoch 0:   1%|▏         | 3/220 [00:01<02:22,  1.53it/s, loss=4.59, v_num=, reduced_train_loss=4.600, global_step=1.000, consumed_samples=8.000]\n",
      "Epoch 0:   1%|▏         | 3/220 [00:01<02:22,  1.53it/s, loss=4.59, v_num=, reduced_train_loss=4.590, global_step=2.000, consumed_samples=16.00]\n",
      "Epoch 0:   2%|▏         | 4/220 [00:02<01:48,  1.99it/s, loss=4.59, v_num=, reduced_train_loss=4.590, global_step=2.000, consumed_samples=16.00]\n",
      "Epoch 0:   2%|▏         | 4/220 [00:02<01:48,  1.99it/s, loss=4.6, v_num=, reduced_train_loss=4.620, global_step=3.000, consumed_samples=24.00] \n",
      "Epoch 0:   2%|▏         | 5/220 [00:02<01:28,  2.44it/s, loss=4.6, v_num=, reduced_train_loss=4.620, global_step=3.000, consumed_samples=24.00]\n",
      "Epoch 0:   2%|▏         | 5/220 [00:02<01:28,  2.44it/s, loss=4.59, v_num=, reduced_train_loss=4.580, global_step=4.000, consumed_samples=32.00]\n",
      "Epoch 0:   3%|▎         | 6/220 [00:02<01:14,  2.87it/s, loss=4.59, v_num=, reduced_train_loss=4.580, global_step=4.000, consumed_samples=32.00]\n",
      "Epoch 0:   3%|▎         | 6/220 [00:02<01:14,  2.87it/s, loss=4.59, v_num=, reduced_train_loss=4.560, global_step=5.000, consumed_samples=40.00]\n",
      "Epoch 0:   3%|▎         | 7/220 [00:02<01:05,  3.27it/s, loss=4.59, v_num=, reduced_train_loss=4.560, global_step=5.000, consumed_samples=40.00]\n",
      "Epoch 0:   3%|▎         | 7/220 [00:02<01:05,  3.27it/s, loss=4.58, v_num=, reduced_train_loss=4.550, global_step=6.000, consumed_samples=48.00]\n",
      "Epoch 0:   4%|▎         | 8/220 [00:02<00:58,  3.64it/s, loss=4.58, v_num=, reduced_train_loss=4.550, global_step=6.000, consumed_samples=48.00]\n",
      "Epoch 0:   4%|▎         | 8/220 [00:02<00:58,  3.64it/s, loss=4.59, v_num=, reduced_train_loss=4.600, global_step=7.000, consumed_samples=56.00]\n",
      "Epoch 0:   4%|▍         | 9/220 [00:02<00:52,  4.00it/s, loss=4.59, v_num=, reduced_train_loss=4.600, global_step=7.000, consumed_samples=56.00]\n",
      "Epoch 0:   4%|▍         | 9/220 [00:02<00:52,  4.00it/s, loss=4.59, v_num=, reduced_train_loss=4.580, global_step=8.000, consumed_samples=64.00]\n",
      "Epoch 0:   5%|▍         | 10/220 [00:02<00:48,  4.35it/s, loss=4.59, v_num=, reduced_train_loss=4.580, global_step=8.000, consumed_samples=64.00]\n",
      "Epoch 0:   5%|▍         | 10/220 [00:02<00:48,  4.35it/s, loss=4.59, v_num=, reduced_train_loss=4.620, global_step=9.000, consumed_samples=72.00]\n",
      "Epoch 0:   5%|▌         | 11/220 [00:02<00:44,  4.68it/s, loss=4.59, v_num=, reduced_train_loss=4.620, global_step=9.000, consumed_samples=72.00]\n",
      "Epoch 0:   5%|▌         | 11/220 [00:02<00:44,  4.67it/s, loss=4.59, v_num=, reduced_train_loss=4.570, global_step=10.00, consumed_samples=80.00]\n",
      "Epoch 0:   5%|▌         | 12/220 [00:02<00:41,  4.98it/s, loss=4.59, v_num=, reduced_train_loss=4.570, global_step=10.00, consumed_samples=80.00]\n",
      "Epoch 0:   5%|▌         | 12/220 [00:02<00:41,  4.98it/s, loss=4.59, v_num=, reduced_train_loss=4.610, global_step=11.00, consumed_samples=88.00]\n",
      "Epoch 0:   6%|▌         | 13/220 [00:02<00:39,  5.29it/s, loss=4.59, v_num=, reduced_train_loss=4.610, global_step=11.00, consumed_samples=88.00]\n",
      "Epoch 0:   6%|▌         | 13/220 [00:02<00:39,  5.29it/s, loss=4.59, v_num=, reduced_train_loss=4.600, global_step=12.00, consumed_samples=96.00]\n",
      "Epoch 0:   6%|▋         | 14/220 [00:02<00:36,  5.60it/s, loss=4.59, v_num=, reduced_train_loss=4.600, global_step=12.00, consumed_samples=96.00]\n",
      "Epoch 0:   6%|▋         | 14/220 [00:02<00:36,  5.60it/s, loss=4.59, v_num=, reduced_train_loss=4.590, global_step=13.00, consumed_samples=104.0]\n",
      "Epoch 0:   7%|▋         | 15/220 [00:02<00:34,  5.90it/s, loss=4.59, v_num=, reduced_train_loss=4.590, global_step=13.00, consumed_samples=104.0]\n",
      "Epoch 0:   7%|▋         | 15/220 [00:02<00:34,  5.90it/s, loss=4.59, v_num=, reduced_train_loss=4.610, global_step=14.00, consumed_samples=112.0]\n",
      "Epoch 0:   7%|▋         | 16/220 [00:02<00:32,  6.19it/s, loss=4.59, v_num=, reduced_train_loss=4.610, global_step=14.00, consumed_samples=112.0]\n",
      "Epoch 0:   7%|▋         | 16/220 [00:02<00:32,  6.19it/s, loss=4.59, v_num=, reduced_train_loss=4.600, global_step=15.00, consumed_samples=120.0]\n",
      "Epoch 0:   8%|▊         | 17/220 [00:02<00:31,  6.48it/s, loss=4.59, v_num=, reduced_train_loss=4.600, global_step=15.00, consumed_samples=120.0]\n",
      "Epoch 0:   8%|▊         | 17/220 [00:02<00:31,  6.48it/s, loss=4.59, v_num=, reduced_train_loss=4.590, global_step=16.00, consumed_samples=128.0]\n",
      "Epoch 0:   8%|▊         | 18/220 [00:02<00:29,  6.75it/s, loss=4.59, v_num=, reduced_train_loss=4.590, global_step=16.00, consumed_samples=128.0]\n",
      "Epoch 0:   8%|▊         | 18/220 [00:02<00:29,  6.75it/s, loss=4.59, v_num=, reduced_train_loss=4.600, global_step=17.00, consumed_samples=136.0]\n",
      "Epoch 0:   9%|▊         | 19/220 [00:02<00:28,  7.01it/s, loss=4.59, v_num=, reduced_train_loss=4.600, global_step=17.00, consumed_samples=136.0]\n",
      "Epoch 0:   9%|▊         | 19/220 [00:02<00:28,  7.01it/s, loss=4.59, v_num=, reduced_train_loss=4.590, global_step=18.00, consumed_samples=144.0]\n",
      "Epoch 0:   9%|▉         | 20/220 [00:02<00:27,  7.26it/s, loss=4.59, v_num=, reduced_train_loss=4.590, global_step=18.00, consumed_samples=144.0]\n",
      "Epoch 0:   9%|▉         | 20/220 [00:02<00:27,  7.26it/s, loss=4.59, v_num=, reduced_train_loss=4.590, global_step=19.00, consumed_samples=152.0]\n",
      "Epoch 0:  10%|▉         | 21/220 [00:02<00:26,  7.50it/s, loss=4.59, v_num=, reduced_train_loss=4.590, global_step=19.00, consumed_samples=152.0]\n",
      "Epoch 0:  10%|▉         | 21/220 [00:02<00:26,  7.50it/s, loss=4.59, v_num=, reduced_train_loss=4.550, global_step=20.00, consumed_samples=160.0]\n",
      "Epoch 0:  10%|█         | 22/220 [00:02<00:25,  7.74it/s, loss=4.59, v_num=, reduced_train_loss=4.550, global_step=20.00, consumed_samples=160.0]\n",
      "Epoch 0:  10%|█         | 22/220 [00:02<00:25,  7.74it/s, loss=4.58, v_num=, reduced_train_loss=4.390, global_step=21.00, consumed_samples=168.0]\n",
      "Epoch 0:  10%|█         | 23/220 [00:02<00:24,  7.94it/s, loss=4.58, v_num=, reduced_train_loss=4.390, global_step=21.00, consumed_samples=168.0]\n",
      "Epoch 0:  10%|█         | 23/220 [00:02<00:24,  7.94it/s, loss=4.56, v_num=, reduced_train_loss=4.200, global_step=22.00, consumed_samples=176.0]\n",
      "Epoch 0:  11%|█         | 24/220 [00:02<00:24,  8.14it/s, loss=4.56, v_num=, reduced_train_loss=4.200, global_step=22.00, consumed_samples=176.0]\n",
      "Epoch 0:  11%|█         | 24/220 [00:02<00:24,  8.14it/s, loss=4.53, v_num=, reduced_train_loss=3.920, global_step=23.00, consumed_samples=184.0]\n",
      "Epoch 0:  11%|█▏        | 25/220 [00:02<00:23,  8.34it/s, loss=4.53, v_num=, reduced_train_loss=3.920, global_step=23.00, consumed_samples=184.0]\n",
      "Epoch 0:  11%|█▏        | 25/220 [00:02<00:23,  8.34it/s, loss=4.47, v_num=, reduced_train_loss=3.550, global_step=24.00, consumed_samples=192.0]\n",
      "Epoch 0:  12%|█▏        | 26/220 [00:03<00:22,  8.55it/s, loss=4.47, v_num=, reduced_train_loss=3.550, global_step=24.00, consumed_samples=192.0]\n",
      "Epoch 0:  12%|█▏        | 26/220 [00:03<00:22,  8.55it/s, loss=4.41, v_num=, reduced_train_loss=3.270, global_step=25.00, consumed_samples=200.0]\n",
      "Epoch 0:  12%|█▏        | 27/220 [00:03<00:22,  8.74it/s, loss=4.41, v_num=, reduced_train_loss=3.270, global_step=25.00, consumed_samples=200.0]\n",
      "Epoch 0:  12%|█▏        | 27/220 [00:03<00:22,  8.74it/s, loss=4.33, v_num=, reduced_train_loss=2.980, global_step=26.00, consumed_samples=208.0]\n",
      "Epoch 0:  13%|█▎        | 28/220 [00:03<00:21,  8.94it/s, loss=4.33, v_num=, reduced_train_loss=2.980, global_step=26.00, consumed_samples=208.0]\n",
      "Epoch 0:  13%|█▎        | 28/220 [00:03<00:21,  8.94it/s, loss=4.24, v_num=, reduced_train_loss=2.740, global_step=27.00, consumed_samples=216.0]\n",
      "Epoch 0:  13%|█▎        | 29/220 [00:03<00:20,  9.10it/s, loss=4.24, v_num=, reduced_train_loss=2.740, global_step=27.00, consumed_samples=216.0]\n",
      "Epoch 0:  13%|█▎        | 29/220 [00:03<00:20,  9.10it/s, loss=4.13, v_num=, reduced_train_loss=2.460, global_step=28.00, consumed_samples=224.0]\n",
      "Epoch 0:  14%|█▎        | 30/220 [00:03<00:20,  9.27it/s, loss=4.13, v_num=, reduced_train_loss=2.460, global_step=28.00, consumed_samples=224.0]\n",
      "Epoch 0:  14%|█▎        | 30/220 [00:03<00:20,  9.27it/s, loss=4.01, v_num=, reduced_train_loss=2.240, global_step=29.00, consumed_samples=232.0]\n",
      "Epoch 0:  14%|█▍        | 31/220 [00:03<00:20,  9.41it/s, loss=4.01, v_num=, reduced_train_loss=2.240, global_step=29.00, consumed_samples=232.0]\n",
      "Epoch 0:  14%|█▍        | 31/220 [00:03<00:20,  9.41it/s, loss=3.89, v_num=, reduced_train_loss=2.060, global_step=30.00, consumed_samples=240.0]\n",
      "Epoch 0:  15%|█▍        | 32/220 [00:03<00:19,  9.58it/s, loss=3.89, v_num=, reduced_train_loss=2.060, global_step=30.00, consumed_samples=240.0]\n",
      "Epoch 0:  15%|█▍        | 32/220 [00:03<00:19,  9.58it/s, loss=3.76, v_num=, reduced_train_loss=1.970, global_step=31.00, consumed_samples=248.0]\n",
      "Epoch 0:  15%|█▌        | 33/220 [00:03<00:19,  9.75it/s, loss=3.76, v_num=, reduced_train_loss=1.970, global_step=31.00, consumed_samples=248.0]\n",
      "Epoch 0:  15%|█▌        | 33/220 [00:03<00:19,  9.75it/s, loss=3.62, v_num=, reduced_train_loss=1.900, global_step=32.00, consumed_samples=256.0]\n",
      "Epoch 0:  15%|█▌        | 34/220 [00:03<00:18,  9.91it/s, loss=3.62, v_num=, reduced_train_loss=1.900, global_step=32.00, consumed_samples=256.0]\n",
      "Epoch 0:  15%|█▌        | 34/220 [00:03<00:18,  9.91it/s, loss=3.48, v_num=, reduced_train_loss=1.800, global_step=33.00, consumed_samples=264.0]\n",
      "Epoch 0:  16%|█▌        | 35/220 [00:03<00:18, 10.07it/s, loss=3.48, v_num=, reduced_train_loss=1.800, global_step=33.00, consumed_samples=264.0]\n",
      "Epoch 0:  16%|█▌        | 35/220 [00:03<00:18, 10.07it/s, loss=3.34, v_num=, reduced_train_loss=1.750, global_step=34.00, consumed_samples=272.0]\n",
      "Epoch 0:  16%|█▋        | 36/220 [00:03<00:18, 10.22it/s, loss=3.34, v_num=, reduced_train_loss=1.750, global_step=34.00, consumed_samples=272.0]\n",
      "Epoch 0:  16%|█▋        | 36/220 [00:03<00:18, 10.22it/s, loss=3.19, v_num=, reduced_train_loss=1.680, global_step=35.00, consumed_samples=280.0]\n",
      "Epoch 0:  17%|█▋        | 37/220 [00:03<00:17, 10.37it/s, loss=3.19, v_num=, reduced_train_loss=1.680, global_step=35.00, consumed_samples=280.0]\n",
      "Epoch 0:  17%|█▋        | 37/220 [00:03<00:17, 10.37it/s, loss=3.05, v_num=, reduced_train_loss=1.660, global_step=36.00, consumed_samples=288.0]\n",
      "Epoch 0:  17%|█▋        | 38/220 [00:03<00:17, 10.52it/s, loss=3.05, v_num=, reduced_train_loss=1.660, global_step=36.00, consumed_samples=288.0]\n",
      "Epoch 0:  17%|█▋        | 38/220 [00:03<00:17, 10.52it/s, loss=2.9, v_num=, reduced_train_loss=1.610, global_step=37.00, consumed_samples=296.0] \n",
      "Epoch 0:  18%|█▊        | 39/220 [00:03<00:16, 10.65it/s, loss=2.9, v_num=, reduced_train_loss=1.610, global_step=37.00, consumed_samples=296.0]\n",
      "Epoch 0:  18%|█▊        | 39/220 [00:03<00:16, 10.65it/s, loss=2.75, v_num=, reduced_train_loss=1.590, global_step=38.00, consumed_samples=304.0]\n",
      "Epoch 0:  18%|█▊        | 40/220 [00:03<00:16, 10.76it/s, loss=2.75, v_num=, reduced_train_loss=1.590, global_step=38.00, consumed_samples=304.0]\n",
      "Epoch 0:  18%|█▊        | 40/220 [00:03<00:16, 10.76it/s, loss=2.59, v_num=, reduced_train_loss=1.570, global_step=39.00, consumed_samples=312.0]\n",
      "Epoch 0:  19%|█▊        | 41/220 [00:03<00:16, 10.89it/s, loss=2.59, v_num=, reduced_train_loss=1.570, global_step=39.00, consumed_samples=312.0]\n",
      "Epoch 0:  19%|█▊        | 41/220 [00:03<00:16, 10.89it/s, loss=2.44, v_num=, reduced_train_loss=1.550, global_step=40.00, consumed_samples=320.0]\n",
      "Epoch 0:  19%|█▉        | 42/220 [00:03<00:16, 11.02it/s, loss=2.44, v_num=, reduced_train_loss=1.550, global_step=40.00, consumed_samples=320.0]\n",
      "Epoch 0:  19%|█▉        | 42/220 [00:03<00:16, 11.02it/s, loss=2.3, v_num=, reduced_train_loss=1.530, global_step=41.00, consumed_samples=328.0] \n",
      "Epoch 0:  20%|█▉        | 43/220 [00:03<00:15, 11.15it/s, loss=2.3, v_num=, reduced_train_loss=1.530, global_step=41.00, consumed_samples=328.0]\n",
      "Epoch 0:  20%|█▉        | 43/220 [00:03<00:15, 11.15it/s, loss=2.17, v_num=, reduced_train_loss=1.510, global_step=42.00, consumed_samples=336.0]\n",
      "Epoch 0:  20%|██        | 44/220 [00:03<00:15, 11.27it/s, loss=2.17, v_num=, reduced_train_loss=1.510, global_step=42.00, consumed_samples=336.0]\n",
      "Epoch 0:  20%|██        | 44/220 [00:03<00:15, 11.27it/s, loss=2.05, v_num=, reduced_train_loss=1.520, global_step=43.00, consumed_samples=344.0]\n",
      "Epoch 0:  20%|██        | 45/220 [00:03<00:15, 11.39it/s, loss=2.05, v_num=, reduced_train_loss=1.520, global_step=43.00, consumed_samples=344.0]\n",
      "Epoch 0:  20%|██        | 45/220 [00:03<00:15, 11.39it/s, loss=1.94, v_num=, reduced_train_loss=1.500, global_step=44.00, consumed_samples=352.0]\n",
      "Epoch 0:  21%|██        | 46/220 [00:03<00:15, 11.50it/s, loss=1.94, v_num=, reduced_train_loss=1.500, global_step=44.00, consumed_samples=352.0]\n",
      "Epoch 0:  21%|██        | 46/220 [00:03<00:15, 11.50it/s, loss=1.86, v_num=, reduced_train_loss=1.500, global_step=45.00, consumed_samples=360.0]\n",
      "Epoch 0:  21%|██▏       | 47/220 [00:04<00:14, 11.62it/s, loss=1.86, v_num=, reduced_train_loss=1.500, global_step=45.00, consumed_samples=360.0]\n",
      "Epoch 0:  21%|██▏       | 47/220 [00:04<00:14, 11.62it/s, loss=1.78, v_num=, reduced_train_loss=1.490, global_step=46.00, consumed_samples=368.0]\n",
      "Epoch 0:  22%|██▏       | 48/220 [00:04<00:14, 11.73it/s, loss=1.78, v_num=, reduced_train_loss=1.490, global_step=46.00, consumed_samples=368.0]\n",
      "Epoch 0:  22%|██▏       | 48/220 [00:04<00:14, 11.73it/s, loss=1.72, v_num=, reduced_train_loss=1.470, global_step=47.00, consumed_samples=376.0]\n",
      "Epoch 0:  22%|██▏       | 49/220 [00:04<00:14, 11.85it/s, loss=1.72, v_num=, reduced_train_loss=1.470, global_step=47.00, consumed_samples=376.0]\n",
      "Epoch 0:  22%|██▏       | 49/220 [00:04<00:14, 11.85it/s, loss=1.67, v_num=, reduced_train_loss=1.480, global_step=48.00, consumed_samples=384.0]\n",
      "Epoch 0:  23%|██▎       | 50/220 [00:04<00:14, 11.96it/s, loss=1.67, v_num=, reduced_train_loss=1.480, global_step=48.00, consumed_samples=384.0]\n",
      "Epoch 0:  23%|██▎       | 50/220 [00:04<00:14, 11.96it/s, loss=1.63, v_num=, reduced_train_loss=1.490, global_step=49.00, consumed_samples=392.0]\n",
      "Epoch 0:  23%|██▎       | 51/220 [00:04<00:14, 12.06it/s, loss=1.63, v_num=, reduced_train_loss=1.490, global_step=49.00, consumed_samples=392.0]\n",
      "Epoch 0:  23%|██▎       | 51/220 [00:04<00:14, 12.06it/s, loss=1.6, v_num=, reduced_train_loss=1.490, global_step=50.00, consumed_samples=400.0] \n",
      "Epoch 0:  24%|██▎       | 52/220 [00:04<00:13, 12.16it/s, loss=1.6, v_num=, reduced_train_loss=1.490, global_step=50.00, consumed_samples=400.0]\n",
      "Epoch 0:  24%|██▎       | 52/220 [00:04<00:13, 12.16it/s, loss=1.58, v_num=, reduced_train_loss=1.460, global_step=51.00, consumed_samples=408.0]\n",
      "Epoch 0:  24%|██▍       | 53/220 [00:04<00:13, 12.25it/s, loss=1.58, v_num=, reduced_train_loss=1.460, global_step=51.00, consumed_samples=408.0]\n",
      "Epoch 0:  24%|██▍       | 53/220 [00:04<00:13, 12.25it/s, loss=1.56, v_num=, reduced_train_loss=1.490, global_step=52.00, consumed_samples=416.0]\n",
      "Epoch 0:  25%|██▍       | 54/220 [00:04<00:13, 12.34it/s, loss=1.56, v_num=, reduced_train_loss=1.490, global_step=52.00, consumed_samples=416.0]\n",
      "Epoch 0:  25%|██▍       | 54/220 [00:04<00:13, 12.34it/s, loss=1.54, v_num=, reduced_train_loss=1.460, global_step=53.00, consumed_samples=424.0]\n",
      "Epoch 0:  25%|██▌       | 55/220 [00:04<00:13, 12.44it/s, loss=1.54, v_num=, reduced_train_loss=1.460, global_step=53.00, consumed_samples=424.0]\n",
      "Epoch 0:  25%|██▌       | 55/220 [00:04<00:13, 12.44it/s, loss=1.52, v_num=, reduced_train_loss=1.440, global_step=54.00, consumed_samples=432.0]\n",
      "Epoch 0:  25%|██▌       | 56/220 [00:04<00:13, 12.54it/s, loss=1.52, v_num=, reduced_train_loss=1.440, global_step=54.00, consumed_samples=432.0]\n",
      "Epoch 0:  25%|██▌       | 56/220 [00:04<00:13, 12.54it/s, loss=1.51, v_num=, reduced_train_loss=1.480, global_step=55.00, consumed_samples=440.0]\n",
      "Epoch 0:  26%|██▌       | 57/220 [00:04<00:12, 12.63it/s, loss=1.51, v_num=, reduced_train_loss=1.480, global_step=55.00, consumed_samples=440.0]\n",
      "Epoch 0:  26%|██▌       | 57/220 [00:04<00:12, 12.63it/s, loss=1.5, v_num=, reduced_train_loss=1.450, global_step=56.00, consumed_samples=448.0] \n",
      "Epoch 0:  26%|██▋       | 58/220 [00:04<00:12, 12.71it/s, loss=1.5, v_num=, reduced_train_loss=1.450, global_step=56.00, consumed_samples=448.0]\n",
      "Epoch 0:  26%|██▋       | 58/220 [00:04<00:12, 12.71it/s, loss=1.5, v_num=, reduced_train_loss=1.460, global_step=57.00, consumed_samples=456.0]\n",
      "Epoch 0:  27%|██▋       | 59/220 [00:04<00:12, 12.80it/s, loss=1.5, v_num=, reduced_train_loss=1.460, global_step=57.00, consumed_samples=456.0]\n",
      "Epoch 0:  27%|██▋       | 59/220 [00:04<00:12, 12.80it/s, loss=1.49, v_num=, reduced_train_loss=1.460, global_step=58.00, consumed_samples=464.0]\n",
      "Epoch 0:  27%|██▋       | 60/220 [00:04<00:12, 12.89it/s, loss=1.49, v_num=, reduced_train_loss=1.460, global_step=58.00, consumed_samples=464.0]\n",
      "Epoch 0:  27%|██▋       | 60/220 [00:04<00:12, 12.89it/s, loss=1.48, v_num=, reduced_train_loss=1.460, global_step=59.00, consumed_samples=472.0]\n",
      "Epoch 0:  28%|██▊       | 61/220 [00:04<00:12, 12.97it/s, loss=1.48, v_num=, reduced_train_loss=1.460, global_step=59.00, consumed_samples=472.0]\n",
      "Epoch 0:  28%|██▊       | 61/220 [00:04<00:12, 12.97it/s, loss=1.48, v_num=, reduced_train_loss=1.440, global_step=60.00, consumed_samples=480.0]\n",
      "Epoch 0:  28%|██▊       | 62/220 [00:04<00:12, 13.06it/s, loss=1.48, v_num=, reduced_train_loss=1.440, global_step=60.00, consumed_samples=480.0]\n",
      "Epoch 0:  28%|██▊       | 62/220 [00:04<00:12, 13.06it/s, loss=1.48, v_num=, reduced_train_loss=1.490, global_step=61.00, consumed_samples=488.0]\n",
      "Epoch 0:  29%|██▊       | 63/220 [00:04<00:11, 13.15it/s, loss=1.48, v_num=, reduced_train_loss=1.490, global_step=61.00, consumed_samples=488.0]\n",
      "Epoch 0:  29%|██▊       | 63/220 [00:04<00:11, 13.15it/s, loss=1.48, v_num=, reduced_train_loss=1.490, global_step=62.00, consumed_samples=496.0]\n",
      "Epoch 0:  29%|██▉       | 64/220 [00:04<00:11, 13.23it/s, loss=1.48, v_num=, reduced_train_loss=1.490, global_step=62.00, consumed_samples=496.0]\n",
      "Epoch 0:  29%|██▉       | 64/220 [00:04<00:11, 13.23it/s, loss=1.47, v_num=, reduced_train_loss=1.430, global_step=63.00, consumed_samples=504.0]\n",
      "Epoch 0:  30%|██▉       | 65/220 [00:04<00:11, 13.31it/s, loss=1.47, v_num=, reduced_train_loss=1.430, global_step=63.00, consumed_samples=504.0]\n",
      "Epoch 0:  30%|██▉       | 65/220 [00:04<00:11, 13.30it/s, loss=1.47, v_num=, reduced_train_loss=1.450, global_step=64.00, consumed_samples=512.0]\n",
      "Epoch 0:  30%|███       | 66/220 [00:04<00:11, 13.38it/s, loss=1.47, v_num=, reduced_train_loss=1.450, global_step=64.00, consumed_samples=512.0]\n",
      "Epoch 0:  30%|███       | 66/220 [00:04<00:11, 13.38it/s, loss=1.47, v_num=, reduced_train_loss=1.460, global_step=65.00, consumed_samples=520.0]\n",
      "Epoch 0:  30%|███       | 67/220 [00:04<00:11, 13.45it/s, loss=1.47, v_num=, reduced_train_loss=1.460, global_step=65.00, consumed_samples=520.0]\n",
      "Epoch 0:  30%|███       | 67/220 [00:04<00:11, 13.45it/s, loss=1.46, v_num=, reduced_train_loss=1.450, global_step=66.00, consumed_samples=528.0]\n",
      "Epoch 0:  31%|███       | 68/220 [00:05<00:11, 13.53it/s, loss=1.46, v_num=, reduced_train_loss=1.450, global_step=66.00, consumed_samples=528.0]\n",
      "Epoch 0:  31%|███       | 68/220 [00:05<00:11, 13.53it/s, loss=1.46, v_num=, reduced_train_loss=1.440, global_step=67.00, consumed_samples=536.0]\n",
      "Epoch 0:  31%|███▏      | 69/220 [00:05<00:11, 13.60it/s, loss=1.46, v_num=, reduced_train_loss=1.440, global_step=67.00, consumed_samples=536.0]\n",
      "Epoch 0:  31%|███▏      | 69/220 [00:05<00:11, 13.60it/s, loss=1.46, v_num=, reduced_train_loss=1.450, global_step=68.00, consumed_samples=544.0]\n",
      "Epoch 0:  32%|███▏      | 70/220 [00:05<00:10, 13.67it/s, loss=1.46, v_num=, reduced_train_loss=1.450, global_step=68.00, consumed_samples=544.0]\n",
      "Epoch 0:  32%|███▏      | 70/220 [00:05<00:10, 13.67it/s, loss=1.46, v_num=, reduced_train_loss=1.480, global_step=69.00, consumed_samples=552.0]\n",
      "Epoch 0:  32%|███▏      | 71/220 [00:05<00:10, 13.68it/s, loss=1.46, v_num=, reduced_train_loss=1.480, global_step=69.00, consumed_samples=552.0]\n",
      "Epoch 0:  32%|███▏      | 71/220 [00:05<00:10, 13.68it/s, loss=1.46, v_num=, reduced_train_loss=1.450, global_step=70.00, consumed_samples=560.0]\n",
      "Epoch 0:  33%|███▎      | 72/220 [00:05<00:10, 13.72it/s, loss=1.46, v_num=, reduced_train_loss=1.450, global_step=70.00, consumed_samples=560.0]\n",
      "Epoch 0:  33%|███▎      | 72/220 [00:05<00:10, 13.72it/s, loss=1.46, v_num=, reduced_train_loss=1.480, global_step=71.00, consumed_samples=568.0]\n",
      "Epoch 0:  33%|███▎      | 73/220 [00:05<00:10, 13.78it/s, loss=1.46, v_num=, reduced_train_loss=1.480, global_step=71.00, consumed_samples=568.0]\n",
      "Epoch 0:  33%|███▎      | 73/220 [00:05<00:10, 13.78it/s, loss=1.46, v_num=, reduced_train_loss=1.420, global_step=72.00, consumed_samples=576.0]\n",
      "Epoch 0:  34%|███▎      | 74/220 [00:05<00:10, 13.85it/s, loss=1.46, v_num=, reduced_train_loss=1.420, global_step=72.00, consumed_samples=576.0]\n",
      "Epoch 0:  34%|███▎      | 74/220 [00:05<00:10, 13.85it/s, loss=1.46, v_num=, reduced_train_loss=1.450, global_step=73.00, consumed_samples=584.0]\n",
      "Epoch 0:  34%|███▍      | 75/220 [00:05<00:10, 13.90it/s, loss=1.46, v_num=, reduced_train_loss=1.450, global_step=73.00, consumed_samples=584.0]\n",
      "Epoch 0:  34%|███▍      | 75/220 [00:05<00:10, 13.90it/s, loss=1.46, v_num=, reduced_train_loss=1.470, global_step=74.00, consumed_samples=592.0]\n",
      "Epoch 0:  35%|███▍      | 76/220 [00:05<00:10, 13.96it/s, loss=1.46, v_num=, reduced_train_loss=1.470, global_step=74.00, consumed_samples=592.0]\n",
      "Epoch 0:  35%|███▍      | 76/220 [00:05<00:10, 13.96it/s, loss=1.46, v_num=, reduced_train_loss=1.490, global_step=75.00, consumed_samples=600.0]\n",
      "Epoch 0:  35%|███▌      | 77/220 [00:05<00:10, 14.00it/s, loss=1.46, v_num=, reduced_train_loss=1.490, global_step=75.00, consumed_samples=600.0]\n",
      "Epoch 0:  35%|███▌      | 77/220 [00:05<00:10, 14.00it/s, loss=1.46, v_num=, reduced_train_loss=1.450, global_step=76.00, consumed_samples=608.0]\n",
      "Epoch 0:  35%|███▌      | 78/220 [00:05<00:10, 14.05it/s, loss=1.46, v_num=, reduced_train_loss=1.450, global_step=76.00, consumed_samples=608.0]\n",
      "Epoch 0:  35%|███▌      | 78/220 [00:05<00:10, 14.05it/s, loss=1.46, v_num=, reduced_train_loss=1.440, global_step=77.00, consumed_samples=616.0]\n",
      "Epoch 0:  36%|███▌      | 79/220 [00:05<00:09, 14.12it/s, loss=1.46, v_num=, reduced_train_loss=1.440, global_step=77.00, consumed_samples=616.0]\n",
      "Epoch 0:  36%|███▌      | 79/220 [00:05<00:09, 14.12it/s, loss=1.46, v_num=, reduced_train_loss=1.440, global_step=78.00, consumed_samples=624.0]\n",
      "Epoch 0:  36%|███▋      | 80/220 [00:05<00:09, 14.18it/s, loss=1.46, v_num=, reduced_train_loss=1.440, global_step=78.00, consumed_samples=624.0]\n",
      "Epoch 0:  36%|███▋      | 80/220 [00:05<00:09, 14.17it/s, loss=1.46, v_num=, reduced_train_loss=1.460, global_step=79.00, consumed_samples=632.0]\n",
      "Epoch 0:  37%|███▋      | 81/220 [00:05<00:09, 14.23it/s, loss=1.46, v_num=, reduced_train_loss=1.460, global_step=79.00, consumed_samples=632.0]\n",
      "Epoch 0:  37%|███▋      | 81/220 [00:05<00:09, 14.23it/s, loss=1.46, v_num=, reduced_train_loss=1.450, global_step=80.00, consumed_samples=640.0]\n",
      "Epoch 0:  37%|███▋      | 82/220 [00:05<00:09, 14.28it/s, loss=1.46, v_num=, reduced_train_loss=1.450, global_step=80.00, consumed_samples=640.0]\n",
      "Epoch 0:  37%|███▋      | 82/220 [00:05<00:09, 14.28it/s, loss=1.46, v_num=, reduced_train_loss=1.440, global_step=81.00, consumed_samples=648.0]\n",
      "Epoch 0:  38%|███▊      | 83/220 [00:05<00:09, 14.34it/s, loss=1.46, v_num=, reduced_train_loss=1.440, global_step=81.00, consumed_samples=648.0]\n",
      "Epoch 0:  38%|███▊      | 83/220 [00:05<00:09, 14.34it/s, loss=1.46, v_num=, reduced_train_loss=1.510, global_step=82.00, consumed_samples=656.0]\n",
      "Epoch 0:  38%|███▊      | 84/220 [00:05<00:09, 14.40it/s, loss=1.46, v_num=, reduced_train_loss=1.510, global_step=82.00, consumed_samples=656.0]\n",
      "Epoch 0:  38%|███▊      | 84/220 [00:05<00:09, 14.39it/s, loss=1.46, v_num=, reduced_train_loss=1.500, global_step=83.00, consumed_samples=664.0]\n",
      "Epoch 0:  39%|███▊      | 85/220 [00:05<00:09, 14.44it/s, loss=1.46, v_num=, reduced_train_loss=1.500, global_step=83.00, consumed_samples=664.0]\n",
      "Epoch 0:  39%|███▊      | 85/220 [00:05<00:09, 14.44it/s, loss=1.46, v_num=, reduced_train_loss=1.480, global_step=84.00, consumed_samples=672.0]\n",
      "Epoch 0:  39%|███▉      | 86/220 [00:05<00:09, 14.48it/s, loss=1.46, v_num=, reduced_train_loss=1.480, global_step=84.00, consumed_samples=672.0]\n",
      "Epoch 0:  39%|███▉      | 86/220 [00:05<00:09, 14.48it/s, loss=1.46, v_num=, reduced_train_loss=1.460, global_step=85.00, consumed_samples=680.0]\n",
      "Epoch 0:  40%|███▉      | 87/220 [00:05<00:09, 14.53it/s, loss=1.46, v_num=, reduced_train_loss=1.460, global_step=85.00, consumed_samples=680.0]\n",
      "Epoch 0:  40%|███▉      | 87/220 [00:05<00:09, 14.53it/s, loss=1.46, v_num=, reduced_train_loss=1.450, global_step=86.00, consumed_samples=688.0]\n",
      "Epoch 0:  40%|████      | 88/220 [00:06<00:09, 14.59it/s, loss=1.46, v_num=, reduced_train_loss=1.450, global_step=86.00, consumed_samples=688.0]\n",
      "Epoch 0:  40%|████      | 88/220 [00:06<00:09, 14.59it/s, loss=1.46, v_num=, reduced_train_loss=1.490, global_step=87.00, consumed_samples=696.0]\n",
      "Epoch 0:  40%|████      | 89/220 [00:06<00:08, 14.65it/s, loss=1.46, v_num=, reduced_train_loss=1.490, global_step=87.00, consumed_samples=696.0]\n",
      "Epoch 0:  40%|████      | 89/220 [00:06<00:08, 14.64it/s, loss=1.46, v_num=, reduced_train_loss=1.440, global_step=88.00, consumed_samples=704.0]\n",
      "Epoch 0:  41%|████      | 90/220 [00:06<00:08, 14.69it/s, loss=1.46, v_num=, reduced_train_loss=1.440, global_step=88.00, consumed_samples=704.0]\n",
      "Epoch 0:  41%|████      | 90/220 [00:06<00:08, 14.69it/s, loss=1.46, v_num=, reduced_train_loss=1.460, global_step=89.00, consumed_samples=712.0]\n",
      "Epoch 0:  41%|████▏     | 91/220 [00:06<00:08, 14.72it/s, loss=1.46, v_num=, reduced_train_loss=1.460, global_step=89.00, consumed_samples=712.0]\n",
      "Epoch 0:  41%|████▏     | 91/220 [00:06<00:08, 14.72it/s, loss=1.46, v_num=, reduced_train_loss=1.470, global_step=90.00, consumed_samples=720.0]\n",
      "Epoch 0:  42%|████▏     | 92/220 [00:06<00:08, 14.77it/s, loss=1.46, v_num=, reduced_train_loss=1.470, global_step=90.00, consumed_samples=720.0]\n",
      "Epoch 0:  42%|████▏     | 92/220 [00:06<00:08, 14.77it/s, loss=1.46, v_num=, reduced_train_loss=1.470, global_step=91.00, consumed_samples=728.0]\n",
      "Epoch 0:  42%|████▏     | 93/220 [00:06<00:08, 14.83it/s, loss=1.46, v_num=, reduced_train_loss=1.470, global_step=91.00, consumed_samples=728.0]\n",
      "Epoch 0:  42%|████▏     | 93/220 [00:06<00:08, 14.83it/s, loss=1.46, v_num=, reduced_train_loss=1.470, global_step=92.00, consumed_samples=736.0]\n",
      "Epoch 0:  43%|████▎     | 94/220 [00:06<00:08, 14.88it/s, loss=1.46, v_num=, reduced_train_loss=1.470, global_step=92.00, consumed_samples=736.0]\n",
      "Epoch 0:  43%|████▎     | 94/220 [00:06<00:08, 14.88it/s, loss=1.46, v_num=, reduced_train_loss=1.440, global_step=93.00, consumed_samples=744.0]\n",
      "Epoch 0:  43%|████▎     | 95/220 [00:06<00:08, 14.93it/s, loss=1.46, v_num=, reduced_train_loss=1.440, global_step=93.00, consumed_samples=744.0]\n",
      "Epoch 0:  43%|████▎     | 95/220 [00:06<00:08, 14.93it/s, loss=1.46, v_num=, reduced_train_loss=1.440, global_step=94.00, consumed_samples=752.0]\n",
      "Epoch 0:  44%|████▎     | 96/220 [00:06<00:08, 14.98it/s, loss=1.46, v_num=, reduced_train_loss=1.440, global_step=94.00, consumed_samples=752.0]\n",
      "Epoch 0:  44%|████▎     | 96/220 [00:06<00:08, 14.98it/s, loss=1.46, v_num=, reduced_train_loss=1.470, global_step=95.00, consumed_samples=760.0]\n",
      "Epoch 0:  44%|████▍     | 97/220 [00:06<00:08, 15.02it/s, loss=1.46, v_num=, reduced_train_loss=1.470, global_step=95.00, consumed_samples=760.0]\n",
      "Epoch 0:  44%|████▍     | 97/220 [00:06<00:08, 15.02it/s, loss=1.46, v_num=, reduced_train_loss=1.430, global_step=96.00, consumed_samples=768.0]\n",
      "Epoch 0:  45%|████▍     | 98/220 [00:06<00:08, 15.07it/s, loss=1.46, v_num=, reduced_train_loss=1.430, global_step=96.00, consumed_samples=768.0]\n",
      "Epoch 0:  45%|████▍     | 98/220 [00:06<00:08, 15.07it/s, loss=1.46, v_num=, reduced_train_loss=1.430, global_step=97.00, consumed_samples=776.0]\n",
      "Epoch 0:  45%|████▌     | 99/220 [00:06<00:08, 15.11it/s, loss=1.46, v_num=, reduced_train_loss=1.430, global_step=97.00, consumed_samples=776.0]\n",
      "Epoch 0:  45%|████▌     | 99/220 [00:06<00:08, 15.10it/s, loss=1.46, v_num=, reduced_train_loss=1.470, global_step=98.00, consumed_samples=784.0]\n",
      "Epoch 0:  45%|████▌     | 100/220 [00:06<00:07, 15.14it/s, loss=1.46, v_num=, reduced_train_loss=1.470, global_step=98.00, consumed_samples=784.0]\n",
      "Epoch 0:  45%|████▌     | 100/220 [00:06<00:07, 15.14it/s, loss=1.46, v_num=, reduced_train_loss=1.480, global_step=99.00, consumed_samples=792.0]\n",
      "\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:01,  8.65it/s]\u001b[A\n",
      "Epoch 0:  46%|████▌     | 101/220 [00:06<00:07, 14.98it/s, loss=1.46, v_num=, reduced_train_loss=1.480, global_step=99.00, consumed_samples=792.0]\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:00, 15.52it/s]\u001b[A\n",
      "Epoch 0:  46%|████▋     | 102/220 [00:06<00:07, 15.10it/s, loss=1.46, v_num=, reduced_train_loss=1.480, global_step=99.00, consumed_samples=792.0]\n",
      "\n",
      "Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:00, 17.33it/s]\u001b[A\n",
      "Epoch 0:  47%|████▋     | 103/220 [00:06<00:07, 15.15it/s, loss=1.46, v_num=, reduced_train_loss=1.480, global_step=99.00, consumed_samples=792.0]\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 4/10 [00:00<00:00, 20.76it/s]\u001b[A\n",
      "Epoch 0:  47%|████▋     | 104/220 [00:06<00:07, 15.25it/s, loss=1.46, v_num=, reduced_train_loss=1.480, global_step=99.00, consumed_samples=792.0]\n",
      "\n",
      "Validation DataLoader 0:  50%|█████     | 5/10 [00:00<00:00, 23.95it/s]\u001b[A\n",
      "Epoch 0:  48%|████▊     | 105/220 [00:06<00:07, 15.36it/s, loss=1.46, v_num=, reduced_train_loss=1.480, global_step=99.00, consumed_samples=792.0]\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 6/10 [00:00<00:00, 26.99it/s]\u001b[A\n",
      "Epoch 0:  48%|████▊     | 106/220 [00:06<00:07, 15.48it/s, loss=1.46, v_num=, reduced_train_loss=1.480, global_step=99.00, consumed_samples=792.0]\n",
      "\n",
      "Validation DataLoader 0:  70%|███████   | 7/10 [00:00<00:00, 29.75it/s]\u001b[A\n",
      "Epoch 0:  49%|████▊     | 107/220 [00:06<00:07, 15.59it/s, loss=1.46, v_num=, reduced_train_loss=1.480, global_step=99.00, consumed_samples=792.0]\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 8/10 [00:00<00:00, 32.18it/s]\u001b[A\n",
      "Epoch 0:  49%|████▉     | 108/220 [00:06<00:07, 15.71it/s, loss=1.46, v_num=, reduced_train_loss=1.480, global_step=99.00, consumed_samples=792.0]\n",
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 9/10 [00:00<00:00, 34.39it/s]\u001b[A\n",
      "Epoch 0:  50%|████▉     | 109/220 [00:06<00:07, 15.82it/s, loss=1.46, v_num=, reduced_train_loss=1.480, global_step=99.00, consumed_samples=792.0]\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:00<00:00, 36.39it/s]\u001b[A\n",
      "Epoch 0:  50%|█████     | 110/220 [00:06<00:06, 15.94it/s, loss=1.46, v_num=, reduced_train_loss=1.480, global_step=99.00, consumed_samples=792.0]\n",
      "Epoch 0:  50%|█████     | 110/220 [00:06<00:06, 15.93it/s, loss=1.46, v_num=, reduced_train_loss=1.480, global_step=99.00, consumed_samples=792.0, val_loss=1.420]\n",
      "\n",
      "                                                                        \u001b[AEpoch 0, global step 100: 'val_loss' reached 1.42065 (best 1.42065), saving model to '/result/nemo_experiments/esm1nv-oas/esm1nv-oas_pretraining/checkpoints/megatron_bert--val_loss=1.42-step=100-consumed_samples=800.0.ckpt' as top 10\n",
      "\n",
      "Epoch 0:  50%|█████     | 111/220 [00:08<00:08, 13.43it/s, loss=1.46, v_num=, reduced_train_loss=1.480, global_step=99.00, consumed_samples=792.0, val_loss=1.420]\n",
      "Epoch 0:  50%|█████     | 111/220 [00:08<00:08, 13.43it/s, loss=1.46, v_num=, reduced_train_loss=1.430, global_step=100.0, consumed_samples=800.0, val_loss=1.420]\n",
      "Epoch 0:  51%|█████     | 112/220 [00:08<00:08, 13.46it/s, loss=1.46, v_num=, reduced_train_loss=1.430, global_step=100.0, consumed_samples=800.0, val_loss=1.420]\n",
      "Epoch 0:  51%|█████     | 112/220 [00:08<00:08, 13.46it/s, loss=1.46, v_num=, reduced_train_loss=1.450, global_step=101.0, consumed_samples=808.0, val_loss=1.420]\n",
      "Epoch 0:  51%|█████▏    | 113/220 [00:08<00:07, 13.50it/s, loss=1.46, v_num=, reduced_train_loss=1.450, global_step=101.0, consumed_samples=808.0, val_loss=1.420]\n",
      "Epoch 0:  51%|█████▏    | 113/220 [00:08<00:07, 13.50it/s, loss=1.46, v_num=, reduced_train_loss=1.420, global_step=102.0, consumed_samples=816.0, val_loss=1.420]\n",
      "Epoch 0:  52%|█████▏    | 114/220 [00:08<00:07, 13.54it/s, loss=1.46, v_num=, reduced_train_loss=1.420, global_step=102.0, consumed_samples=816.0, val_loss=1.420]\n",
      "Epoch 0:  52%|█████▏    | 114/220 [00:08<00:07, 13.54it/s, loss=1.45, v_num=, reduced_train_loss=1.420, global_step=103.0, consumed_samples=824.0, val_loss=1.420]\n",
      "Epoch 0:  52%|█████▏    | 115/220 [00:08<00:07, 13.58it/s, loss=1.45, v_num=, reduced_train_loss=1.420, global_step=103.0, consumed_samples=824.0, val_loss=1.420]\n",
      "Epoch 0:  52%|█████▏    | 115/220 [00:08<00:07, 13.58it/s, loss=1.45, v_num=, reduced_train_loss=1.420, global_step=104.0, consumed_samples=832.0, val_loss=1.420]\n",
      "Epoch 0:  53%|█████▎    | 116/220 [00:08<00:07, 13.62it/s, loss=1.45, v_num=, reduced_train_loss=1.420, global_step=104.0, consumed_samples=832.0, val_loss=1.420]\n",
      "Epoch 0:  53%|█████▎    | 116/220 [00:08<00:07, 13.62it/s, loss=1.45, v_num=, reduced_train_loss=1.410, global_step=105.0, consumed_samples=840.0, val_loss=1.420]\n",
      "Epoch 0:  53%|█████▎    | 117/220 [00:08<00:07, 13.66it/s, loss=1.45, v_num=, reduced_train_loss=1.410, global_step=105.0, consumed_samples=840.0, val_loss=1.420]\n",
      "Epoch 0:  53%|█████▎    | 117/220 [00:08<00:07, 13.66it/s, loss=1.45, v_num=, reduced_train_loss=1.410, global_step=106.0, consumed_samples=848.0, val_loss=1.420]\n",
      "Epoch 0:  54%|█████▎    | 118/220 [00:08<00:07, 13.70it/s, loss=1.45, v_num=, reduced_train_loss=1.410, global_step=106.0, consumed_samples=848.0, val_loss=1.420]\n",
      "Epoch 0:  54%|█████▎    | 118/220 [00:08<00:07, 13.70it/s, loss=1.44, v_num=, reduced_train_loss=1.410, global_step=107.0, consumed_samples=856.0, val_loss=1.420]\n",
      "Epoch 0:  54%|█████▍    | 119/220 [00:08<00:07, 13.73it/s, loss=1.44, v_num=, reduced_train_loss=1.410, global_step=107.0, consumed_samples=856.0, val_loss=1.420]\n",
      "Epoch 0:  54%|█████▍    | 119/220 [00:08<00:07, 13.73it/s, loss=1.44, v_num=, reduced_train_loss=1.430, global_step=108.0, consumed_samples=864.0, val_loss=1.420]\n",
      "Epoch 0:  55%|█████▍    | 120/220 [00:08<00:07, 13.77it/s, loss=1.44, v_num=, reduced_train_loss=1.430, global_step=108.0, consumed_samples=864.0, val_loss=1.420]\n",
      "Epoch 0:  55%|█████▍    | 120/220 [00:08<00:07, 13.77it/s, loss=1.44, v_num=, reduced_train_loss=1.480, global_step=109.0, consumed_samples=872.0, val_loss=1.420]\n",
      "Epoch 0:  55%|█████▌    | 121/220 [00:08<00:07, 13.81it/s, loss=1.44, v_num=, reduced_train_loss=1.480, global_step=109.0, consumed_samples=872.0, val_loss=1.420]\n",
      "Epoch 0:  55%|█████▌    | 121/220 [00:08<00:07, 13.81it/s, loss=1.44, v_num=, reduced_train_loss=1.410, global_step=110.0, consumed_samples=880.0, val_loss=1.420]\n",
      "Epoch 0:  55%|█████▌    | 122/220 [00:08<00:07, 13.86it/s, loss=1.44, v_num=, reduced_train_loss=1.410, global_step=110.0, consumed_samples=880.0, val_loss=1.420]\n",
      "Epoch 0:  55%|█████▌    | 122/220 [00:08<00:07, 13.86it/s, loss=1.44, v_num=, reduced_train_loss=1.430, global_step=111.0, consumed_samples=888.0, val_loss=1.420]\n",
      "Epoch 0:  56%|█████▌    | 123/220 [00:08<00:06, 13.90it/s, loss=1.44, v_num=, reduced_train_loss=1.430, global_step=111.0, consumed_samples=888.0, val_loss=1.420]\n",
      "Epoch 0:  56%|█████▌    | 123/220 [00:08<00:06, 13.90it/s, loss=1.43, v_num=, reduced_train_loss=1.410, global_step=112.0, consumed_samples=896.0, val_loss=1.420]\n",
      "Epoch 0:  56%|█████▋    | 124/220 [00:08<00:06, 13.94it/s, loss=1.43, v_num=, reduced_train_loss=1.410, global_step=112.0, consumed_samples=896.0, val_loss=1.420]\n",
      "Epoch 0:  56%|█████▋    | 124/220 [00:08<00:06, 13.94it/s, loss=1.43, v_num=, reduced_train_loss=1.410, global_step=113.0, consumed_samples=904.0, val_loss=1.420]\n",
      "Epoch 0:  57%|█████▋    | 125/220 [00:08<00:06, 13.99it/s, loss=1.43, v_num=, reduced_train_loss=1.410, global_step=113.0, consumed_samples=904.0, val_loss=1.420]\n",
      "Epoch 0:  57%|█████▋    | 125/220 [00:08<00:06, 13.99it/s, loss=1.43, v_num=, reduced_train_loss=1.430, global_step=114.0, consumed_samples=912.0, val_loss=1.420]\n",
      "Epoch 0:  57%|█████▋    | 126/220 [00:08<00:06, 14.03it/s, loss=1.43, v_num=, reduced_train_loss=1.430, global_step=114.0, consumed_samples=912.0, val_loss=1.420]\n",
      "Epoch 0:  57%|█████▋    | 126/220 [00:08<00:06, 14.02it/s, loss=1.43, v_num=, reduced_train_loss=1.400, global_step=115.0, consumed_samples=920.0, val_loss=1.420]\n",
      "Epoch 0:  58%|█████▊    | 127/220 [00:09<00:06, 14.07it/s, loss=1.43, v_num=, reduced_train_loss=1.400, global_step=115.0, consumed_samples=920.0, val_loss=1.420]\n",
      "Epoch 0:  58%|█████▊    | 127/220 [00:09<00:06, 14.07it/s, loss=1.43, v_num=, reduced_train_loss=1.410, global_step=116.0, consumed_samples=928.0, val_loss=1.420]\n",
      "Epoch 0:  58%|█████▊    | 128/220 [00:09<00:06, 14.11it/s, loss=1.43, v_num=, reduced_train_loss=1.410, global_step=116.0, consumed_samples=928.0, val_loss=1.420]\n",
      "Epoch 0:  58%|█████▊    | 128/220 [00:09<00:06, 14.11it/s, loss=1.43, v_num=, reduced_train_loss=1.390, global_step=117.0, consumed_samples=936.0, val_loss=1.420]\n",
      "Epoch 0:  59%|█████▊    | 129/220 [00:09<00:06, 14.14it/s, loss=1.43, v_num=, reduced_train_loss=1.390, global_step=117.0, consumed_samples=936.0, val_loss=1.420]\n",
      "Epoch 0:  59%|█████▊    | 129/220 [00:09<00:06, 14.14it/s, loss=1.42, v_num=, reduced_train_loss=1.440, global_step=118.0, consumed_samples=944.0, val_loss=1.420]\n",
      "Epoch 0:  59%|█████▉    | 130/220 [00:09<00:06, 14.17it/s, loss=1.42, v_num=, reduced_train_loss=1.440, global_step=118.0, consumed_samples=944.0, val_loss=1.420]\n",
      "Epoch 0:  59%|█████▉    | 130/220 [00:09<00:06, 14.17it/s, loss=1.42, v_num=, reduced_train_loss=1.410, global_step=119.0, consumed_samples=952.0, val_loss=1.420]\n",
      "Epoch 0:  60%|█████▉    | 131/220 [00:09<00:06, 14.21it/s, loss=1.42, v_num=, reduced_train_loss=1.410, global_step=119.0, consumed_samples=952.0, val_loss=1.420]\n",
      "Epoch 0:  60%|█████▉    | 131/220 [00:09<00:06, 14.21it/s, loss=1.42, v_num=, reduced_train_loss=1.420, global_step=120.0, consumed_samples=960.0, val_loss=1.420]\n",
      "Epoch 0:  60%|██████    | 132/220 [00:09<00:06, 14.25it/s, loss=1.42, v_num=, reduced_train_loss=1.420, global_step=120.0, consumed_samples=960.0, val_loss=1.420]\n",
      "Epoch 0:  60%|██████    | 132/220 [00:09<00:06, 14.25it/s, loss=1.42, v_num=, reduced_train_loss=1.400, global_step=121.0, consumed_samples=968.0, val_loss=1.420]\n",
      "Epoch 0:  60%|██████    | 133/220 [00:09<00:06, 14.28it/s, loss=1.42, v_num=, reduced_train_loss=1.400, global_step=121.0, consumed_samples=968.0, val_loss=1.420]\n",
      "Epoch 0:  60%|██████    | 133/220 [00:09<00:06, 14.28it/s, loss=1.42, v_num=, reduced_train_loss=1.470, global_step=122.0, consumed_samples=976.0, val_loss=1.420]\n",
      "Epoch 0:  61%|██████    | 134/220 [00:09<00:06, 14.31it/s, loss=1.42, v_num=, reduced_train_loss=1.470, global_step=122.0, consumed_samples=976.0, val_loss=1.420]\n",
      "Epoch 0:  61%|██████    | 134/220 [00:09<00:06, 14.31it/s, loss=1.42, v_num=, reduced_train_loss=1.410, global_step=123.0, consumed_samples=984.0, val_loss=1.420]\n",
      "Epoch 0:  61%|██████▏   | 135/220 [00:09<00:05, 14.34it/s, loss=1.42, v_num=, reduced_train_loss=1.410, global_step=123.0, consumed_samples=984.0, val_loss=1.420]\n",
      "Epoch 0:  61%|██████▏   | 135/220 [00:09<00:05, 14.34it/s, loss=1.42, v_num=, reduced_train_loss=1.470, global_step=124.0, consumed_samples=992.0, val_loss=1.420]\n",
      "Epoch 0:  62%|██████▏   | 136/220 [00:09<00:05, 14.37it/s, loss=1.42, v_num=, reduced_train_loss=1.470, global_step=124.0, consumed_samples=992.0, val_loss=1.420]\n",
      "Epoch 0:  62%|██████▏   | 136/220 [00:09<00:05, 14.37it/s, loss=1.42, v_num=, reduced_train_loss=1.430, global_step=125.0, consumed_samples=1e+3, val_loss=1.420] \n",
      "Epoch 0:  62%|██████▏   | 137/220 [00:09<00:05, 14.39it/s, loss=1.42, v_num=, reduced_train_loss=1.430, global_step=125.0, consumed_samples=1e+3, val_loss=1.420]\n",
      "Epoch 0:  62%|██████▏   | 137/220 [00:09<00:05, 14.39it/s, loss=1.42, v_num=, reduced_train_loss=1.400, global_step=126.0, consumed_samples=1008.0, val_loss=1.420]\n",
      "Epoch 0:  63%|██████▎   | 138/220 [00:09<00:05, 14.43it/s, loss=1.42, v_num=, reduced_train_loss=1.400, global_step=126.0, consumed_samples=1008.0, val_loss=1.420]\n",
      "Epoch 0:  63%|██████▎   | 138/220 [00:09<00:05, 14.43it/s, loss=1.42, v_num=, reduced_train_loss=1.410, global_step=127.0, consumed_samples=1016.0, val_loss=1.420]\n",
      "Epoch 0:  63%|██████▎   | 139/220 [00:09<00:05, 14.45it/s, loss=1.42, v_num=, reduced_train_loss=1.410, global_step=127.0, consumed_samples=1016.0, val_loss=1.420]\n",
      "Epoch 0:  63%|██████▎   | 139/220 [00:09<00:05, 14.45it/s, loss=1.42, v_num=, reduced_train_loss=1.460, global_step=128.0, consumed_samples=1024.0, val_loss=1.420]\n",
      "Epoch 0:  64%|██████▎   | 140/220 [00:09<00:05, 14.49it/s, loss=1.42, v_num=, reduced_train_loss=1.460, global_step=128.0, consumed_samples=1024.0, val_loss=1.420]\n",
      "Epoch 0:  64%|██████▎   | 140/220 [00:09<00:05, 14.49it/s, loss=1.42, v_num=, reduced_train_loss=1.420, global_step=129.0, consumed_samples=1032.0, val_loss=1.420]\n",
      "Epoch 0:  64%|██████▍   | 141/220 [00:09<00:05, 14.52it/s, loss=1.42, v_num=, reduced_train_loss=1.420, global_step=129.0, consumed_samples=1032.0, val_loss=1.420]\n",
      "Epoch 0:  64%|██████▍   | 141/220 [00:09<00:05, 14.52it/s, loss=1.42, v_num=, reduced_train_loss=1.410, global_step=130.0, consumed_samples=1040.0, val_loss=1.420]\n",
      "Epoch 0:  65%|██████▍   | 142/220 [00:09<00:05, 14.55it/s, loss=1.42, v_num=, reduced_train_loss=1.410, global_step=130.0, consumed_samples=1040.0, val_loss=1.420]\n",
      "Epoch 0:  65%|██████▍   | 142/220 [00:09<00:05, 14.55it/s, loss=1.42, v_num=, reduced_train_loss=1.450, global_step=131.0, consumed_samples=1048.0, val_loss=1.420]\n",
      "Epoch 0:  65%|██████▌   | 143/220 [00:09<00:05, 14.59it/s, loss=1.42, v_num=, reduced_train_loss=1.450, global_step=131.0, consumed_samples=1048.0, val_loss=1.420]\n",
      "Epoch 0:  65%|██████▌   | 143/220 [00:09<00:05, 14.59it/s, loss=1.43, v_num=, reduced_train_loss=1.460, global_step=132.0, consumed_samples=1056.0, val_loss=1.420]\n",
      "Epoch 0:  65%|██████▌   | 144/220 [00:09<00:05, 14.62it/s, loss=1.43, v_num=, reduced_train_loss=1.460, global_step=132.0, consumed_samples=1056.0, val_loss=1.420]\n",
      "Epoch 0:  65%|██████▌   | 144/220 [00:09<00:05, 14.62it/s, loss=1.43, v_num=, reduced_train_loss=1.420, global_step=133.0, consumed_samples=1064.0, val_loss=1.420]\n",
      "Epoch 0:  66%|██████▌   | 145/220 [00:09<00:05, 14.63it/s, loss=1.43, v_num=, reduced_train_loss=1.420, global_step=133.0, consumed_samples=1064.0, val_loss=1.420]\n",
      "Epoch 0:  66%|██████▌   | 145/220 [00:09<00:05, 14.63it/s, loss=1.43, v_num=, reduced_train_loss=1.420, global_step=134.0, consumed_samples=1072.0, val_loss=1.420]\n",
      "Epoch 0:  66%|██████▋   | 146/220 [00:09<00:05, 14.67it/s, loss=1.43, v_num=, reduced_train_loss=1.420, global_step=134.0, consumed_samples=1072.0, val_loss=1.420]\n",
      "Epoch 0:  66%|██████▋   | 146/220 [00:09<00:05, 14.67it/s, loss=1.43, v_num=, reduced_train_loss=1.420, global_step=135.0, consumed_samples=1080.0, val_loss=1.420]\n",
      "Epoch 0:  67%|██████▋   | 147/220 [00:10<00:04, 14.70it/s, loss=1.43, v_num=, reduced_train_loss=1.420, global_step=135.0, consumed_samples=1080.0, val_loss=1.420]\n",
      "Epoch 0:  67%|██████▋   | 147/220 [00:10<00:04, 14.70it/s, loss=1.43, v_num=, reduced_train_loss=1.440, global_step=136.0, consumed_samples=1088.0, val_loss=1.420]\n",
      "Epoch 0:  67%|██████▋   | 148/220 [00:10<00:04, 14.73it/s, loss=1.43, v_num=, reduced_train_loss=1.440, global_step=136.0, consumed_samples=1088.0, val_loss=1.420]\n",
      "Epoch 0:  67%|██████▋   | 148/220 [00:10<00:04, 14.72it/s, loss=1.43, v_num=, reduced_train_loss=1.440, global_step=137.0, consumed_samples=1096.0, val_loss=1.420]\n",
      "Epoch 0:  68%|██████▊   | 149/220 [00:10<00:04, 14.75it/s, loss=1.43, v_num=, reduced_train_loss=1.440, global_step=137.0, consumed_samples=1096.0, val_loss=1.420]\n",
      "Epoch 0:  68%|██████▊   | 149/220 [00:10<00:04, 14.75it/s, loss=1.43, v_num=, reduced_train_loss=1.410, global_step=138.0, consumed_samples=1104.0, val_loss=1.420]\n",
      "Epoch 0:  68%|██████▊   | 150/220 [00:10<00:04, 14.78it/s, loss=1.43, v_num=, reduced_train_loss=1.410, global_step=138.0, consumed_samples=1104.0, val_loss=1.420]\n",
      "Epoch 0:  68%|██████▊   | 150/220 [00:10<00:04, 14.78it/s, loss=1.43, v_num=, reduced_train_loss=1.410, global_step=139.0, consumed_samples=1112.0, val_loss=1.420]\n",
      "Epoch 0:  69%|██████▊   | 151/220 [00:10<00:04, 14.81it/s, loss=1.43, v_num=, reduced_train_loss=1.410, global_step=139.0, consumed_samples=1112.0, val_loss=1.420]\n",
      "Epoch 0:  69%|██████▊   | 151/220 [00:10<00:04, 14.81it/s, loss=1.43, v_num=, reduced_train_loss=1.430, global_step=140.0, consumed_samples=1120.0, val_loss=1.420]\n",
      "Epoch 0:  69%|██████▉   | 152/220 [00:10<00:04, 14.84it/s, loss=1.43, v_num=, reduced_train_loss=1.430, global_step=140.0, consumed_samples=1120.0, val_loss=1.420]\n",
      "Epoch 0:  69%|██████▉   | 152/220 [00:10<00:04, 14.83it/s, loss=1.43, v_num=, reduced_train_loss=1.410, global_step=141.0, consumed_samples=1128.0, val_loss=1.420]\n",
      "Epoch 0:  70%|██████▉   | 153/220 [00:10<00:04, 14.86it/s, loss=1.43, v_num=, reduced_train_loss=1.410, global_step=141.0, consumed_samples=1128.0, val_loss=1.420]\n",
      "Epoch 0:  70%|██████▉   | 153/220 [00:10<00:04, 14.86it/s, loss=1.43, v_num=, reduced_train_loss=1.430, global_step=142.0, consumed_samples=1136.0, val_loss=1.420]\n",
      "Epoch 0:  70%|███████   | 154/220 [00:10<00:04, 14.88it/s, loss=1.43, v_num=, reduced_train_loss=1.430, global_step=142.0, consumed_samples=1136.0, val_loss=1.420]\n",
      "Epoch 0:  70%|███████   | 154/220 [00:10<00:04, 14.88it/s, loss=1.43, v_num=, reduced_train_loss=1.410, global_step=143.0, consumed_samples=1144.0, val_loss=1.420]\n",
      "Epoch 0:  70%|███████   | 155/220 [00:10<00:04, 14.90it/s, loss=1.43, v_num=, reduced_train_loss=1.410, global_step=143.0, consumed_samples=1144.0, val_loss=1.420]\n",
      "Epoch 0:  70%|███████   | 155/220 [00:10<00:04, 14.90it/s, loss=1.43, v_num=, reduced_train_loss=1.450, global_step=144.0, consumed_samples=1152.0, val_loss=1.420]\n",
      "Epoch 0:  71%|███████   | 156/220 [00:10<00:04, 14.93it/s, loss=1.43, v_num=, reduced_train_loss=1.450, global_step=144.0, consumed_samples=1152.0, val_loss=1.420]\n",
      "Epoch 0:  71%|███████   | 156/220 [00:10<00:04, 14.93it/s, loss=1.43, v_num=, reduced_train_loss=1.450, global_step=145.0, consumed_samples=1160.0, val_loss=1.420]\n",
      "Epoch 0:  71%|███████▏  | 157/220 [00:10<00:04, 14.95it/s, loss=1.43, v_num=, reduced_train_loss=1.450, global_step=145.0, consumed_samples=1160.0, val_loss=1.420]\n",
      "Epoch 0:  71%|███████▏  | 157/220 [00:10<00:04, 14.95it/s, loss=1.43, v_num=, reduced_train_loss=1.450, global_step=146.0, consumed_samples=1168.0, val_loss=1.420]\n",
      "Epoch 0:  72%|███████▏  | 158/220 [00:10<00:04, 14.97it/s, loss=1.43, v_num=, reduced_train_loss=1.450, global_step=146.0, consumed_samples=1168.0, val_loss=1.420]\n",
      "Epoch 0:  72%|███████▏  | 158/220 [00:10<00:04, 14.97it/s, loss=1.43, v_num=, reduced_train_loss=1.410, global_step=147.0, consumed_samples=1176.0, val_loss=1.420]\n",
      "Epoch 0:  72%|███████▏  | 159/220 [00:10<00:04, 14.99it/s, loss=1.43, v_num=, reduced_train_loss=1.410, global_step=147.0, consumed_samples=1176.0, val_loss=1.420]\n",
      "Epoch 0:  72%|███████▏  | 159/220 [00:10<00:04, 14.99it/s, loss=1.43, v_num=, reduced_train_loss=1.410, global_step=148.0, consumed_samples=1184.0, val_loss=1.420]\n",
      "Epoch 0:  73%|███████▎  | 160/220 [00:10<00:03, 15.02it/s, loss=1.43, v_num=, reduced_train_loss=1.410, global_step=148.0, consumed_samples=1184.0, val_loss=1.420]\n",
      "Epoch 0:  73%|███████▎  | 160/220 [00:10<00:03, 15.02it/s, loss=1.43, v_num=, reduced_train_loss=1.440, global_step=149.0, consumed_samples=1192.0, val_loss=1.420]\n",
      "Epoch 0:  73%|███████▎  | 161/220 [00:10<00:03, 15.04it/s, loss=1.43, v_num=, reduced_train_loss=1.440, global_step=149.0, consumed_samples=1192.0, val_loss=1.420]\n",
      "Epoch 0:  73%|███████▎  | 161/220 [00:10<00:03, 15.04it/s, loss=1.43, v_num=, reduced_train_loss=1.420, global_step=150.0, consumed_samples=1200.0, val_loss=1.420]\n",
      "Epoch 0:  74%|███████▎  | 162/220 [00:10<00:03, 15.07it/s, loss=1.43, v_num=, reduced_train_loss=1.420, global_step=150.0, consumed_samples=1200.0, val_loss=1.420]\n",
      "Epoch 0:  74%|███████▎  | 162/220 [00:10<00:03, 15.07it/s, loss=1.43, v_num=, reduced_train_loss=1.430, global_step=151.0, consumed_samples=1208.0, val_loss=1.420]\n",
      "Epoch 0:  74%|███████▍  | 163/220 [00:10<00:03, 15.10it/s, loss=1.43, v_num=, reduced_train_loss=1.430, global_step=151.0, consumed_samples=1208.0, val_loss=1.420]\n",
      "Epoch 0:  74%|███████▍  | 163/220 [00:10<00:03, 15.09it/s, loss=1.43, v_num=, reduced_train_loss=1.410, global_step=152.0, consumed_samples=1216.0, val_loss=1.420]\n",
      "Epoch 0:  75%|███████▍  | 164/220 [00:10<00:03, 15.12it/s, loss=1.43, v_num=, reduced_train_loss=1.410, global_step=152.0, consumed_samples=1216.0, val_loss=1.420]\n",
      "Epoch 0:  75%|███████▍  | 164/220 [00:10<00:03, 15.12it/s, loss=1.42, v_num=, reduced_train_loss=1.400, global_step=153.0, consumed_samples=1224.0, val_loss=1.420]\n",
      "Epoch 0:  75%|███████▌  | 165/220 [00:10<00:03, 15.15it/s, loss=1.42, v_num=, reduced_train_loss=1.400, global_step=153.0, consumed_samples=1224.0, val_loss=1.420]\n",
      "Epoch 0:  75%|███████▌  | 165/220 [00:10<00:03, 15.15it/s, loss=1.43, v_num=, reduced_train_loss=1.470, global_step=154.0, consumed_samples=1232.0, val_loss=1.420]\n",
      "Epoch 0:  75%|███████▌  | 166/220 [00:10<00:03, 15.18it/s, loss=1.43, v_num=, reduced_train_loss=1.470, global_step=154.0, consumed_samples=1232.0, val_loss=1.420]\n",
      "Epoch 0:  75%|███████▌  | 166/220 [00:10<00:03, 15.18it/s, loss=1.43, v_num=, reduced_train_loss=1.450, global_step=155.0, consumed_samples=1240.0, val_loss=1.420]\n",
      "Epoch 0:  76%|███████▌  | 167/220 [00:10<00:03, 15.21it/s, loss=1.43, v_num=, reduced_train_loss=1.450, global_step=155.0, consumed_samples=1240.0, val_loss=1.420]\n",
      "Epoch 0:  76%|███████▌  | 167/220 [00:10<00:03, 15.21it/s, loss=1.43, v_num=, reduced_train_loss=1.410, global_step=156.0, consumed_samples=1248.0, val_loss=1.420]\n",
      "Epoch 0:  76%|███████▋  | 168/220 [00:11<00:03, 15.23it/s, loss=1.43, v_num=, reduced_train_loss=1.410, global_step=156.0, consumed_samples=1248.0, val_loss=1.420]\n",
      "Epoch 0:  76%|███████▋  | 168/220 [00:11<00:03, 15.23it/s, loss=1.43, v_num=, reduced_train_loss=1.390, global_step=157.0, consumed_samples=1256.0, val_loss=1.420]\n",
      "Epoch 0:  77%|███████▋  | 169/220 [00:11<00:03, 15.26it/s, loss=1.43, v_num=, reduced_train_loss=1.390, global_step=157.0, consumed_samples=1256.0, val_loss=1.420]\n",
      "Epoch 0:  77%|███████▋  | 169/220 [00:11<00:03, 15.26it/s, loss=1.43, v_num=, reduced_train_loss=1.430, global_step=158.0, consumed_samples=1264.0, val_loss=1.420]\n",
      "Epoch 0:  77%|███████▋  | 170/220 [00:11<00:03, 15.28it/s, loss=1.43, v_num=, reduced_train_loss=1.430, global_step=158.0, consumed_samples=1264.0, val_loss=1.420]\n",
      "Epoch 0:  77%|███████▋  | 170/220 [00:11<00:03, 15.28it/s, loss=1.43, v_num=, reduced_train_loss=1.410, global_step=159.0, consumed_samples=1272.0, val_loss=1.420]\n",
      "Epoch 0:  78%|███████▊  | 171/220 [00:11<00:03, 15.30it/s, loss=1.43, v_num=, reduced_train_loss=1.410, global_step=159.0, consumed_samples=1272.0, val_loss=1.420]\n",
      "Epoch 0:  78%|███████▊  | 171/220 [00:11<00:03, 15.30it/s, loss=1.42, v_num=, reduced_train_loss=1.410, global_step=160.0, consumed_samples=1280.0, val_loss=1.420]\n",
      "Epoch 0:  78%|███████▊  | 172/220 [00:11<00:03, 15.32it/s, loss=1.42, v_num=, reduced_train_loss=1.410, global_step=160.0, consumed_samples=1280.0, val_loss=1.420]\n",
      "Epoch 0:  78%|███████▊  | 172/220 [00:11<00:03, 15.32it/s, loss=1.42, v_num=, reduced_train_loss=1.400, global_step=161.0, consumed_samples=1288.0, val_loss=1.420]\n",
      "Epoch 0:  79%|███████▊  | 173/220 [00:11<00:03, 15.34it/s, loss=1.42, v_num=, reduced_train_loss=1.400, global_step=161.0, consumed_samples=1288.0, val_loss=1.420]\n",
      "Epoch 0:  79%|███████▊  | 173/220 [00:11<00:03, 15.33it/s, loss=1.42, v_num=, reduced_train_loss=1.400, global_step=162.0, consumed_samples=1296.0, val_loss=1.420]\n",
      "Epoch 0:  79%|███████▉  | 174/220 [00:11<00:02, 15.36it/s, loss=1.42, v_num=, reduced_train_loss=1.400, global_step=162.0, consumed_samples=1296.0, val_loss=1.420]\n",
      "Epoch 0:  79%|███████▉  | 174/220 [00:11<00:02, 15.36it/s, loss=1.42, v_num=, reduced_train_loss=1.430, global_step=163.0, consumed_samples=1304.0, val_loss=1.420]\n",
      "Epoch 0:  80%|███████▉  | 175/220 [00:11<00:02, 15.38it/s, loss=1.42, v_num=, reduced_train_loss=1.430, global_step=163.0, consumed_samples=1304.0, val_loss=1.420]\n",
      "Epoch 0:  80%|███████▉  | 175/220 [00:11<00:02, 15.38it/s, loss=1.42, v_num=, reduced_train_loss=1.400, global_step=164.0, consumed_samples=1312.0, val_loss=1.420]\n",
      "Epoch 0:  80%|████████  | 176/220 [00:11<00:02, 15.40it/s, loss=1.42, v_num=, reduced_train_loss=1.400, global_step=164.0, consumed_samples=1312.0, val_loss=1.420]\n",
      "Epoch 0:  80%|████████  | 176/220 [00:11<00:02, 15.40it/s, loss=1.42, v_num=, reduced_train_loss=1.390, global_step=165.0, consumed_samples=1320.0, val_loss=1.420]\n",
      "Epoch 0:  80%|████████  | 177/220 [00:11<00:02, 15.40it/s, loss=1.42, v_num=, reduced_train_loss=1.390, global_step=165.0, consumed_samples=1320.0, val_loss=1.420]\n",
      "Epoch 0:  80%|████████  | 177/220 [00:11<00:02, 15.40it/s, loss=1.42, v_num=, reduced_train_loss=1.400, global_step=166.0, consumed_samples=1328.0, val_loss=1.420]\n",
      "Epoch 0:  81%|████████  | 178/220 [00:11<00:02, 15.42it/s, loss=1.42, v_num=, reduced_train_loss=1.400, global_step=166.0, consumed_samples=1328.0, val_loss=1.420]\n",
      "Epoch 0:  81%|████████  | 178/220 [00:11<00:02, 15.42it/s, loss=1.41, v_num=, reduced_train_loss=1.400, global_step=167.0, consumed_samples=1336.0, val_loss=1.420]\n",
      "Epoch 0:  81%|████████▏ | 179/220 [00:11<00:02, 15.44it/s, loss=1.41, v_num=, reduced_train_loss=1.400, global_step=167.0, consumed_samples=1336.0, val_loss=1.420]\n",
      "Epoch 0:  81%|████████▏ | 179/220 [00:11<00:02, 15.44it/s, loss=1.42, v_num=, reduced_train_loss=1.410, global_step=168.0, consumed_samples=1344.0, val_loss=1.420]\n",
      "Epoch 0:  82%|████████▏ | 180/220 [00:11<00:02, 15.46it/s, loss=1.42, v_num=, reduced_train_loss=1.410, global_step=168.0, consumed_samples=1344.0, val_loss=1.420]\n",
      "Epoch 0:  82%|████████▏ | 180/220 [00:11<00:02, 15.46it/s, loss=1.41, v_num=, reduced_train_loss=1.380, global_step=169.0, consumed_samples=1352.0, val_loss=1.420]\n",
      "Epoch 0:  82%|████████▏ | 181/220 [00:11<00:02, 15.48it/s, loss=1.41, v_num=, reduced_train_loss=1.380, global_step=169.0, consumed_samples=1352.0, val_loss=1.420]\n",
      "Epoch 0:  82%|████████▏ | 181/220 [00:11<00:02, 15.48it/s, loss=1.41, v_num=, reduced_train_loss=1.420, global_step=170.0, consumed_samples=1360.0, val_loss=1.420]\n",
      "Epoch 0:  83%|████████▎ | 182/220 [00:11<00:02, 15.49it/s, loss=1.41, v_num=, reduced_train_loss=1.420, global_step=170.0, consumed_samples=1360.0, val_loss=1.420]\n",
      "Epoch 0:  83%|████████▎ | 182/220 [00:11<00:02, 15.49it/s, loss=1.41, v_num=, reduced_train_loss=1.450, global_step=171.0, consumed_samples=1368.0, val_loss=1.420]\n",
      "Epoch 0:  83%|████████▎ | 183/220 [00:11<00:02, 15.50it/s, loss=1.41, v_num=, reduced_train_loss=1.450, global_step=171.0, consumed_samples=1368.0, val_loss=1.420]\n",
      "Epoch 0:  83%|████████▎ | 183/220 [00:11<00:02, 15.50it/s, loss=1.41, v_num=, reduced_train_loss=1.430, global_step=172.0, consumed_samples=1376.0, val_loss=1.420]\n",
      "Epoch 0:  84%|████████▎ | 184/220 [00:11<00:02, 15.52it/s, loss=1.41, v_num=, reduced_train_loss=1.430, global_step=172.0, consumed_samples=1376.0, val_loss=1.420]\n",
      "Epoch 0:  84%|████████▎ | 184/220 [00:11<00:02, 15.52it/s, loss=1.41, v_num=, reduced_train_loss=1.390, global_step=173.0, consumed_samples=1384.0, val_loss=1.420]\n",
      "Epoch 0:  84%|████████▍ | 185/220 [00:11<00:02, 15.54it/s, loss=1.41, v_num=, reduced_train_loss=1.390, global_step=173.0, consumed_samples=1384.0, val_loss=1.420]\n",
      "Epoch 0:  84%|████████▍ | 185/220 [00:11<00:02, 15.54it/s, loss=1.41, v_num=, reduced_train_loss=1.400, global_step=174.0, consumed_samples=1392.0, val_loss=1.420]\n",
      "Epoch 0:  85%|████████▍ | 186/220 [00:11<00:02, 15.56it/s, loss=1.41, v_num=, reduced_train_loss=1.400, global_step=174.0, consumed_samples=1392.0, val_loss=1.420]\n",
      "Epoch 0:  85%|████████▍ | 186/220 [00:11<00:02, 15.56it/s, loss=1.41, v_num=, reduced_train_loss=1.410, global_step=175.0, consumed_samples=1400.0, val_loss=1.420]\n",
      "Epoch 0:  85%|████████▌ | 187/220 [00:12<00:02, 15.58it/s, loss=1.41, v_num=, reduced_train_loss=1.410, global_step=175.0, consumed_samples=1400.0, val_loss=1.420]\n",
      "Epoch 0:  85%|████████▌ | 187/220 [00:12<00:02, 15.58it/s, loss=1.41, v_num=, reduced_train_loss=1.410, global_step=176.0, consumed_samples=1408.0, val_loss=1.420]\n",
      "Epoch 0:  85%|████████▌ | 188/220 [00:12<00:02, 15.60it/s, loss=1.41, v_num=, reduced_train_loss=1.410, global_step=176.0, consumed_samples=1408.0, val_loss=1.420]\n",
      "Epoch 0:  85%|████████▌ | 188/220 [00:12<00:02, 15.60it/s, loss=1.41, v_num=, reduced_train_loss=1.400, global_step=177.0, consumed_samples=1416.0, val_loss=1.420]\n",
      "Epoch 0:  86%|████████▌ | 189/220 [00:12<00:01, 15.61it/s, loss=1.41, v_num=, reduced_train_loss=1.400, global_step=177.0, consumed_samples=1416.0, val_loss=1.420]\n",
      "Epoch 0:  86%|████████▌ | 189/220 [00:12<00:01, 15.61it/s, loss=1.41, v_num=, reduced_train_loss=1.380, global_step=178.0, consumed_samples=1424.0, val_loss=1.420]\n",
      "Epoch 0:  86%|████████▋ | 190/220 [00:12<00:01, 15.63it/s, loss=1.41, v_num=, reduced_train_loss=1.380, global_step=178.0, consumed_samples=1424.0, val_loss=1.420]\n",
      "Epoch 0:  86%|████████▋ | 190/220 [00:12<00:01, 15.63it/s, loss=1.4, v_num=, reduced_train_loss=1.390, global_step=179.0, consumed_samples=1432.0, val_loss=1.420] \n",
      "Epoch 0:  87%|████████▋ | 191/220 [00:12<00:01, 15.65it/s, loss=1.4, v_num=, reduced_train_loss=1.390, global_step=179.0, consumed_samples=1432.0, val_loss=1.420]\n",
      "Epoch 0:  87%|████████▋ | 191/220 [00:12<00:01, 15.65it/s, loss=1.4, v_num=, reduced_train_loss=1.390, global_step=180.0, consumed_samples=1440.0, val_loss=1.420]\n",
      "Epoch 0:  87%|████████▋ | 192/220 [00:12<00:01, 15.66it/s, loss=1.4, v_num=, reduced_train_loss=1.390, global_step=180.0, consumed_samples=1440.0, val_loss=1.420]\n",
      "Epoch 0:  87%|████████▋ | 192/220 [00:12<00:01, 15.66it/s, loss=1.4, v_num=, reduced_train_loss=1.410, global_step=181.0, consumed_samples=1448.0, val_loss=1.420]\n",
      "Epoch 0:  88%|████████▊ | 193/220 [00:12<00:01, 15.68it/s, loss=1.4, v_num=, reduced_train_loss=1.410, global_step=181.0, consumed_samples=1448.0, val_loss=1.420]\n",
      "Epoch 0:  88%|████████▊ | 193/220 [00:12<00:01, 15.68it/s, loss=1.4, v_num=, reduced_train_loss=1.390, global_step=182.0, consumed_samples=1456.0, val_loss=1.420]\n",
      "Epoch 0:  88%|████████▊ | 194/220 [00:12<00:01, 15.70it/s, loss=1.4, v_num=, reduced_train_loss=1.390, global_step=182.0, consumed_samples=1456.0, val_loss=1.420]\n",
      "Epoch 0:  88%|████████▊ | 194/220 [00:12<00:01, 15.70it/s, loss=1.4, v_num=, reduced_train_loss=1.380, global_step=183.0, consumed_samples=1464.0, val_loss=1.420]\n",
      "Epoch 0:  89%|████████▊ | 195/220 [00:12<00:01, 15.72it/s, loss=1.4, v_num=, reduced_train_loss=1.380, global_step=183.0, consumed_samples=1464.0, val_loss=1.420]\n",
      "Epoch 0:  89%|████████▊ | 195/220 [00:12<00:01, 15.71it/s, loss=1.4, v_num=, reduced_train_loss=1.410, global_step=184.0, consumed_samples=1472.0, val_loss=1.420]\n",
      "Epoch 0:  89%|████████▉ | 196/220 [00:12<00:01, 15.73it/s, loss=1.4, v_num=, reduced_train_loss=1.410, global_step=184.0, consumed_samples=1472.0, val_loss=1.420]\n",
      "Epoch 0:  89%|████████▉ | 196/220 [00:12<00:01, 15.73it/s, loss=1.4, v_num=, reduced_train_loss=1.410, global_step=185.0, consumed_samples=1480.0, val_loss=1.420]\n",
      "Epoch 0:  90%|████████▉ | 197/220 [00:12<00:01, 15.75it/s, loss=1.4, v_num=, reduced_train_loss=1.410, global_step=185.0, consumed_samples=1480.0, val_loss=1.420]\n",
      "Epoch 0:  90%|████████▉ | 197/220 [00:12<00:01, 15.75it/s, loss=1.4, v_num=, reduced_train_loss=1.390, global_step=186.0, consumed_samples=1488.0, val_loss=1.420]\n",
      "Epoch 0:  90%|█████████ | 198/220 [00:12<00:01, 15.76it/s, loss=1.4, v_num=, reduced_train_loss=1.390, global_step=186.0, consumed_samples=1488.0, val_loss=1.420]\n",
      "Epoch 0:  90%|█████████ | 198/220 [00:12<00:01, 15.76it/s, loss=1.4, v_num=, reduced_train_loss=1.380, global_step=187.0, consumed_samples=1496.0, val_loss=1.420]\n",
      "Epoch 0:  90%|█████████ | 199/220 [00:12<00:01, 15.76it/s, loss=1.4, v_num=, reduced_train_loss=1.380, global_step=187.0, consumed_samples=1496.0, val_loss=1.420]\n",
      "Epoch 0:  90%|█████████ | 199/220 [00:12<00:01, 15.76it/s, loss=1.4, v_num=, reduced_train_loss=1.370, global_step=188.0, consumed_samples=1504.0, val_loss=1.420]\n",
      "Epoch 0:  91%|█████████ | 200/220 [00:12<00:01, 15.75it/s, loss=1.4, v_num=, reduced_train_loss=1.370, global_step=188.0, consumed_samples=1504.0, val_loss=1.420]\n",
      "Epoch 0:  91%|█████████ | 200/220 [00:12<00:01, 15.75it/s, loss=1.4, v_num=, reduced_train_loss=1.380, global_step=189.0, consumed_samples=1512.0, val_loss=1.420]\n",
      "Epoch 0:  91%|█████████▏| 201/220 [00:12<00:01, 15.76it/s, loss=1.4, v_num=, reduced_train_loss=1.380, global_step=189.0, consumed_samples=1512.0, val_loss=1.420]\n",
      "Epoch 0:  91%|█████████▏| 201/220 [00:12<00:01, 15.76it/s, loss=1.4, v_num=, reduced_train_loss=1.420, global_step=190.0, consumed_samples=1520.0, val_loss=1.420]\n",
      "Epoch 0:  92%|█████████▏| 202/220 [00:12<00:01, 15.78it/s, loss=1.4, v_num=, reduced_train_loss=1.420, global_step=190.0, consumed_samples=1520.0, val_loss=1.420]\n",
      "Epoch 0:  92%|█████████▏| 202/220 [00:12<00:01, 15.78it/s, loss=1.4, v_num=, reduced_train_loss=1.400, global_step=191.0, consumed_samples=1528.0, val_loss=1.420]\n",
      "Epoch 0:  92%|█████████▏| 203/220 [00:12<00:01, 15.79it/s, loss=1.4, v_num=, reduced_train_loss=1.400, global_step=191.0, consumed_samples=1528.0, val_loss=1.420]\n",
      "Epoch 0:  92%|█████████▏| 203/220 [00:12<00:01, 15.79it/s, loss=1.39, v_num=, reduced_train_loss=1.390, global_step=192.0, consumed_samples=1536.0, val_loss=1.420]\n",
      "Epoch 0:  93%|█████████▎| 204/220 [00:12<00:01, 15.81it/s, loss=1.39, v_num=, reduced_train_loss=1.390, global_step=192.0, consumed_samples=1536.0, val_loss=1.420]\n",
      "Epoch 0:  93%|█████████▎| 204/220 [00:12<00:01, 15.81it/s, loss=1.39, v_num=, reduced_train_loss=1.390, global_step=193.0, consumed_samples=1544.0, val_loss=1.420]\n",
      "Epoch 0:  93%|█████████▎| 205/220 [00:12<00:00, 15.83it/s, loss=1.39, v_num=, reduced_train_loss=1.390, global_step=193.0, consumed_samples=1544.0, val_loss=1.420]\n",
      "Epoch 0:  93%|█████████▎| 205/220 [00:12<00:00, 15.83it/s, loss=1.39, v_num=, reduced_train_loss=1.410, global_step=194.0, consumed_samples=1552.0, val_loss=1.420]\n",
      "Epoch 0:  94%|█████████▎| 206/220 [00:13<00:00, 15.84it/s, loss=1.39, v_num=, reduced_train_loss=1.410, global_step=194.0, consumed_samples=1552.0, val_loss=1.420]\n",
      "Epoch 0:  94%|█████████▎| 206/220 [00:13<00:00, 15.84it/s, loss=1.39, v_num=, reduced_train_loss=1.380, global_step=195.0, consumed_samples=1560.0, val_loss=1.420]\n",
      "Epoch 0:  94%|█████████▍| 207/220 [00:13<00:00, 15.86it/s, loss=1.39, v_num=, reduced_train_loss=1.380, global_step=195.0, consumed_samples=1560.0, val_loss=1.420]\n",
      "Epoch 0:  94%|█████████▍| 207/220 [00:13<00:00, 15.86it/s, loss=1.39, v_num=, reduced_train_loss=1.410, global_step=196.0, consumed_samples=1568.0, val_loss=1.420]\n",
      "Epoch 0:  95%|█████████▍| 208/220 [00:13<00:00, 15.87it/s, loss=1.39, v_num=, reduced_train_loss=1.410, global_step=196.0, consumed_samples=1568.0, val_loss=1.420]\n",
      "Epoch 0:  95%|█████████▍| 208/220 [00:13<00:00, 15.87it/s, loss=1.39, v_num=, reduced_train_loss=1.390, global_step=197.0, consumed_samples=1576.0, val_loss=1.420]\n",
      "Epoch 0:  95%|█████████▌| 209/220 [00:13<00:00, 15.89it/s, loss=1.39, v_num=, reduced_train_loss=1.390, global_step=197.0, consumed_samples=1576.0, val_loss=1.420]\n",
      "Epoch 0:  95%|█████████▌| 209/220 [00:13<00:00, 15.89it/s, loss=1.39, v_num=, reduced_train_loss=1.400, global_step=198.0, consumed_samples=1584.0, val_loss=1.420]\n",
      "Epoch 0:  95%|█████████▌| 210/220 [00:13<00:00, 15.90it/s, loss=1.39, v_num=, reduced_train_loss=1.400, global_step=198.0, consumed_samples=1584.0, val_loss=1.420]\n",
      "Epoch 0:  95%|█████████▌| 210/220 [00:13<00:00, 15.90it/s, loss=1.39, v_num=, reduced_train_loss=1.390, global_step=199.0, consumed_samples=1592.0, val_loss=1.420]\n",
      "\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:00, 12.19it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▌| 211/220 [00:13<00:00, 15.84it/s, loss=1.39, v_num=, reduced_train_loss=1.390, global_step=199.0, consumed_samples=1592.0, val_loss=1.420]\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:00, 16.00it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▋| 212/220 [00:13<00:00, 15.86it/s, loss=1.39, v_num=, reduced_train_loss=1.390, global_step=199.0, consumed_samples=1592.0, val_loss=1.420]\n",
      "\n",
      "Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:00, 16.09it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 213/220 [00:13<00:00, 15.86it/s, loss=1.39, v_num=, reduced_train_loss=1.390, global_step=199.0, consumed_samples=1592.0, val_loss=1.420]\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 4/10 [00:00<00:00, 19.97it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 214/220 [00:13<00:00, 15.92it/s, loss=1.39, v_num=, reduced_train_loss=1.390, global_step=199.0, consumed_samples=1592.0, val_loss=1.420]\n",
      "\n",
      "Validation DataLoader 0:  50%|█████     | 5/10 [00:00<00:00, 22.20it/s]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 215/220 [00:13<00:00, 15.97it/s, loss=1.39, v_num=, reduced_train_loss=1.390, global_step=199.0, consumed_samples=1592.0, val_loss=1.420]\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 6/10 [00:00<00:00, 25.03it/s]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 216/220 [00:13<00:00, 16.02it/s, loss=1.39, v_num=, reduced_train_loss=1.390, global_step=199.0, consumed_samples=1592.0, val_loss=1.420]\n",
      "\n",
      "Validation DataLoader 0:  70%|███████   | 7/10 [00:00<00:00, 27.69it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▊| 217/220 [00:13<00:00, 16.08it/s, loss=1.39, v_num=, reduced_train_loss=1.390, global_step=199.0, consumed_samples=1592.0, val_loss=1.420]\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 8/10 [00:00<00:00, 29.94it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 218/220 [00:13<00:00, 16.14it/s, loss=1.39, v_num=, reduced_train_loss=1.390, global_step=199.0, consumed_samples=1592.0, val_loss=1.420]\n",
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 9/10 [00:00<00:00, 32.10it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 219/220 [00:13<00:00, 16.20it/s, loss=1.39, v_num=, reduced_train_loss=1.390, global_step=199.0, consumed_samples=1592.0, val_loss=1.420]\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:00<00:00, 34.09it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 220/220 [00:13<00:00, 16.25it/s, loss=1.39, v_num=, reduced_train_loss=1.390, global_step=199.0, consumed_samples=1592.0, val_loss=1.420]\n",
      "Epoch 0: 100%|██████████| 220/220 [00:13<00:00, 16.25it/s, loss=1.39, v_num=, reduced_train_loss=1.390, global_step=199.0, consumed_samples=1592.0, val_loss=1.380]\n",
      "\n",
      "                                                                        \u001b[AEpoch 0, global step 200: 'val_loss' reached 1.38422 (best 1.38422), saving model to '/result/nemo_experiments/esm1nv-oas/esm1nv-oas_pretraining/checkpoints/megatron_bert--val_loss=1.38-step=200-consumed_samples=1600.0.ckpt' as top 10\n",
      "[NeMo I 2023-08-17 16:22:48 nlp_overrides:226] Removing checkpoint: /result/nemo_experiments/esm1nv-oas/esm1nv-oas_pretraining/checkpoints/megatron_bert--val_loss=1.42-step=100-consumed_samples=800.0-last.ckpt\n",
      "\n",
      "Epoch 0: 100%|██████████| 220/220 [00:14<00:00, 14.79it/s, loss=1.39, v_num=, reduced_train_loss=1.390, global_step=199.0, consumed_samples=1592.0, val_loss=1.380]`Trainer.fit` stopped: `max_steps=200` reached.\n",
      "\n",
      "Epoch 0: 100%|██████████| 220/220 [00:14<00:00, 14.79it/s, loss=1.39, v_num=, reduced_train_loss=1.390, global_step=199.0, consumed_samples=1592.0, val_loss=1.380]\n",
      "[NeMo I 2023-08-17 16:22:49 pretrain_oas:24] ************** Finished Training ***********\n"
     ]
    }
   ],
   "source": [
    "! rm -rf /result/nemo_experiments/esm1nv-oas\n",
    "std_out = ! cd {BIONEMO_WORKSPACE}/examples/protein/esm1nv && python pretrain_oas.py\n",
    "print('\\n'.join(std_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1a330c",
   "metadata": {},
   "source": [
    "### Results \n",
    "\n",
    "The training run will create a directory called `esm1nv-oas_pretraining` in `/result/nemo_experiments/esm1nv-oas` containing the files (logs, checkpoints, etc.) for the training run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a00a635",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints\n",
      "cmd-args.log\n",
      "events.out.tfevents.1692289352.drugdiscovery3-dt.335.0\n",
      "git-info.log\n",
      "hparams.yaml\n",
      "lightning_logs.txt\n",
      "nemo_error_log.txt\n",
      "nemo_log_globalrank-0_localrank-0.txt\n"
     ]
    }
   ],
   "source": [
    "! ls /result/nemo_experiments/esm1nv-oas/esm1nv-oas_pretraining"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
