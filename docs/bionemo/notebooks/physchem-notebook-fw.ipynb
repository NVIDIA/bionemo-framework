{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning LLM in BioNeMo for a Downstream Task"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook covers how to fine-tune MegaMolBART (MMB) for downstream task of predicting physicochemical properties of drugs._\n",
    "\n",
    "One of the improtant tasks for chemoinformaticians is to develop models for predicting properties of small molecules.\n",
    "\n",
    "These properties may include physicochemical parameters, such as lipophilicity, solubility, hydration free energy (LogP, LogD, and so on). It can also include certain pharmacokinetic/dynamic behaviors, such as Blood-Brain-Barrier/CNS permeability and Vd.\n",
    "\n",
    "Modeling such properties can often become challenging along with choosing the appropriate and relevant descriptors/features for developing such prediction models.\n",
    "\n",
    "In this notebook, we will use the encoder of pretrained Megamolbart model and add a MLP prediction head trained for physico-chemical parameter predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Assumptions\n",
    "\n",
    "This tutorial assumes that the user has access to BioNeMo framework and NVIDIA's BCP and DGX-Cloud compute infrastructure. The user is also expected to have required background details about \n",
    "- the BioNeMo framework, as described in the <a href=\"../quickstart-fw.md\"> Quickstart Guide</a>, and \n",
    "- running the model training jobs on <a href=\"../bcp-specific-commands-fw.md\"> BCP</a>. \n",
    "\n",
    "All model training and finetuning related commands should be executed inside the BioNeMo docker container.\n",
    "\n",
    "The working directory needs to be ``/workspace/bionemo/examples/molecule/megamolbart`` for updating and running the following code. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section below, we will be using the one of the following datasets curated by MoleculeNet -- ESOL dataset (https://moleculenet.org/datasets-1)\n",
    "\n",
    "* Lipophilicity: Experimental results of octanol/water distribution coefficient(logD at pH 7.4) [n=4200]\n",
    "* FreeSolv: Experimental and calculated hydration free energy of small molecules in water [n=642]\n",
    "* ESOL: Water solubility data(log solubility in mols per litre) for common organic small molecules [n=1129]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Train Model for Compound Water Solubility (ESOL) Prediction using MMB Pretrained Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch BioNeMo development container "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "bash launch.sh dev"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locate physchem downstream task config in ``/workspace/bionemo/examples/molecule/megamolbart/conf/finetune_config.yaml``\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and preprocess dataset easily using BioNeMo yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#Set download location for datasets inside finetune_config.yaml\n",
    "data:\n",
    "    dataset_path: /data/physchem\n",
    "\n",
    "    #Let's also ensure that our dataset is split to create training, validation and test sets\n",
    "    split_data: True\n",
    "    val_frac: 0.15 # proportion of samples used for validation set\n",
    "    test_frac: 0.15 # proportion of samples used for test set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ensure that we don't try to train a model yet and instead run preprocess steps. \n",
    "\n",
    "Simply set do_training to False inside our yaml config. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "do_training: False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets run our downstream script and pass in the finetune_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#assuming pwd: /workspace/bionemo/examples/molecule/megamolbart\n",
    "python downstream_physchem.py --config-path=./conf/ --config-name=finetune_config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now have datasets for all three properties downloaded from MoleculeNet in our /data/phsychem folder.\n",
    "\n",
    "Now's we'll specify which datasets we want to use to train our MLP prediction head by once again using our yaml config.  This is done by simply setting model.data.task_name parameter to the name of the folder where we stored our ESOL dataset which is 'delaney-processed'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "data:\n",
    "    task_name: SAMPL #specifies which MoleculeNet physchem dataset to use for training, expected values: SAMPL, Lipophilicity, or delaney-processed\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's indicated which column contains our SMILES string and our target value of interest for training the model based on the column headers in the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "data:\n",
    "    sequence_column: 'smiles'\n",
    "    target_column: 'measured log solubility in mols per litre'\n",
    "\n",
    "#These parameters are all nested under the data key in the yaml file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to ensure that our script skips preprocessing and trains a model by setting do_training to True and we ensure that do_testing is set to True to also use of test dataset for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "do_training: True\n",
    "do_testing: True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we have already set reasonable defaults for the other model paramters necessary for training, it is important to note the parameters of the config.\n",
    "\n",
    "The path to the pretrained megamolbart model should be already set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "restore_from_path: /model/molecule/megamolbart/megamolbart.nemo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the model parameters, we can set whether we want the encoder to be frozen or not, our micro batch size and other downstream task parameters used by the MLP prediction head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "model:\n",
    "    encoder_frozen: True\n",
    "    micro_batch_size: 32\n",
    "\n",
    "    downstream_task:\n",
    "        n_outputs: 1\n",
    "        hidden_layer_size: 128\n",
    "        loss_func: MSELoss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to train a model for our downstream task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assuming pwd: /workspace/bionemo/examples/molecule/megamolbart\n",
    "python downstream_physchem.py --config-path=./conf/ --config-name=finetune_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#A successful run should display summary statistics of training the model. An example of this is shown below: \n",
    "\n",
    "wandb: Run summary:\n",
    "wandb:       consumed_samples 76800.0\n",
    "wandb:                  epoch 100\n",
    "wandb:            global_step 2399.0\n",
    "wandb:                     lr 0.00077\n",
    "wandb:     reduced_train_loss 0.17997\n",
    "wandb:              test_loss 0.47823\n",
    "wandb:       test_step_timing 0.00071\n",
    "wandb:  train_backward_timing 0.00076\n",
    "wandb:      train_step_timing 0.00416\n",
    "wandb:    trainer/global_step 2400\n",
    "wandb:               val_loss 0.42474\n",
    "wandb: validation_step_timing 0.00153"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "If the samples in the experiment folder are already processed, it will give an runtime error ``no samples left to consume``. To avoid this error, users can either delete or move the experiment folder, or set ``++exp_manager.resume_if_exists=false`` in the command line argument. \n",
    ":::\n",
    "\n",
    "Now that we've trained on the ESOL dataset, to change which dataset we train is simple. To do a run where we train using the Lipophilicity dataset instead can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python downstream_physchem.py \\\n",
    "--config-path=./conf/ --config-name=finetune_config \\\n",
    "model.data.task_name=Lipophilicity \\\n",
    "model.data.target_column=exp \\\n",
    "++exp_manager.resume_if_exists=false"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Logging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of your experiment, including model checkpoints, can then be found in /result/nemo_experiments/. \n",
    "\n",
    "All logs from the experiment as well as the config parameters used to run that experiments are stores here as well.   \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Optional]** Setting up Weights and Biases account for tracking the model training process\n",
    "\n",
    "If you would like to monitor the MegaMolBART model training process, set up Weights and Biases access by following the links:\n",
    "\n",
    "1. For setting up the account: https://wandb.ai/site  \n",
    "2. Once the account is set, copy the API key: https://wandb.ai/authorize  \n",
    "3. Use this key in your .env file.  \n",
    "\n",
    "\n",
    "Monitoring the model training progress with Weights and Biases\n",
    "\n",
    "Following are examples plots showing the model training run, as logged and plotted by weights and Biases (www.wandb.ai)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Wandb Plots](../images/wandai_charts.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Optional**] Model parameters can also be changed by passing them as arguments to the script. This removes the need to edit and save the yaml config each time. \n",
    "\n",
    "For example, we could run both the preprocessing step and model training using the commands below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing\n",
    "python downstream_physchem.py --config-path=./conf/ --config-name=finetune_config \\\n",
    "do_training=False \\\n",
    "model.data.dataset_path=/data/physchem \\\n",
    "model.data.split_data=True model.data.val_frac=0.15 \\\n",
    "model.data.test_frac=0.15 \n",
    "\n",
    "#Model Training for downstream task\n",
    "python downstream_physchem.py --config-path=./conf/ --config-name=finetune_config \\\n",
    "do_training=True \\\n",
    "restore_from_path=/model/molecule/megamolbart/megamolbart.nemo \\\n",
    "model.encoder_frozen=True \\\n",
    "model.micro_batch_size=32 \\\n",
    "model.data.train_ds.data_file=/data/physchem/delaney-processed_splits/train.csv \\\n",
    "model.data.validation_ds.data_file=/data/physchem/delaney-processed_splits/val.csv \\\n",
    "model.data.test_ds.data_file=/data/physchem/delaney-processed_splits/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
