{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c252c4d8",
   "metadata": {},
   "source": [
    "# MolMIM ZINC15 Training Dataset Setup\n",
    "### Preparation of a drug-like subset of ZINC15 dataset\n",
    "The [ZINC15](https://zinc15.docking.org/) dataset is a freely available catalog of commercially purchasable chemical compounds. It is desirable to make some changes to the dataset before training chemical language models like MegaMolBART or MolMIM. In this notebook, we will walk through the steps to create a drug-like subset of the ZINC15 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620321b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15553f5",
   "metadata": {},
   "source": [
    "### 1. Download files\n",
    "Raw data files will be downloaded to `/workspace/bionemo/data/raw/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962563c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will download just 1 file for now and illustrate the filtering steps on it\n",
    "\n",
    "! mkdir -p /workspace/bionemo/data/raw\n",
    "! wget http://files.docking.org/2D/AB/ABAC.txt + /workspace/bionemo/raw/ABAC.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14766ea6",
   "metadata": {},
   "source": [
    "### 2. Process files\n",
    "#### 2.1 Compute molecular properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9179d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, Crippen, QED\n",
    "\n",
    "def calculate_descriptors(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return {\n",
    "        \"Molecular Weight\": np.nan,\n",
    "        \"cLogP\": np.nan,\n",
    "        \"Number of Hydrogen Bond Donors\": np.nan,\n",
    "        \"Number of Hydrogen Bond Acceptors\": np.nan,\n",
    "        \"QED Score\": np.nan,\n",
    "        \"Canonical SMILES\": np.nan  # Add canonical SMILES to the output\n",
    "    }\n",
    "    \n",
    "    mol_weight = Descriptors.ExactMolWt(mol)\n",
    "    clogp = Crippen.MolLogP(mol)\n",
    "    hbd = Descriptors.NumHDonors(mol)\n",
    "    hba = Descriptors.NumHAcceptors(mol)\n",
    "    qed_score = QED.qed(mol)\n",
    "    canonical_smiles = Chem.MolToSmiles(mol)  # Generate canonical SMILES\n",
    "    \n",
    "    return {\n",
    "        \"Molecular Weight\": mol_weight,\n",
    "        \"cLogP\": clogp,\n",
    "        \"Number of Hydrogen Bond Donors\": hbd,\n",
    "        \"Number of Hydrogen Bond Acceptors\": hba,\n",
    "        \"QED Score\": qed_score,\n",
    "        \"canon_smiles\": canonical_smiles  # Add canonical SMILES to the output\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb2a908",
   "metadata": {},
   "outputs": [],
   "source": [
    "zinc_file = \"ABAC.txt\"\n",
    "input_df = pd.read_csv(zinc_file, sep=\"\\t\", usecols=[\"zinc_id\", \"smiles\"])\n",
    "\n",
    "results = []\n",
    "for zinc_id, smiles in tqdm(zip(input_df[\"zinc_id\"], input_df[\"smiles\"]), total=len(input_df)):\n",
    "    descriptor = calculate_descriptors(smiles)\n",
    "    descriptor['zinc_id'] = zinc_id  # Keep the original zinc_id\n",
    "    descriptor['smiles'] = smiles  # Keep the original input SMILES\n",
    "    results.append(descriptor)\n",
    "prop_df = pd.DataFrame(results) # computed properties are stored in this DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20ba0f9",
   "metadata": {},
   "source": [
    "#### 2.2 Get vocabulary compliance\n",
    "Get the vocabulary of the SMILES strings and ensure that it is a subset of the allowed vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2748d396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.common.tokenizers.regex_tokenizer import RegExTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb30c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File for allowed vocabulary\n",
    "# Note: This file can be changed according to the user's needs\n",
    "TOKENIZER_VOCAB = os.environ[\"BIONEMO_HOME\"] + \"/tokenizers/molecule/megamolbart/vocab/megamolbart.vocab\"\n",
    "MAX_TOKEN_LENGTH = 126\n",
    "\n",
    "TOKENIZER_MODEL = os.environ[\"BIONEMO_HOME\"] + \"/tokenizers/molecule/megamolbart/vocab/megamolbart.model\"\n",
    "regex_tokenizer = RegExTokenizer().load_tokenizer(regex_file=TOKENIZER_MODEL, vocab_file=TOKENIZER_VOCAB)\n",
    "\n",
    "with open(TOKENIZER_VOCAB, \"r\") as f:\n",
    "    allowed_vocab = f.readlines()\n",
    "curr_allowed_vocab = set([v.strip() for v in allowed_vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a780aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_compliance_check(smiles, tokenizer, vocab_allowed, max_token_length):\n",
    "    \"\"\"Checks if the SMILES string only contains vocabulary in `allowed_vocab`\n",
    "    and if the token length is less than or equal to `max_token_length\"\"\"\n",
    "    tokens = set(tokenizer.text_to_tokens(smiles))\n",
    "    return tokens.issubset(vocab_allowed) and len(tokens) <= max_token_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a840a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_df[\"vocab_compliant\"] = prop_df[\"canon_smiles\"].progress_apply(lambda smi: vocab_compliance_check(\n",
    "    smi, regex_tokenizer, curr_allowed_vocab, MAX_TOKEN_LENGTH))\n",
    "prop_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b709fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_df[\"vocab_compliant\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f34d016",
   "metadata": {},
   "source": [
    "#### 2.3 Filter out undesirable molecules\n",
    "In this step, we filter out non-druglike molecules and molecules that do not adhere to the allowed vocabulary and token length. Druglikeness is estimate using the following criteria.\n",
    "1. [Lipinski's rule of 5 compliance](https://www.sciencedirect.com/science/article/abs/pii/S0169409X96004231)\n",
    "2. [Quantitative Estimate of Druglikeness](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3524573/) (QED score) with a cutoff of `0.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a36e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_param_string = \"\"\"Molecular Weight: # Lipinski\n",
    "  min: null\n",
    "  max: 500\n",
    "Number of Hydrogen Bond Donors: # Lipinski\n",
    "  min: null\n",
    "  max: 5\n",
    "Number of Hydrogen Bond Acceptors: # Lipinski\n",
    "  min: null\n",
    "  max: 10\n",
    "cLogP: # Lipinski\n",
    "  min: null\n",
    "  max: 5\n",
    "QED Score:\n",
    "  min: 0.5\n",
    "  max: null\n",
    "\"\"\"\n",
    "filter_params = yaml.safe_load(filter_param_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2f16d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_filter_mask(df, prop, min_val=None, max_val=None):\n",
    "    \"\"\"Returns filter mask by filtering on min_val and max_val of a prop (inclusive range) \"\"\"\n",
    "    if min_val is not None and max_val is not None:\n",
    "        return (df[prop] >= min_val) & (df[prop] <= max_val)\n",
    "    if min_val is None and max_val is not None:\n",
    "        return df[prop] <= max_val\n",
    "    if min_val is not None and max_val is None:\n",
    "        return df[prop] >= min_val\n",
    "    if min_val is None and max_val is None:\n",
    "        raise ValueError(f\"Both min_val and max_val cannot be None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52dd879",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_init = len(prop_df)\n",
    "print(f\"Number of molecules before filtering: {n_init}\")\n",
    "filter_mask = pd.Series(True, index=prop_df.index)\n",
    "for prop in filter_params.keys():\n",
    "    ifilter = make_filter_mask(prop_df, prop, filter_params[prop][\"min\"], filter_params[prop][\"max\"])\n",
    "    filter_mask = filter_mask & ifilter\n",
    "\n",
    "prop_filt_df = prop_df[filter_mask & prop_df[\"vocab_compliant\"]][[\"zinc_id\", \"canon_smiles\"]]\n",
    "prop_filt_df = prop_filt_df.reset_index(drop=True)\n",
    "n_filt = len(prop_filt_df)\n",
    "print(f\"Number of molecules left after filtering: {n_filt}\")\n",
    "print(f\"Percentage of molecules filtered out: {((n_init-n_filt)/n_init) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d71bee8",
   "metadata": {},
   "source": [
    "### 3. Write filtered output to file\n",
    "Filtered files will be written to `/workspace/bionemo/data/filtered/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5ffc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p /workspace/bionemo/data/filtered/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8407036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set name of output file here\n",
    "processed_filepath = \"/workspace/bionemo/data/filtered/\"\n",
    "processed_filename = f\"{processed_filepath}/ABAC_filtered.txt\"\n",
    "prop_filt_df.to_csv(processed_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be1dda5",
   "metadata": {},
   "source": [
    "### 4. Split files into chunks of 10100000\n",
    "10.1M dataset shards will be written to `/workspace/bionemo/data/split_data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64920084",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p /workspace/bionemo/data/split_data/\n",
    "! cd /workspace/bionemo/data/split_data/; tail -q -n +2 /workspace/bionemo/data/filtered/** | split -d -l 10100000 -a 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5920dc4",
   "metadata": {},
   "source": [
    "### 5. Split data into train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac95663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train/val_full/test_full sets\n",
    "split_data_file_path = \"/workspace/bionemo/data/split_data/\"\n",
    "output_dir = \"/workspace/bionemo/data/\"\n",
    "os.makedirs(f\"{output_dir}/train\", exist_ok=True)\n",
    "os.makedirs(f\"{output_dir}/val_full\", exist_ok=True)\n",
    "os.makedirs(f\"{output_dir}/test_full\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffc5412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for splitting\n",
    "test_frac = 0.005\n",
    "val_frac = 0.005\n",
    "number_of_files = 1\n",
    "\n",
    "for file_idx in range(number_of_files):\n",
    "    file = f\"x{file_idx:03d}\"\n",
    "    input_file = f\"{split_data_file_path}/{file}\"\n",
    "    print(f\"Reading input file: {input_file}\")\n",
    "    input_df = pd.read_csv(input_file, header=None, names=['zinc_id', 'canon_smiles'], usecols=[0, 1])\n",
    "\n",
    "    # Calculate sample sizes before size of dataframe changes\n",
    "    test_samples = max(int(test_frac * input_df.shape[0]), 1)\n",
    "    val_samples = max(int(val_frac * input_df.shape[0]), 1)\n",
    "\n",
    "    test_df = input_df.sample(n=test_samples, random_state=file_idx)\n",
    "    input_df = input_df.drop(test_df.index)  # remove test data from training data\n",
    "\n",
    "    val_df = input_df.sample(n=val_samples, random_state=file_idx)\n",
    "    input_df = input_df.drop(val_df.index)  # remove validation data from training data\n",
    "\n",
    "    input_df.to_csv(f'{output_dir}/train/{file}.csv', index=False)\n",
    "    test_df.to_csv(f'{output_dir}/test_full/{file}.csv', index=False)\n",
    "    val_df.to_csv(f'{output_dir}/val_full/{file}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5256d775",
   "metadata": {},
   "source": [
    "### [OPTIONAL] 6. Clustering validation and test set for faster evaluation\n",
    "We can reduce the size of the validation and test sets to speed up our validation in loop and test evaluation. In order to do this without affecting the diversity of the full validation and test tests, we cluster the molecules and pick some exemplars from the clusters to form smaller validation and test sets. We perform clustering using ECFP4/128-bit fingerprint with `MiniBatchKMeans` clustering algorithm.\n",
    "Clustered validation set outputs are written to `/workspace/bionemo/data/val/` and clustered test set outputs are written to `/workspace/bionemo/data/test/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165e61a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a102f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_dataset(file_path, num_files, num_clusters, batch_size, sample_frac):\n",
    "    \"\"\"Clusters dataset by ECFP4/128-bit similarity\n",
    "    file_path: Path to files\n",
    "    num_clusters: Number of desired clusters\n",
    "    batch_size: Batch size for MiniBatchKmeans\n",
    "    sample_frac: Fraction of each cluster to select\"\"\"\n",
    "    all_files = [Path(f\"{file_path}/x{i:03}.csv\") for i in range(num_files)]\n",
    "\n",
    "    compound_df = pd.concat([pd.read_csv(f) for f in all_files], ignore_index=True)\n",
    "\n",
    "    morgan_fp_128 = rdFingerprintGenerator.GetMorganGenerator(radius=2,fpSize=128)\n",
    "    fps_128 = compound_df[\"canon_smiles\"].progress_apply(lambda s: morgan_fp_128.GetFingerprintAsNumPy(Chem.MolFromSmiles(s)))\n",
    "    X_fp_128 = np.concatenate([fp.reshape(-1, 1) for fp in fps_128.to_list()], axis=1).T\n",
    "    \n",
    "    # Perform clustering\n",
    "    cluster_labels = MiniBatchKMeans(n_clusters=num_clusters, batch_size=batch_size, \n",
    "                                     random_state=0, n_init=\"auto\").fit(X_fp_128)\n",
    "    compound_df[\"cluster_label\"] = cluster_labels.labels_\n",
    "    sample_df = compound_df.groupby('cluster_label').apply(lambda x: x.sample(frac=sample_frac, random_state=0))\n",
    "\n",
    "    return sample_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa4d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set fraction of validation set for \n",
    "val_frac = \n",
    "\n",
    "number_of_files = 176\n",
    "num_clusters = 10000\n",
    "batch_size = 200000\n",
    "val_cluster_df = cluster_dataset(\"/workspace/bionemo/data/val_full/\", number_of_files, \n",
    "                                 num_clusters, batch_size, val_frac)\n",
    "\n",
    "os.makedirs(\"/workspace/bionemo/data/val/\", exist_ok=True)\n",
    "val_cluster_df[[\"zinc_id\", \"canon_smiles\"]].to_csv(\"/workspace/bionemo/data/val/val_clustered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc664ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set fraction of validation set for \n",
    "test_frac = \n",
    "\n",
    "number_of_files = 176\n",
    "num_clusters = 10000\n",
    "batch_size = 200000\n",
    "test_cluster_df = cluster_dataset(\"/workspace/bionemo/data/test_full/\", number_of_files, \n",
    "                                 num_clusters, batch_size, test_frac)\n",
    "\n",
    "os.makedirs(\"/workspace/bionemo/data/test/\", exist_ok=True)\n",
    "val_cluster_df[[\"zinc_id\", \"canon_smiles\"]].to_csv(\"/workspace/bionemo/data/test/test_clustered.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
