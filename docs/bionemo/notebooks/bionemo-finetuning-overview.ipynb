{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune pre-trained models in BioNeMo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example covers general fine-tuning capability of BioNeMo framework and also the NeMo framework, which BioNeMo is based on.\n",
    "\n",
    "Transfer learning is an important machine learning technique that uses a modelâ€™s knowledge of one task to make it perform better on another. Fine-tuning is one of the techniques to perform transfer learning. It is an essential part of the recipe for many state-of-the-art results where a base model is first pretrained on a task with abundant training data and then fine-tuned on different tasks of interest where the training data is less abundant or even scarce. \n",
    "\n",
    "\n",
    "## Setup and Assumptions\n",
    "\n",
    "This tutorial assumes that a copy of the BioNeMo framework repo exists on workstation or server and has been mounted inside the container at `/workspace/bionemo` as described in the <a href=\"../quickstart-fw.html#code-development\">Code Development section of the Quickstart Guide</a>. This path will be referred to with the variable `BIONEMO_WORKSPACE` in the tutorial. \n",
    "\n",
    "All commands should be executed inside the BioNeMo docker container."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune MMB model in BioNeMo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finetuning Configuration**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BioNeMo framework supports easy fine-tuning by partially/fully loading the pretrained weights from a checkpoint into the currently instantiated model. Note that the currently instantiated model should have parameters that match the pre-trained checkpoint (such that weights may load properly). \n",
    "\n",
    "Pre-trained weights can be provided using a path to a NeMo model (via ``restore_from_path``). This is done through:\n",
    "\n",
    "* adding ```restore_from_path``` to the config yaml\n",
    "* passing ```restore_from_path``` as a command line argument into your script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python examples/molecule/megamolbart/pretrain.py \\\n",
    "    --config-path=<path to dir of configs> \\\n",
    "    --config-name=<name of config without .yaml>) \\\n",
    "    do_training=True \\\n",
    "    ++model.data.dataset.train=<data files> \\ # x000 for a single file for x_OP_000..186_CL_ for a range\n",
    "    ++model.data.dataset.val=<data files> \\\n",
    "    ++model.data.dataset.test=<data files> \\\n",
    "    trainer.devices=$NGC_GPUS_PER_NODE \\\n",
    "    trainer.accelerator='gpu' \\\n",
    "    restore_from_path=\"<path to .nemo model file>\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conveniently, we can follow this approach to finetune any other BioNeMo model as well. Simply change the training script path to the model of interest. For example, to finetune ESM-1nv model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python examples/protein/esm1nv/pretrain.py \\\n",
    "    --config-path=<path to dir of configs> \\\n",
    "    --config-name=<name of config without .yaml>) \\\n",
    "    do_training=True \\\n",
    "    ++model.data.dataset.train=<data files> \\ # x000 for a single file for x_OP_000..186_CL_ for a range\n",
    "    ++model.data.dataset.val=<data files> \\\n",
    "    ++model.data.dataset.test=<data files> \\\n",
    "    trainer.devices=$NGC_GPUS_PER_NODE \\\n",
    "    trainer.accelerator='gpu' \\\n",
    "    restore_from_path=\"<path to .nemo model file>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "It is important to have the dataset intended for model fine-tuning process in the format compatible with the model training datasets. For example, SMILES ot FASTA formats for small molecules and proteins, respectively. Mismatch in the expected dataset format could result into ``pickle`` errors, such as the following:\n",
    "```_pickle.PicklingError: Can't pickle <class 'Boost.Python.ArgumentError'>: import of module 'Boost.Python' failed```\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pretrained model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within a BioNeMo training script, to load a file for the purposes for fine-tuning we must use both:\n",
    "* the ```restore_from()``` method from NeMo\n",
    "* ```BioNeMoSaveRestoreConnector```\n",
    "\n",
    "These instructions are for loading fully trained checkpoints for fine-tuning. For resuming an unfinished training experiment, use the Experiment Manager to do so by setting the ``resume_if_exists`` flag to True.\n",
    "\n",
    "For more granular control over how resuming from a pretrained model is done, we created the ``BioNeMoSaveRestoreConnector``. Based on the NeMo ``NLPSaveRestoreConnector``, this allows for changes in the embedding matrix. In conjunction with the NeMo ``restore_from()`` method, you can set vocabulary size at the time of loading our model with ``BioNeMoSaveRestoreConnector`` if needed. An example can be found in our ``pretrain.py`` script for ProtT5-nv (``examples/protein/prott5nv/pretrain.py``)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "model = ProtT5nvModel.restore_from(\n",
    "    cfg.restore_from_path, cfg.model, trainer=trainer,\n",
    "    # 128 -- is the number of padded vocabulary in MegatronT5Model\n",
    "    save_restore_connector=BioNeMoSaveRestoreConnector(vocab_size=128),\n",
    "    # support loading weights with mismatch in embeddings (for example, alibi)\n",
    "    strict=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
