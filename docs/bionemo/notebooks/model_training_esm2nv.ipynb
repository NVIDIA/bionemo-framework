{
 "cells": [
  {
<<<<<<< HEAD
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESM2nv Model Training using BioNeMo "
=======
   "cell_type": "markdown",
   "id": "eae5c045-bec3-437f-a4e5-b5c39743d5bf",
   "metadata": {},
   "source": [
    "# ESM-2nv: Data Preprocessing and Model Training Using BioNeMo\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "  <b>NOTE:</b> This notebook has been tested on both an A1000 GPU and an A100, and is compatible with BioNeMo Framework v1.6, v1.7 and v1.8. The expected runtime is less than 1 hour on the A1000 and ~3 minutes on the A100.\n",
    "</div>\n",
    "\n",
    "\n",
    "### Demo Objectives\n",
    "The purpose of this tutorial is to provide an example use case for training a BioNeMo large language model using the BioNeMo framework. In this tutorial, you will gain experience in:\n",
    "\n",
    "1. Preprocessing the UniRef50 and UniRef90 data for ESM-2nv.\n",
    "2. Pretraining and continuing training from a checkpoint for ESM-2nv.\n",
    "3. Performing inference with ESM-2nv.\n",
    "\n",
    "### Overview - ESM-2nv Model\n",
    "\n",
    "**ESM-2nv** is based on the public ESM-2 model, which is a BERT architecture trained on millions of protein sequences from the UniProt database. ESM-2nv learns the patterns and dependencies between amino acids that ultimately give rise to a protein’s 2D structure. These can include properties such as alpha helix or beta sheet, as well as cellular location, thermostability, solubility, and other protein properties. For more information, check the [ESM-2nv model card](../models/esm2-nv.md)\n",
    "\n",
    "This ESM-2nv model training example walkthrough will show how to utilize compute resources, download and preprocess datasets, and perform model training on single and multiple nodes.\n",
    "\n",
    "The model was trained on UniRef50 and UniRef90 protein sequences, truncated to a maximum length of 1022 amino acids.\n",
    "\n",
    "### Setup\n",
    "Ensure that you have read through the [Getting Started](../index.md) section, can run the BioNeMo Framework Docker container, and have configured the NGC Command Line Interface (CLI) within the container. It is assumed that this notebook is being executed from within the container. Additionally, this tutorial depends on the [ESM-2nv](../models/esm2-nv.md) model.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> Some of the cells below generate long text output.  We're using <pre>%%capture --no-display --no-stderr cell_output</pre> to suppress this output.  Comment or delete this line in the cells below to restore full output.</div>\n",
    "\n",
    "### Import and install all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca974709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
>>>>>>> bionemo-gitlab/dorotat/rebase-bionemo1-dev
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "metadata": {},
   "source": [
    "The purpose of this tutorial is to provide an example use case of training a BioNeMo Large Language model using the BioNeMo framework. In this tutorial, the user will obtain experience in\n",
    "- adjusting config files, and setting launch parameters for ESM-2nv training jobs\n",
    "- launching single and multi-node, multi-gpu training runs\n",
    "- using NVIDIA's Base Command Platform commands for LLM model training"
=======
   "id": "311d9dfb-f927-400e-8c8b-af4a0962376f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Home Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2e561ce-d169-4846-bb09-d41b4b2898b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bionemo_home = \"/workspace/bionemo\"\n",
    "os.environ['BIONEMO_HOME'] = bionemo_home\n",
    "os.chdir(bionemo_home)"
>>>>>>> bionemo-gitlab/dorotat/rebase-bionemo1-dev
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "metadata": {},
   "source": [
    ":::{note}\n",
    "This tutorial focuses on ESM-2nv model training as an example, and the walk through can be easily modified for performing ProtT5-nv model training. The relevant config files and scripts for ProtT5-nv are provided in ``/workspace/bionemo/examples/protein/prott5nv/``.\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview - ESM2nv model\n",
    "\n",
    "**ESM-2nv** is based on the public ESM-2 model, which is a BERT architecture trained on millions of protein sequences from the UniProt database. ESM-2nv learns the patterns and dependencies between amino acids that ultimately give rise to a protein’s 2D structure. These can include properties such as alpha helix or beta sheet, plus cellular location, thermostability, solubility, and other protein properties. For more information, you can check the [ESM2nv model card](../models/esm2-nv.md)\n",
    "\n",
    "\n",
    "This ESM-2nv model training example walkthrough will show how to utilize the compute resources, download and preprocess the datasets, and perform model training on single and multiple nodes.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Before diving in, please ensure that you have completed all steps in the [Getting Started](./index.md) section.\n",
    "\n",
    "\n",
    "Additionally, this tutorial depends on:\n",
    "    - the [ESM-2](../models/esm2-nv.md) model\n",
    "    - use of [NGC BCP](../bcp-specific-commands-fw.md)\n",
    "    - creating an NGC workspace, described in [Preparing Workspace and Data for Pre-training](../preprocessing-bcp-training-esm2nv.md)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requesting compute resources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access to DGX compute resources via NGC site or NGC CLI\n",
    "\n",
    "As a prerequisite, configure your access to the DGX compute resources and required contents either via NVIDIA's [Base Command Platform](https://docs.nvidia.com/base-command-platform/index.html) or [NGC-CLI](https://docs.ngc.nvidia.com/cli/cmd.html) using ```ngc config set``` command. \n",
    "\n",
    "For more details on how to request the resources, visit [Running BioNeMo on DGX-Cloud using BCP](../../bcp-specific-commands-fw.md)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch an Interactive NGC Job\n",
    "First, let's request the resource for running the model training in an interactive manner. \n",
    "\n",
    "Here is an example command for requesting resources using NGC CLI. This command uses your NGC credentials to authenticate to NGC, requests the DGX instances requested under '--instance', retrieves the Docker image for BioNeMo provided via NGC, sets up the pre-training environment within a container launched from this image, and initiates a 'sleep infinity' command to launch a long-running interactive session.\n",
    "\n",
    "In the configuration below, update `{deploy_ngc_org}` and `{deploy_ngc_team}` with the correct NGC org and team name, respectively. If there is no team name, you can omit it. Refer to [NGC documentation](https://docs.ngc.nvidia.com/cli/cmd_batch.html#run) for more details.  \n",
    "\n",
    "Also, make sure to update the relevant arguments according to your compute setup, BioNeMo image, workspaces, instance types, and so on.\n",
    "You will need to create a NGC workspace and replace the WORKSPACE_ID in the commands below with the ID assigned to your workspace. Refer Part 1 at [Preparing Workspace and Data for Pre-Training](../preprocessing-bcp-training-esm2nv.md) or NGC documentation for more details. \n",
    "\n",
    "  ```bash\n",
    "  ngc base-command job run \\\n",
    "    --name \"example-training-1\" \\\n",
    "    --org {deploy_ngc_org} \\\n",
    "    --team {deploy_ngc_team} \\\n",
    "    --ace nv-us-east-2 \\\n",
    "    --instance INSTANCE_TYPE \\            # Compute node type, such as dgxa100.80g.8.norm \n",
    "    --array-type PYTORCH \\\n",
    "    --replicas 2 \\\n",
    "    --image \"nvcr.io/nvidia/clara/bionemo-framework:1.0\" \\     # Image path for BioNeMo\n",
    "    --result /results \\\n",
    "    --workspace WORKSPACE_ID:/example_training:RW \\ \n",
    "    --port 8888 \\\n",
    "    --total-runtime 1D \\\n",
    "    --preempt RUNONCE \\                   \n",
    "    --priority NORMAL \\                   # Priority level for the jog execution [LOW, NORMAL, HIGH]\n",
    "    --order 1 \\                           # Priority order for the jog execution [1-99]\n",
    "    --commandline \"sleep infinity\"  # This command can be replaced with the model training command: python pretrain.py....\n",
    "  ```\n",
    "\n",
    "<br>\n",
    "\n",
    "Store the JOB ID provided after running the command. You will need this ID for attaching to the JOB shell and runing the training commands. \n",
    "\n",
    "You can get information about the job's status using `ngc base-command info <YOUR-JOB-ID>`\n",
    "\n",
    "Once the resources are assigned for the job and the BioNeMo container is running, we'll proceed ahead via `ngc base-command job exec <YOUR-JOB-ID>`\n",
    "\n",
    "<br>\n",
    "\n",
    "**Notes**\n",
    "- The ``bcprun`` command provided in the cells [below](#single-node-or-multi-node-setup) can also be submitted as ``--commandline`` argument (instead of launching interactive shell).\n",
    "- The interactive job launch example shown here is using interactive shell interface. \n",
    "- For larger training jobs, it is **strongly** advised to launch them using the launch script as a part of the ``ngc base-command job run`` command, as mentioned in [Running BioNeMo on DGX-Cloud using BCP](../../bcp-specific-commands-fw.md). For ESM2nv training with the full dataset, we recommend luanching a model pre-training job using `<BioNeMO_Workspace>/examples/protein/esm2nv/scripts/pretrain_esm2_bcp.sh`.\n",
    "- The script expects the pretraining data to be preprocessed and saved in a mounted workspace, you can follow the steps detailed in [Preparing Workspace and Data for Pre-training](../../preprocessing-bcp-training-esm2nv.md) to prepare such a workspace, and to preprocess the raw dataset.  The script `pretrain_esm2_bcp.sh` script can be configured to read the pre-processed dataset directly from the NGC workspace."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing for a Down-sampled Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and pre-processing the example dataset\n",
    "\n",
    "To briefly showcase the model training capacities of BioNeMo Framework, we will use a very small subset of the original UniRef50 dataset (5,000 sequences) that is provided as a part of the sample datasets located in ```${BIONEMO_HOME}/examples/tests/test_data/uniref202104_esm2_qc```\n",
    "\n",
    "The data is stored in zip file, so run the following the command to get raw fasta files: \n",
    "```bash\n",
    "unzip ${BIONEMO_HOME}/examples/tests/test_data/uniref202104_esm2_qc.zip -d ${BIONEMO_HOME}/examples/tests/test_data/\n",
    "```\n",
    "\n",
    "For the purpose of this tutorial, we use a sample subset of the data in NGC registry, included in the BioNeMo Framework codebase at examples/tests/test_data/uniref202104_esm2_qc.zip.\n",
    "Using the unzipped contents of this file, we first generate the pre-processed /train, /val, /test folders with protein sequences separated into batch CSV files:"
=======
   "id": "a2014769-c619-4e1d-8977-9eff1c60ef98",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download Model Checkpoints\n",
    "\n",
    "The following code will download the pretrained model `esm2nv_650M_converted.nemo` from the NGC registry.\n",
    "\n",
    "In BioNeMo FW, there are numerous ESM models available, including ESM-1nv, ESM-2nv 8M with randomly initialized weights, ESM-2nv fine-tuned for secondary structure downstream prediction tasks with LoRA, ESM-2nv 650M, and ESM-2nv 3B. We also have a configuration file for training ESM-2nv 15B available at `examples/protein/esm2nv/conf/pretrain_esm2_15B.yaml` if needed.\n",
    "\n",
    "For demo purposes, we have chosen to showcase the ESM-2nv 650M model. For more details on the [ESM-1nv](https://docs.nvidia.com/bionemo-framework/latest/models/esm1-nv.html) or [ESM-2nv](https://docs.nvidia.com/bionemo-framework/latest/models/esm2-nv.html), consult the corresponding model cards. To find the model names and checkpoint names, please see the `artifacts_paths.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c374a53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NGC CLI API KEY and ORG for the model download\n",
    "# If these variables are not already set in the container, uncomment below\n",
    "# to define and set with your API KEY and ORG\n",
    "#api_key = <your_api_key>\n",
    "#ngc_cli_org = <ngc_cli_org>\n",
    "# Update the environment variable\n",
    "#os.environ['NGC_CLI_API_KEY'] = api_key\n",
    "#os.environ['NGC_CLI_ORG'] = ngc_cli_org\n",
    "\n",
    "# Set variables and paths for model and checkpoint\n",
    "model_name = \"esm2nv\" # change to esm1nv for ESM1\n",
    "model_version = \"esm2nv_650m\" # change to esm1nv for ESM1\n",
    "actual_checkpoint_name = \"esm2nv_650M_converted.nemo\" #  change to esm1nv.nemo for ESM1\n",
    "model_path = os.path.join(bionemo_home, 'models')\n",
    "checkpoint_path = os.path.join(model_path, actual_checkpoint_name)\n",
    "os.environ['MODEL_PATH'] = model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "298eea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    !cd /workspace/bionemo && \\\n",
    "    python download_artifacts.py --model_dir models --models {model_version}\n",
    "else:\n",
    "    print(f\"Model {model_version} already exists at {model_path}.\")"
>>>>>>> bionemo-gitlab/dorotat/rebase-bionemo1-dev
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "metadata": {},
   "source": [
    "```bash\n",
    "cd ${BIONEMO_HOME}\n",
    "python examples/protein/esm2nv/pretrain.py \\\n",
    "    --config-path=conf \\\n",
    "    ++do_training=False \\\n",
    "    ++model.data.val_size=500 \\\n",
    "    ++model.data.test_size=100 \\\n",
    "    ++model.data.train.uf50_datapath=/workspace/bionemo/examples/tests/test_data/uniref202104_esm2_qc_test200_val200/uniref50_train_filt.fasta \\\n",
    "    ++model.data.train.uf90_datapath=/workspace/bionemo/examples/tests/test_data/uniref202104_esm2_qc_test200_val200/ur90_ur50_sampler.fasta \\\n",
    "    ++model.data.train.cluster_mapping_tsv=/workspace/bionemo/examples/tests/test_data/uniref202104_esm2_qc_test200_val200/mapping.tsv \\\n",
    "    ++model.data.dataset_path=/workspace/bionemo/examples/tests/test_data/uniref202104_esm2_qc_test200_val200\n",
    "```"
=======
   "id": "8f82deaa-2400-4359-ac10-561cb7d712c4",
   "metadata": {},
   "source": [
    "### Preprocess Data for ESM-2nv\n",
    "\n",
    "To briefly showcase the model training capabilities of the BioNeMo Framework, we will use the UniRef50 and UniRef90 datasets to provide a diverse yet non-redundant set of protein sequences. By using both, the model can learn from a wide range of sequence variants while avoiding redundancy. This helps in capturing diverse features and patterns that are relevant for protein function and structure prediction, while also preventing overfitting and improving generalization. For demo purposes, a portion of the sample datasets is located in `${bionemo_home}/examples/tests/test_data/uniref202104_esm2_qc`.\n",
    "\n",
    "The data is stored in a zip file, so run the following command to extract the raw FASTA files and a cluster mapping file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64df3382",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "# Define the path to the extracted directory\n",
    "datapath_dir = os.path.join(bionemo_home, 'examples/tests/test_data/uniref202104_esm2_qc_test200_val200')\n",
    "\n",
    "# Define the path to the zip file\n",
    "zip_file = f\"{datapath_dir}.zip\"\n",
    "\n",
    "# Check if the directory already exists\n",
    "if not os.path.exists(datapath_dir): \n",
    "    ! unzip {zip_file} -d {bionemo_home}/examples/tests/test_data/\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13f7494",
   "metadata": {},
   "source": [
    "The `mapping.tsv` file is used to associate protein sequences with their respective clusters. This helps to reduce redundancy, organize data, and evaluate model performance by tracking sequence similarity and ensuring diverse training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13f7494",
   "metadata": {},
   "source": [
    "The `cluster_mapping.tsv` is used to associate protein sequences with their respective clusters, helping to reduce redundancy, organize data, and evaluate model performance by tracking sequence similarity and ensuring diverse training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13f7494",
   "metadata": {},
   "source": [
    "The `mapping.tsv` file is used to associate protein sequences with their respective clusters. This helps to reduce redundancy, organize data, and evaluate model performance by tracking sequence similarity and ensuring diverse training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d69ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uf50_fasta = os.path.join(bionemo_home, f'{datapath_dir}/uniref50_train_filt.fasta')\n",
    "train_uf90_fasta = os.path.join(bionemo_home, f'{datapath_dir}/ur90_ur50_sampler.fasta')\n",
    "train_cluster_mapping_tsv = os.path.join(bionemo_home, f'{datapath_dir}/mapping.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34feba3",
   "metadata": {},
   "source": [
    "Using the unzipped contents of this file, we first create the preprocessed `/train`, `/val`, and `/test` folders, organizing protein sequences into batch CSV files. It is important to utilize both datasets if you plan to use ESM-2 as originally created. However, if you use your own data, as demonstrated in [this notebook](./esm2_paratope_finetuning.ipynb), you can opt to use a single data source.\n",
    "\n",
    "The same approach applies to the clustering mapping file. The `ESM2Preprocess` class can handle clustering indirectly as part of the dataset preparation process. It leverages UniRef50 to UniRef90 clustering mappings to organize protein sequences, ensuring that data is appropriately clustered for training and validation.\n",
    "\n",
    "Please note that this script does not perform clustering itself but relies on pre-defined clustering mappings provided in a TSV file format to organize protein sequences. The expected format is a TSV file where the first column represents the cluster ID (FASTA header in UniRef50) and the second column lists the members separated by commas. The members correspond to entries in the UniRef90 FASTA file.\n",
    "\n",
    "The preprocessing steps are:\n",
    "\n",
    "1. Download the dataset from a specified URL or NGC registry.\n",
    "2. Extract and decompress the downloaded data if necessary.\n",
    "3. Index the FASTA file using `pyfastx` to facilitate data access.\n",
    "4. Split the dataset into training, validation, and test sets.\n",
    "5. Convert the FASTA sequences into CSV format, dividing them into multiple files if needed.\n",
    "6. Generate additional files like memmaps or sorted FASTA files if required for specific use cases.\n",
    "\n",
    "For more details about the preprocessing steps, please consult the `.../bionemo/data/preprocess/protein/preprocess.py` file and the documentation found [here](/bionemo/README.md).\n",
    "\n",
    "To preprocess the data defined in the previous cell, use the `pretrain.py` script and set the `do_training` parameter to `False`, as shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc36928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "! python examples/protein/esm2nv/pretrain.py \\\n",
    "    --config-path=conf \\\n",
    "    --config-name=pretrain_esm2_650M \\\n",
    "    ++do_training=False \\\n",
    "    ++do_preprocessing=True \\\n",
    "    ++model.data.val_size=500 \\\n",
    "    ++model.data.test_size=100 \\\n",
    "    ++model.data.train.uf50_datapath={train_uf50_fasta} \\\n",
    "    ++model.data.train.uf90_datapath={train_uf90_fasta} \\\n",
    "    ++model.data.train.cluster_mapping_tsv={train_cluster_mapping_tsv} \\\n",
    "    ++model.data.dataset_path={datapath_dir}"
>>>>>>> bionemo-gitlab/dorotat/rebase-bionemo1-dev
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-node or Multi-node setup \n",
    "\n",
    "In these test runs, we will use preconfigured parameters provided in the ```pretrain_esm2_650M.yaml``` config file located in the ```${BIONEMO_HOME}/examples/protein/esm2nv/conf``` folder. \n",
    "\n",
    "We will also set other parameters suitable for a quick run, such as ```++trainer.max_steps=1000``` and ```++val_check_interval=100``` . User can update these parameters by editing the ``.yaml`` config file or as additional command line arguments, as shown in the example below. User can select the full dataset and adjust other parameters - for example - as shown in the ``base_config.yaml`` file.\n",
    "\n",
    "As we are connected to the compute node, we navigate to the BioNeMo home folder using the command ``cd ${BIONEMO_HOME}``, and execute the following command in the terminal.\n",
    "\n",
    "The ```bcprun``` command is similar to ```srun``` command in [SLURM](https://slurm.schedmd.com/documentation.html), you can find more details at the NVIDIA [BCP User Guide](https://docs.nvidia.com/base-command-platform/user-guide/index.html). Note, when using ```bcprun```, we need to add backslashes to all environment variables inside ```--cmd``` so they are not pre-maturely expanded. \n",
    "\n",
    "<details>\n",
    "<summary><i><h3>BCP Run commands (click to expand)</h3></i></summary>\n",
    "\n",
    "```bash\n",
    "bcprun --nnodes=1 --npernode=8 \\\n",
    "    --cmd \"cd \\${BIONEMO_HOME}/examples/protein/esm2nv/ && python pretrain.py \\\n",
    "    --config-path=conf \\\n",
    "    --config-name=pretrain_esm2_650M do_training=True ++trainer.max_steps=1000 ++trainer.val_check_interval=100 \\\n",
    "    ++trainer.devices=8 \\\n",
    "    ++model.data.dataset_path=/example_training/data/uf50/ \\\n",
    "    ++model.data.uf90.uniref90_path=/example_training/data/uf90 \\\n",
    "    ++model.data.cluster_mapping_tsv=\\${BIONEMO_HOME}/examples/tests/test_data/uniref202104_esm2_qc/mapping.tsv \\\n",
    "    ++model.validation.validation_enabled=True ++model.micro_batch_size=2 \\\n",
    "    ++trainer.val_check_interval=12 ++exp_manager.create_wandb_logger=False ++model.tensor_model_parallel_size=1 \\\n",
    "    ++trainer.accumulate_grad_batches=1 ++exp_manager.checkpoint_callback_params.always_save_nemo=False ++trainer.precision=16 \\\n",
    "    ++exp_manager.exp_dir=/results/esm2_pretraining ++exp_manager.resume_if_exists=False ++model.dwnstr_task_validation.enabled=False\"\n",
    "```\n",
    "\n",
    "\n",
    "<br><br>\n",
    "To run the model training on multiple nodes, you will have to update parameters accordingly, for example, the command running the model training job on 4 nodes would look like:\n",
    "\n",
    "```bash\n",
    "bcprun --nnodes=4 --npernode=8 \\\n",
    "    --cmd \"cd \\${BIONEMO_HOME}/examples/protein/esm2nv/ python examples/protein/esm2nv/pretrain.py \\\n",
    "    --config-path=conf \\\n",
    "    --config-name=pretrain_esm2_650M do_training=True ++trainer.max_steps=1000 ++trainer.val_check_interval=100 \\\n",
    "    ++trainer.devices=8 \\\n",
    "    ++model.data.dataset_path=/example_training/data/uf50/ \\\n",
    "    ++model.data.uf90.uniref90_path=/example_training/data/uf90 \\\n",
    "    ++model.data.cluster_mapping_tsv=/example_training/data/uniref202104_esm2_qa/mapping.tsv \\\n",
    "    ++model.validation.validation_enabled=False ++model.micro_batch_size=2 \\\n",
    "    ++trainer.val_check_interval=12 ++exp_manager.create_wandb_logger=False ++model.tensor_model_parallel_size=1 \\\n",
    "    ++trainer.accumulate_grad_batches=1 ++exp_manager.checkpoint_callback_params.always_save_nemo=False ++trainer.precision=16 \\\n",
    "    ++exp_manager.exp_dir=/results/esm2_pretraining ++exp_manager.resume_if_exists=False ++model.dwnstr_task_validation.enabled=false\"\n",
    "```\n",
    "</details>\n",
    "\n",
    ":::{note}\n",
    "To run the model training job on a local workstation, user can directly execute the `pretrain.py` script with desired configurations. For example, \n",
    "```bash\n",
    "python examples/protein/esm2nv/pretrain.py \n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging with WandB\n",
    "\n",
    "If you are launching the model training job interactively from the terminal, you can set your Weights and Biases access via ```wandb login <YOUR_WANDB_API_KEY>``` or checkout https://docs.wandb.ai/ref/cli/wandb-login for more information. Alternatively, you may also export the API key as a variable at the time of launching the job via command-line, as shown in ``${BIONEMO_HOME}/examples/protein/esm2nv/scripts/pretrain_esm2_bcp.sh``"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output and Results\n",
    "\n",
    "\n",
    "As the ESM2nv model training job is launched, BioNeMo will print out some of the details related to **compute resources**, **model training configuration**, and **dataset** being used for training. As the job progresses, it will also print out various details related to the test/train/validation steps and accuracy matrices at a set intervals. \n",
    "\n",
    "![esm2nv_1.png](../images/esm2nv_1.png)\n",
    "\n",
    "![esm2nv_2.png](../images/esm2nv_2.png)\n",
    "\n",
    "\n",
    "Upon the completion of training process, it will also print out the details related to log files, model checkpoints, and so on, that will also be saved in the directory as configured (usually ``/result``).\n",
    "\n",
    "![esm2nv_3.png](../images/esm2nv_3.png)\n",
    "\n",
    "\n",
    "Finally, if Weights and Biases logging was enabled (for example, ```++exp_manager.create_wandb_logger=True``` ), you can also visualize the model training progress and resulting matrices, and the summary also gets printed on the termainal at the end of the training job. \n",
    "\n",
    "![esm2nv_4.png](../images/esm2nv_4.png)"
=======
   "id": "8a93374a",
   "metadata": {},
   "source": [
    "##### Command Line and YAML Configuration for `pretrain.py`\n",
    "\n",
    "Parameters starting with `--` are passed as command line arguments to `pretrain.py`. Below are examples of such parameters:\n",
    "\n",
    "- **`--config-path`** and **`--config-name`**:  \n",
    "  These specify the folder and the YAML file name for the configuration. The path is relative to `pretrain.py`. For instance:\n",
    "  \n",
    "  - `config-path`: Refers to the configuration folder, e.g., `examples/protein/esm2nv/conf`.\n",
    "  - `config-name`: Refers to the YAML configuration file, e.g., `pretrain_esm2_650M.yaml`.\n",
    "\n",
    "  The full path for the configuration file in this example would be:  \n",
    "  `examples/protein/esm2nv/conf/pretrain_esm2_650M.yaml`.\n",
    "\n",
    "Parameters starting with `++` are configurable within the YAML file. Below are some examples of such parameters found in the `pretrain_esm2_650M.yaml` file, which inherits from `base_config.yaml`:\n",
    "\n",
    "- **`do_training`**:  \n",
    "  Set to `False` if you only want to preprocess the data without initiating training.\n",
    "\n",
    "- **`model.data.val_size`** and **`model.data.test_size`**:  \n",
    "  These specify the sizes of the validation and test datasets, respectively.\n",
    "\n",
    "- **`model.data.train.uf50_datapath`**:  \n",
    "  Specifies the path to the UniRef50 FASTA file.\n",
    "\n",
    "- **`model.data.train.uf90_datapath`**:  \n",
    "  Specifies the path to the UniRef90 FASTA file.\n",
    "\n",
    "- **`model.data.train.cluster_mapping_tsv`**:  \n",
    "  Specifies the path to the mapping file that maps UniRef50 clusters to UniRef90 sequences.\n",
    "\n",
    "- **`model.data.dataset_path`**:  \n",
    "  Specifies the path to the output directory for the preprocessed UniRef50 and UniRef90 data. After processing, the following directories will be created:\n",
    "\n",
    "  - **`uf50`**:  \n",
    "    Contains `train`/`test`/`val` splits, each with files like `x000.csv`.\n",
    "  \n",
    "  - **`uf90`**:  \n",
    "    Contains a folder named `uf90_csvs`, with files like `x000.csv`. Note that there will be no train/test/val splits in this directory, as UniRef90 is only used during training.\n",
    "\n",
    "Changes can also be made directly to the YAML file instead of overwriting arguments through the command line."
>>>>>>> bionemo-gitlab/dorotat/rebase-bionemo1-dev
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "metadata": {},
   "source": [
    "### Data Pre-processing for the Full Dataset [Optional]\n",
    "\n",
    "In [Preparing Workspace and Data for Pre-training](\"../../preprocessing-bcp-training-esm2nv.md\"), \n",
    "we discussed how to preprocess the NGC-supported pre-training dataset for ESM2.  The \n",
    "end result is the full-preprocessed ESM2 dataset, stored in a directory on an NGC \n",
    "workspace.  We advise using the dataset and scripts referenced in that section.\n",
    "\n",
    "Under the hood, the script `examples/protein/esm2nv/scripts/preprocess_esm2_bcp.sh` \n",
    "discussed in [Preparing Workspace and Data for Pre-training](\"../../preprocessing-bcp-training-esm2nv.md\"),\n",
    "uses the BioNeMo framework class `ESM2Preprocess`.  For example\n",
    "\n",
    "```python\n",
    "from bionemo.data.preprocess.protein.preprocess import ESM2Preprocess\n",
    "data = ESM2Preprocess()\n",
    "data.prepare_dataset()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative datasets [Optional]\n",
    "\n",
    "We recommend using the NGC-supported pre-training dataset, but, alternatively, \n",
    "you can also download datasets that are not available in the BioNeMo Framework. \n",
    "\n",
    "For example, raw pre-training data can be directly downloaded from Universal Protein Resource \n",
    "(UniProt), which is a comprehensive resource for protein sequence and annotation data [[1]](https://www.uniprot.org/help/about). \n",
    "\n",
    "UniRef is a set of Reference Clusters with sequences from the [UniProt Knowledge](https://www.uniprot.org/help/uniprotkb) base and selected [UniParc](https://www.uniprot.org/help/uniparc) records. UniRef50 is a \"second derivation\" of UniRef100: Uniref90 is generated by clustering UniRef100 seed sequences and UniRef50 is generated by clustering UniRef90 sequences. For more information refer to the [UniRef](https://www.uniprot.org/help/uniref) page.\n",
    "\n",
    "Downloading datasets from UniRef, or elsewhere, can be done in two ways:\n",
    "\n",
    "**A) Using bash and wget pointing to the dataset's URL**\n",
    "\n",
    "```bash\n",
    "mkdir -p /tmp/data/protein/esm2nv  \n",
    "wget -P /tmp/data/protein/esm2nv <URL>\n",
    "```\n",
    "\n",
    "**B) Transfering from the local machine to the container**\n",
    "\n",
    "```bash\n",
    "docker cp <dataset directory and filename> container_id:/<container directory and filename>\n",
    "```\n",
    "\n",
    "Then, once the data is downloaded, we can start moving files and using the Data Loaders and [Data Module](../data-module-fw.md) to make sure the dataset is in a format the BioNeMo Framework can operate. It is not guaranteed that the ESM2Preprocess class will handle datasets other than those from UniProt.\n",
    "\n",
    "Note:\n",
    "1. We recommend downloading large datasets directly to the NGC compute host where \n",
    "preprocessing is done. Data transfer between NGC workspace and NGC compute host is fast, \n",
    "but between NGC workspace and non-NGC compute host is slow."
=======
   "id": "e2260d73",
   "metadata": {},
   "source": [
    "### Pretrain from Scratch\n",
    "\n",
    "Now we will perform pretraining of ESM-2 from scratch using our prepared data and the parameters provided in the `pretrain_esm2_650M.yaml` config file located in the `${bionemo_home}/examples/protein/esm2nv/conf` folder.\n",
    "\n",
    "For the purpose of this demo example, we will shorten the time required for training by setting the following parameters: `++trainer.max_steps=1` and `++val_check_interval=1`. Users can update these parameters by editing the `.yaml` config file or by overriding config arguments at runtime using Hydra, as shown in the example below.\n",
    "\n",
    "- `trainer.devices`: Specifies the number of GPUs to use.\n",
    "- `trainer.max_steps`: Sets the maximum number of training steps.\n",
    "- `trainer.val_check_interval`: Determines how often to run validation.\n",
    "- `trainer.limit_train_batches` and `trainer.limit_val_batches`: Limit the number of batches for training and validation respectively.\n",
    "- `micro_batch_size`: Refers to the number of samples processed in a single forward/backward pass before performing a weight update.\n",
    "\n",
    "Lastly, you can change the configuration used to `pretrain_esm2_8M` if you have hardware constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e837ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "! python examples/protein/esm2nv/pretrain.py \\\n",
    "  --config-path=conf \\\n",
    "  --config-name=pretrain_esm2_650M \\\n",
    "  name={model_name}_pretrain \\\n",
    "  ++do_training=True \\\n",
    "  ++model.data.dataset_path={datapath_dir} \\\n",
    "  ++trainer.devices=1 \\\n",
    "  ++model.micro_batch_size=1 \\\n",
    "  ++trainer.max_steps=1 \\\n",
    "  ++trainer.val_check_interval=1 \\\n",
    "  ++exp_manager.create_wandb_logger=False \\\n",
    "  ++trainer.limit_train_batches=1 \\\n",
    "  ++trainer.limit_val_batches=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093d6f4b",
   "metadata": {},
   "source": [
    "As the ESM-2nv model training job is launched, BioNeMo will print out details related to **compute resources**, **model training configuration**, and the **dataset** being used for training. As the job progresses, it will also print various details related to the test/train/validation steps and accuracy metrics at set intervals.\n",
    "\n",
    "Upon completion of the training process, it will print details related to log files, model checkpoints, and so on, which will also be saved in the directory as configured (usually `/result`).\n",
    "\n",
    "Finally, if Weights and Biases logging was enabled (for example, `++exp_manager.create_wandb_logger=True`), you can visualize the model training progress and resulting metrics. The summary will also be printed in the terminal at the end of the training job.\n",
    "\n",
    "### Continue Training from an Existing Checkpoint\n",
    "\n",
    "To continue the pretraining of the foundation model, use the `pretrain.py` script and set `exp_manager.resume_if_exists=True` to load the model weights and previous run's metadata. This configuration also picks up the learning rate from the scheduler at the end of the previous run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b08b0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "! python examples/protein/esm2nv/pretrain.py \\\n",
    "  --config-path=conf \\\n",
    "  --config-name=pretrain_esm2_650M \\\n",
    "  name={model_name}_continued_training \\\n",
    "  ++do_training=True \\\n",
    "  ++model.data.dataset_path={datapath_dir} \\\n",
    "  ++trainer.devices=1 \\\n",
    "  ++model.micro_batch_size=1 \\\n",
    "  ++trainer.max_steps=2 \\\n",
    "  ++trainer.val_check_interval=1 \\\n",
    "  ++exp_manager.create_wandb_logger=False \\\n",
    "  ++exp_manager.resume_if_exists=True \\\n",
    "  ++trainer.limit_train_batches=1 \\\n",
    "  ++trainer.limit_val_batches=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1655e7e8",
   "metadata": {},
   "source": [
    "If Weights and Biases logging was enabled (for example, `++exp_manager.create_wandb_logger=True`), you can also visualize the model training progress and resulting metrics. The summary will also be printed in the terminal at the end of the training job.\n",
    "\n",
    "In other notebooks, you can explore how to [perform inference on your own data](./esm2_oas_inferencing.ipynb), [cluster such embeddings](/bionemo/docs/bionemo/notebooks/protein-esm2nv-clustering.ipynb), [bring and preprocess your own data for training your own ESM model](/bionemo/docs/bionemo/notebooks/esm2_paratope_finetuning.ipynb), and [finetune existing ESM models](/bionemo/docs/bionemo/notebooks/esm2_FLIP_finetuning.ipynb).\n"
>>>>>>> bionemo-gitlab/dorotat/rebase-bionemo1-dev
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
=======
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
>>>>>>> bionemo-gitlab/dorotat/rebase-bionemo1-dev
}
