{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Fine-tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves as a demo for implementing our EncoderFineTuning class from scratch, hooking up the data, setting up the configs, and creating a fine-tuning script."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task head plays a crucial role in fine-tuning for a downstream task. As a part of transfer learning, a pre-trained model is often utilized to learn generic features from a large-scale dataset. However, these features might not be directly applicable to the specific task at hand. By incorporating an MLP task head, which consists of one or more fully connected layers, the model can adapt and specialize to the target task. The MLP task head serves as a flexible and adaptable component that learns task-specific representations by leveraging the pre-trained features as a foundation. Through fine-tuning, the MLP task head enables the model to learn and extract task-specific patterns, improving performance and addressing the nuances of the downstream task. It acts as a critical bridge between the pre-trained model and the specific task, enabling efficient and effective transfer of knowledge."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assist in creating your own task/prediction head, we have created the EncoderFinetuning abstract base class to help you to quickly implement a feed forward network for training on a downstream task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Assumptions\n",
    "\n",
    "This tutorial assumes that a copy of the BioNeMo framework repo exists on workstation or server and has been mounted inside the container at `/workspace/bionemo`. This path will be referred to with the variable `BIONEMO_WORKSPACE` in the tutorial. \n",
    "\n",
    "All commands should be executed inside the BioNeMo docker container.\n",
    "\n",
    "A user may create/place the following codes and execute files from ``$BIONEMO_WORKSPACE/examples/<molecule_or_protein>/<model_name>/`` folder, which needs to be adjusted according to the use case. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will go over implementing our own downstream task model based on EncoderFinetuning, using a MLP regressor prediction head. \n",
    "\n",
    "To successfully accomplish this we need to define some key classes/files:\n",
    "\n",
    "* Custom dataset class - defines functions to process our dataset and prepare batches\n",
    "* BioNeMo data module class - performs additional the data-driven functions such as creation of train/val/test datasets\n",
    "* Downstream Task Model class - extends the BioNeMo EncoderFinetuning class, which provides help abstract methods that help you define your prediction head architecture, loss function, pretrained model encoder that you want to fine-tune. \n",
    "* Config yaml file - to specify model parameters and control behavior of model at runtime\n",
    "* Training script - launches model training of our downtream task model\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset class by extending from ```torch.utils.data.Dataset``` or from BioNeMo's dataset classes found in ```bionemo.data.datasets```. \n",
    "\n",
    "For the purposes of this demo, we'll assume we are using the FreeSolv dataset from MoleculeNet to train our prediction, and our downstream task will be to predict the hyration free energy of small molecules in water. Therefore, the custom BioNeMo dataset class will be appropriate (found in ```bionemo.data.datasets.single_value_dataset.SingleValueDataset```) as it faciliates predicting on a single value. \n",
    "\n",
    "An excerpt from the class is shown below:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SingleValueDataset(Dataset):\n",
    "    def __init__(self, datafiles, max_seq_length, emb_batch_size=None, \n",
    "                 model=None, input_column: str='SMILES', \n",
    "                 target_column: str='y', task: str=\"regression\")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```SingleValueDataset``` accepts the path to the data, the column name of the input, the column name of the target values and other parameters. Simply extend ```SingleValueDataset``` class in a similar way to customize your class for your data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To coordinate the creation of training, validation and testing datasets from your data, we need to use a BioNeMo data module class. To do this we simply extend the ```BioNeMoDataModule``` class (located at ```bionemo.core.BioNeMoDataModule```) which defines helpful abstract methods that use your dataset class. At minimum, we need to define our ```__init__(), train_dataset(), val_dataset(), test_dataset()``` when extending ```BioNeMoDataModule```.\n",
    "\n",
    "We have already done this and created the ```SingleValueDataModule``` (located at ```bionemo.data.datasets.single_value_dataset.SingleValueDataModule```) for use with the ```SingleValueDataset```. Make note of the use of our ```SingleValueDataset``` class inside the ```_create_dataset()``` function "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class SingleValueDataModule(BioNeMoDataModule):\n",
    "    def __init__(self, cfg, trainer, model):\n",
    "        super().__init__(cfg, trainer)\n",
    "        self.model = model\n",
    "        self.parent_cfg = cfg\n",
    "        if self.cfg.task_type not in [\"regression\", \"classification\"]:\n",
    "            raise ValueError(\"Invalid task_type was provided {}. \" + \\\n",
    "                             \"Supported task_type: 'classification' and 'regression'\".format(self.cfg.task))\n",
    "        if self.cfg.task_type == \"classification\":\n",
    "            self.tokenizer = Label2IDTokenizer()\n",
    "        else:\n",
    "            self.tokenizer = None\n",
    "\n",
    "    def _update_tokenizer(self, tokenizer, labels):\n",
    "        tokenizer = tokenizer.build_vocab(labels)\n",
    "        return tokenizer\n",
    "\n",
    "    # helper function for creating Datasets\n",
    "    def _create_dataset(self, split, files):\n",
    "        datafiles = os.path.join(self.cfg.dataset_path, \n",
    "                                 split, \n",
    "                                 files)\n",
    "        datafiles = expand_dataset_paths(datafiles, \".csv\")\n",
    "        dataset = SingleValueDataset(\n",
    "            datafiles=datafiles, \n",
    "            max_seq_length=self.parent_cfg.seq_length,\n",
    "            emb_batch_size=self.cfg.emb_batch_size,\n",
    "            model=self.model, \n",
    "            input_column=self.cfg.sequence_column, \n",
    "            target_column=self.cfg.target_column\n",
    "            )\n",
    "        if self.tokenizer is not None:\n",
    "            self.tokenizer = self._update_tokenizer(\n",
    "                self.tokenizer, \n",
    "                dataset.labels.reshape(-1, 1)\n",
    "                )\n",
    "            dataset.labels = get_data._tokenize_labels([self.tokenizer], dataset.labels.reshape(1, 1, -1), [self.cfg.num_classes])[0][0] \n",
    "        return dataset\n",
    "\n",
    "    # uses our _create_dataset function to instantiate a training dataset\n",
    "    def train_dataset(self):\n",
    "        \"\"\"Creates a training dataset\n",
    "        Returns:\n",
    "            Dataset: dataset to use for training\n",
    "        \"\"\"\n",
    "        self.train_ds = self._create_dataset(\"train\", \n",
    "                                             self.cfg.dataset.train)\n",
    "        return self.train_ds\n",
    "\n",
    "    def val_dataset(self):\n",
    "        \"\"\"Creates a validation dataset\n",
    "        Returns:\n",
    "            Dataset: dataset to use for validation\n",
    "        \"\"\"\n",
    "        if \"val\" in self.cfg.dataset:\n",
    "            self.val_ds = self._create_dataset(\"val\", \n",
    "                                             self.cfg.dataset.val)\n",
    "            return self.val_ds\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def test_dataset(self):\n",
    "        \"\"\"Creates a testing dataset\n",
    "        Returns:\n",
    "            Dataset: dataset to use for testing\n",
    "        \"\"\"\n",
    "        if \"test\" in self.cfg.dataset:\n",
    "            self.test_ds = self._create_dataset(\"test\", \n",
    "                                                self.cfg.dataset.test)\n",
    "            return self.test_ds\n",
    "        else:\n",
    "            pass\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Downstream Task Model Class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dataset classes are created, we are ready to create the model class that will define the model architecture necessary to train on our downstream task. BioNeMo provides the EncoderFinetuning which allows us to quickly create a model for adding a prediction head to a pretrained model by quickly and easily extending the class and overriding abstract methods within the class.\n",
    "\n",
    "Let's create a class, DownstreamTaskModel, based on EncoderFinetuning where we will setup our task head and the encoder model. We will use our MLPModel class as well, however, you can implement your own model to use with your class. \n",
    "\n",
    "It is important to note that we are required to implement the abstract methods withins the EncoderFinetuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bionemo.utils\n",
    "from functools import lru_cache\n",
    "from nemo.utils.model_utils import import_class_by_path\n",
    "from bionemo.model.core import MLPModel\n",
    "from bionemo.model.core.encoder_finetuning import EncoderFineTuning\n",
    "\n",
    "#import a BioNeMo data module or your custom data module\n",
    "from bionemo.data.datasets.single_value_dataset import SingleValueDataModule\n",
    "\n",
    "class DownstreamTaskModel(EncoderFineTuning):\n",
    "\n",
    "    def __init__(self, cfg, trainer):\n",
    "        super().__init__(cfg.model, trainer=trainer) \n",
    "\n",
    "        #store config parameters within object so they can be access easily\n",
    "        self.full_cfg = cfg\n",
    "\n",
    "        # we want our downstream model to behave differently based on whether the\n",
    "        # encoder_frozen config parameter is set to True or False so we store it for \n",
    "        # convenient access within the object\n",
    "        self.encoder_frozen = self.full_cfg.model.encoder_frozen\n",
    "        self.batch_target_name = self.cfg.data.target_column\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        super().setup_optimization(optim_config=self.cfg.finetuning_optim)\n",
    "\n",
    "        if self._scheduler is None:\n",
    "            return self._optimizer\n",
    "        else:\n",
    "            return [self._optimizer], [self._scheduler]\n",
    "\n",
    "    # use this function to define what the loss func of the task head should be\n",
    "    def build_loss_fn(self):\n",
    "        return bionemo.utils.lookup_or_use(torch.nn, self.cfg.downstream_task.loss_func)\n",
    "\n",
    "    # define the architecture of our prediction task head for the downstream task\n",
    "    def build_task_head(self):\n",
    "\n",
    "        # we create an instance of MLPModel using parameters defined in the config file\n",
    "        # choose the right task head architecture based on your downstream task (for example,. regression vs classification)\n",
    "        regressor = MLPModel(layer_sizes=[self.encoder_model.cfg.model.hidden_size, self.cfg.downstream_task.hidden_layer_size, self.cfg.downstream_task.n_outputs],\n",
    "            dropout=0.1,\n",
    "        )\n",
    "\n",
    "        # we can use pytorch libraries to further define our architecture and tensor operations\n",
    "        task_head = nn.Sequential(regressor, nn.Flatten(start_dim=0))\n",
    "        return task_head\n",
    "\n",
    "    # returns the model from which we will use the pretrained encoder\n",
    "    def setup_encoder_model(self, cfg, trainer):\n",
    "        infer_class = import_class_by_path(self.full_cfg.infer_target)\n",
    "        pretrained_model = infer_class(\n",
    "            self.full_cfg, \n",
    "            freeze=self.encoder_frozen, #determines whether encoders weights are trainable\n",
    "            restore_path=self.full_cfg.restore_from_path,\n",
    "            training=not self.cfg.encoder_frozen)\n",
    "        return pretrained_model\n",
    "\n",
    "    # use this function to define all your data operations\n",
    "    # in this example, we use the config parameter to determine the value of our model variable\n",
    "    # then we pass it into an instance of SingleValueDataModule()\n",
    "    @lru_cache\n",
    "    def data_setup(self):\n",
    "        if self.encoder_frozen:\n",
    "            model = self.encoder_model\n",
    "        else:\n",
    "            model = None\n",
    "        self.data_module = SingleValueDataModule(\n",
    "            self.cfg, self.trainer, model=model\n",
    "        )\n",
    "\n",
    "    # ensures that we create our necessary datasets \n",
    "    def on_fit_start(self):\n",
    "        self.build_train_valid_test_datasets()\n",
    "        return super().on_fit_start()\n",
    "\n",
    "    # function that simply instatiates our datasets and stores them within our object \n",
    "    def build_train_valid_test_datasets(self):\n",
    "        self._train_ds = self.data_module.get_sampled_train_dataset()\n",
    "        self._validation_ds = self.data_module.get_sampled_val_dataset()\n",
    "        self._test_ds = self.data_module.get_sampled_test_dataset()\n",
    "\n",
    "    # define the behavior for retrieving embeddings from encoder\n",
    "    def encoder_forward(self, bart_model, batch: dict):\n",
    "        if self.encoder_frozen:\n",
    "            enc_output = batch[\"embeddings\"]\n",
    "        else:\n",
    "            enc_output = bart_model.seq_to_embeddings(batch[\"embeddings\"])\n",
    "        return enc_output\n",
    "\n",
    "    # define additional operations on the encoder output\n",
    "    # in this example we simply convert the values of the tensor to float\n",
    "    # see forward() in encoder_finetuning.py for additional information\n",
    "    def extract_for_task_head(self, input_tensor):\n",
    "        return input_tensor.float()\n",
    "    \n",
    "    def get_target_from_batch(self, batch):\n",
    "        ret = batch['target']\n",
    "\n",
    "        return ret.float()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config YAML"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our ```DownstreamTaskModel``` defined, let's create a config yaml file (```downstream_task_example.yaml```) that will define specific values of tunable hyperparameters, file paths and other important parameters needed by our model. \n",
    "\n",
    "An example config file can be found in ```examples/molecule/megamolbart/conf/finetune_config.yaml```.\n",
    "\n",
    "Most importantly, our config file:\n",
    "\n",
    "* provides the path to our pretrained model using the '```restore_from_path```' parameter\n",
    "* the model parameters, including the ```loss_func, hidden_layer_size, n_outputs``` to be used by our prediction head\n",
    "* important data related parameters such as ```task_type, dataset_path, sequence_column, target_column```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "name: downstream_task_example\n",
    "defaults: \n",
    "  - pretrain_small_span_aug\n",
    "do_training: True # set to false if data preprocessing steps must be completed\n",
    "do_testing: True # set to true to run evaluation on test data after training, requires test_dataset section\n",
    "restore_from_path: /model/molecule/megamolbart/megamolbart.nemo\n",
    "target: bionemo.model.molecule.megamolbart.MegaMolBARTModel\n",
    "infer_target: bionemo.model.molecule.megamolbart.infer.MegaMolBARTInference\n",
    "\n",
    "trainer:\n",
    "  devices: 1 # number of GPUs or CPUs\n",
    "  num_nodes: 1\n",
    "  max_epochs: 100 # use max_steps instead with NeMo Megatron models\n",
    "  max_steps: 10000 # consumed_samples = global_step * micro_batch_size * data_parallel_size * accumulate_grad_batches\n",
    "  val_check_interval: 8 # set to integer when using steps to determine frequency of validation, use fraction with epochs\n",
    "  limit_val_batches: 20 # number of batches in validation step, use fraction for fraction of data, 0 to disable\n",
    "  limit_test_batches: 100 # number of batches in test step, use fraction for fraction of data, 0 to disable\n",
    "\n",
    "exp_manager:\n",
    "  wandb_logger_kwargs:\n",
    "    project: ${name}_finetuning\n",
    "    name: ${name}_finetuning_encoder_frozen_${model.encoder_frozen}\n",
    "  checkpoint_callback_params:\n",
    "    monitor: val_loss # use molecular accuracy to select best checkpoints\n",
    "    mode: min # use min or max of monitored metric to select best checkpoints\n",
    "    filename: '${name}-${model.name}--{val_loss:.2f}-{step}-{consumed_samples}'\n",
    "  resume_if_exists: True\n",
    "\n",
    "model:\n",
    "  encoder_frozen: True\n",
    "  post_process: False\n",
    "  micro_batch_size: 32 # NOTE: adjust to occupy ~ 90% of GPU memory\n",
    "  global_batch_size: null\n",
    "  tensor_model_parallel_size: 1  # model parallelism\n",
    "  \n",
    "  downstream_task:\n",
    "    n_outputs: 1\n",
    "    hidden_layer_size: 128\n",
    "    loss_func: MSELoss\n",
    "\n",
    "  data:\n",
    "    # Finetuning data params\n",
    "    task_type: 'regression'\n",
    "    dataset_path: /data/physchem/SAMPL\n",
    "    sequence_column: 'smiles'\n",
    "    target_column: 'expt'\n",
    "    emb_batch_size: ${model.micro_batch_size}\n",
    "    dataset:\n",
    "      train: x000\n",
    "      val: x000\n",
    "      test: x000\n",
    "    num_workers: 8\n",
    "  \n",
    "  finetuning_optim:\n",
    "    name: adam\n",
    "    lr: 0.001\n",
    "    betas:\n",
    "      - 0.9\n",
    "      - 0.999\n",
    "    eps: 1e-8\n",
    "    weight_decay: 0.01\n",
    "    sched:\n",
    "      name: WarmupAnnealing\n",
    "      min_lr: 0.00001\n",
    "      last_epoch: -1\n",
    "      warmup_steps: 100\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Script"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we'll need a training script to launch our model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.core.config import hydra_runner\n",
    "from nemo.utils import logging\n",
    "from omegaconf.omegaconf import OmegaConf\n",
    "from bionemo.model.utils import (setup_trainer,)\n",
    "\n",
    "import DownstreamTaskModel #import our model class\n",
    "\n",
    "@hydra_runner(config_path=\"conf\", config_name=\"downstream_task_example\") \n",
    "def main(cfg) -> None:\n",
    "\n",
    "    logging.info(\"\\n\\n************* Finetune config ****************\")\n",
    "    logging.info(f'\\n{OmegaConf.to_yaml(cfg)}')\n",
    "\n",
    "    trainer = setup_trainer(\n",
    "         cfg, builder=None)\n",
    "\n",
    "    # we instantiate our model \n",
    "    model = DownstreamTaskModel(cfg, trainer)\n",
    "\n",
    "    if cfg.do_training:\n",
    "        logging.info(\"************** Starting Training ***********\")\n",
    "        trainer.fit(model) # train our downstream task model using the dataset defined in config\n",
    "        logging.info(\"************** Finished Training ***********\")\n",
    "\n",
    "    if cfg.do_testing:\n",
    "        if \"test\" in cfg.model.data.dataset:\n",
    "            trainer.test(model)\n",
    "        else:\n",
    "            raise UserWarning(\"Skipping testing, test dataset file was not provided. Specify 'test_ds.data_file' in yaml config\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can launch our training by simply calling:\n",
    "\n",
    "```\n",
    "python training_script.py\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More examples of training models for downstream tasks in BioNeMo can be found in our physicochemical property prediction notebook [here](./physchem-notebook-fw.ipynb)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "text"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
