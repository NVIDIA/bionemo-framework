{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de21eb76",
   "metadata": {},
   "source": [
    "# Adding the OAS Dataset: Customizing Dataset Object and Dataloader Functions\n",
    "This tutorial is the second part of a series focused on adding a new dataset to BioNeMo using the [Observed Antibody Space (OAS)](https://opig.stats.ox.ac.uk/webapps/oas/) database. There are three steps to this task:\n",
    "\n",
    "\n",
    "1. Preprocessing includes download of the raw data and any additional preparation steps, such as extracting the files. It also includes dividing the data into train, validation, and test splits. The preprocessing step can make use of two BioNeMo base classes, RemoteResource and ResourcePreprocessor, from bionemo.utils.remote and bionemo.data.preprocess, respectively. Their use is optional but they provide some basic functionality which can accelerate development. This step is covered by this tutorial. This objective was accomplished by the first tutorial, Downloading and Preprocessing.\n",
    "\n",
    "2. Development of the new dataset class. Here, the NeMo dataset class CSVMemMapDataset will be used. This step was covered in the last tutorial, Modifying the Dataset Class.\n",
    "\n",
    "3. Modification of the dataloader classes. This tutorial will cover customizing DataLoader objects using the newly created OAS datasets. This will include specifics on instantiating actual Dataset classes, customizing the collate function, and instantiating a dataloader. We will also review how these steps are executed within the BioNeMo model classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb2b8ab",
   "metadata": {},
   "source": [
    "## Setup and Assumptions\n",
    "This tutorial assumes that a copy of the BioNeMo framework repo exists on workstation or server and has been mounted inside the container at /workspace/bionemo as described in the Code Development section of the Quickstart Guide. This path will be referred to with the variable BIONEMO_WORKSPACE in the tutorial.\n",
    "\n",
    "All commands should be executed inside the BioNeMo docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5436ced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIONEMO_WORKSPACE = '/workspace/bionemo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36e63a30",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "### Utility functions \n",
    "\n",
    "from IPython.display import Code\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "BIONEMO_WORKSPACE = '/workspace/bionemo'\n",
    "def stage_files(tag: str,\n",
    "                source_directory: str = f'{BIONEMO_WORKSPACE}/examples/oas_dataset'):\n",
    "    \"\"\"Stage files for each step of the tutorial\"\"\"\n",
    "    source_path = os.path.join(source_directory, tag)\n",
    "    \n",
    "    data_path = os.path.join(BIONEMO_WORKSPACE, 'bionemo/data/preprocess/protein')\n",
    "    shutil.copyfile(os.path.join(source_path, 'oas_paired_subset_download.sh'), \n",
    "                    os.path.join(data_path, 'oas_paired_subset_download.sh'))\n",
    "    \n",
    "    preprocess_path = os.path.join(BIONEMO_WORKSPACE, 'bionemo/data/preprocess/protein')\n",
    "    shutil.copyfile(os.path.join(source_path, 'oas_preprocess.py'), \n",
    "                    os.path.join(preprocess_path, 'oas_preprocess.py'))\n",
    "    \n",
    "    config_path = os.path.join(BIONEMO_WORKSPACE, 'examples/protein/esm1nv/conf')\n",
    "    shutil.copyfile(os.path.join(source_path, 'pretrain_oas.yaml'), \n",
    "                    os.path.join(config_path, 'pretrain_oas.yaml'))\n",
    "    \n",
    "    pretrain_path = os.path.join(BIONEMO_WORKSPACE, 'examples/protein/esm1nv')\n",
    "    shutil.copyfile(os.path.join(source_path, 'pretrain_oas.py'), \n",
    "                    os.path.join(pretrain_path, 'pretrain_oas.py'))\n",
    "\n",
    "    collate_path = os.path.join(BIONEMO_WORKSPACE, 'bionemo/data/dataloader')\n",
    "    shutil.copyfile(os.path.join(source_path, 'custom_protein_collate.py'), \n",
    "                    os.path.join(collate_path, 'custom_protein_collate.py'))\n",
    "\n",
    "    model_path = os.path.join(BIONEMO_WORKSPACE, 'bionemo/model/protein/esm1nv')\n",
    "    shutil.copyfile(os.path.join(source_path, 'custom_esm1nv_model.py'), \n",
    "                    os.path.join(model_path, 'custom_esm1nv_model.py'))\n",
    "\n",
    "    \n",
    "def show_code(filename: str,\n",
    "              language: str,\n",
    "              start_line = None,\n",
    "              end_line = None,\n",
    "              end_column = None):\n",
    "    \"\"\"Display syntax highlighted section of code\"\"\"\n",
    "    \n",
    "    with open(filename, 'r') as fh:\n",
    "        code = fh.readlines()\n",
    "\n",
    "    if end_line:\n",
    "        code = code[:end_line]\n",
    "        code.append('...\\n')\n",
    "    if start_line:\n",
    "        code = code[start_line:]\n",
    "        code.insert(0, '...\\n')\n",
    "    if end_column:\n",
    "        for line in code:\n",
    "            line = line[:end_column] + '...\\n'\n",
    "        \n",
    "    code = ''.join(code)\n",
    "    return Code(data=code, language=language)\n",
    "\n",
    "\n",
    "def filter_log(logfile_list, regex):\n",
    "    \"\"\"Filter a list of log output until a regex match is found\"\"\"\n",
    "\n",
    "    reg = re.compile(regex)\n",
    "    string_matches = filter(reg.search, logfile_list)\n",
    "    position_matches = list(map(lambda x: logfile_list.index(x), string_matches))\n",
    "    logfile_list = logfile_list[position_matches[0]:]\n",
    "    return '\\n'.join(logfile_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "439d012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TUTORIAL_FILE_VERSION = 'step_999_final'\n",
    "stage_files(TUTORIAL_FILE_VERSION, source_directory=f'{BIONEMO_WORKSPACE}/examples/oas_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca1e8c1",
   "metadata": {},
   "source": [
    "## Customizing a collate function\n",
    "\n",
    "In the last tutorial we saw how you can modify your yaml file to use a different set of data with existing tooling, in some cases, this isn't enough. The `collate_fn` parameter of pytorch DataLoaders if used for last minute adjustments to batches, including masking, shuffling, batching, padding, and other slight modifications to the input data. In BioNeMo, we build our collate function ontop of collators used for language modeling (`bionemo/data/dataloader/collate.py`). \n",
    "\n",
    "The collate function is ultimately injected into the dataloader upon construction. To customize further, we can simply extend the existing `ProteinCollate` class with our own additional collation, followed by a call to the parents method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41ac8ff2",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"c1\"># Copyright (c) 2022, NVIDIA CORPORATION.</span>\n",
       "<span class=\"c1\"># SPDX-License-Identifier: Apache-2.0</span>\n",
       "\n",
       "<span class=\"c1\"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>\n",
       "<span class=\"c1\"># you may not use this file except in compliance with the License.</span>\n",
       "<span class=\"c1\"># You may obtain a copy of the License at</span>\n",
       "<span class=\"c1\">#</span>\n",
       "<span class=\"c1\">#     http://www.apache.org/licenses/LICENSE-2.0</span>\n",
       "<span class=\"c1\">#</span>\n",
       "<span class=\"c1\"># Unless required by applicable law or agreed to in writing, software</span>\n",
       "<span class=\"c1\"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>\n",
       "<span class=\"c1\"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>\n",
       "<span class=\"c1\"># See the License for the specific language governing permissions and</span>\n",
       "<span class=\"c1\"># limitations under the License.</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n",
       "\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">typing</span> <span class=\"kn\">import</span> <span class=\"n\">List</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">nemo.collections.common.tokenizers</span> <span class=\"kn\">import</span> <span class=\"n\">TokenizerSpec</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">bionemo.data.dataloader.collate</span> <span class=\"kn\">import</span> <span class=\"p\">(</span>\n",
       "    <span class=\"n\">BertCollate</span><span class=\"p\">,</span>\n",
       "    <span class=\"n\">BertMasking</span><span class=\"p\">,</span>\n",
       "    <span class=\"n\">SentencePieceTokenizerAdapter</span><span class=\"p\">,</span>\n",
       "<span class=\"p\">)</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">bionemo.data.dataloader.protein_collate</span> <span class=\"kn\">import</span> <span class=\"n\">ProteinBertCollate</span>\n",
       "\n",
       "\n",
       "<span class=\"n\">__all__</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">&#39;CustomProteinBertCollate&#39;</span><span class=\"p\">]</span>\n",
       "\n",
       "\n",
       "<span class=\"k\">class</span> <span class=\"nc\">CustomProteinBertCollate</span><span class=\"p\">(</span><span class=\"n\">ProteinBertCollate</span><span class=\"p\">):</span>\n",
       "   <span class=\"k\">def</span> <span class=\"nf\">collate_fn</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">batch</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">],</span> <span class=\"n\">label_pad</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">):</span>\n",
       "<span class=\"w\">        </span><span class=\"sd\">&#39;&#39;&#39;</span>\n",
       "<span class=\"sd\">        Parent does things like add padding, mask, onehot transformations, etc.</span>\n",
       "<span class=\"sd\">        in this case, we do the same thing but we sort our batch on values (strings)</span>\n",
       "<span class=\"sd\">        does this do anything useful in pratice? maybe not, but thats okay.</span>\n",
       "\n",
       "<span class=\"sd\">        This method ultimately will get injected into a DataLoader. We do this as a </span>\n",
       "<span class=\"sd\">        part of the standard dataloader setup method inside our ESM1nv model, by instantiating</span>\n",
       "<span class=\"sd\">        this class and then injecting the collate_fn.</span>\n",
       "<span class=\"sd\">        &#39;&#39;&#39;</span>\n",
       "        <span class=\"n\">extra</span> <span class=\"o\">=</span> <span class=\"p\">[]</span> <span class=\"c1\"># Handles odd cases</span>\n",
       "        <span class=\"k\">if</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">batch</span><span class=\"p\">)</span> <span class=\"o\">%</span> <span class=\"mi\">2</span> <span class=\"o\">==</span> <span class=\"mi\">1</span><span class=\"p\">:</span>\n",
       "            <span class=\"n\">batch</span><span class=\"p\">,</span> <span class=\"n\">extra</span> <span class=\"o\">=</span> <span class=\"n\">batch</span><span class=\"p\">[:</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n",
       "\n",
       "        <span class=\"n\">back</span><span class=\"p\">,</span> <span class=\"n\">front</span> <span class=\"o\">=</span> <span class=\"n\">batch</span><span class=\"p\">[:</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">batch</span><span class=\"p\">)],</span> <span class=\"n\">batch</span><span class=\"p\">[</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">batch</span><span class=\"p\">):]</span>\n",
       "        <span class=\"n\">batch</span> <span class=\"o\">=</span> <span class=\"n\">back</span> <span class=\"o\">+</span> <span class=\"n\">front</span> <span class=\"o\">+</span> <span class=\"n\">extra</span>\n",
       "\n",
       "        <span class=\"n\">new_batch</span> <span class=\"o\">=</span> <span class=\"p\">[</span> <span class=\"s1\">&#39;A&#39;</span> <span class=\"o\">*</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">seq</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">seq</span> <span class=\"ow\">in</span> <span class=\"n\">batch</span> <span class=\"p\">]</span>\n",
       "\n",
       "        <span class=\"k\">return</span> <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">collate_fn</span><span class=\"p\">(</span><span class=\"n\">new_batch</span><span class=\"p\">,</span> <span class=\"n\">label_pad</span><span class=\"p\">)</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{c+c1}{\\PYZsh{} Copyright (c) 2022, NVIDIA CORPORATION.}\n",
       "\\PY{c+c1}{\\PYZsh{} SPDX\\PYZhy{}License\\PYZhy{}Identifier: Apache\\PYZhy{}2.0}\n",
       "\n",
       "\\PY{c+c1}{\\PYZsh{} Licensed under the Apache License, Version 2.0 (the \\PYZdq{}License\\PYZdq{});}\n",
       "\\PY{c+c1}{\\PYZsh{} you may not use this file except in compliance with the License.}\n",
       "\\PY{c+c1}{\\PYZsh{} You may obtain a copy of the License at}\n",
       "\\PY{c+c1}{\\PYZsh{}}\n",
       "\\PY{c+c1}{\\PYZsh{}     http://www.apache.org/licenses/LICENSE\\PYZhy{}2.0}\n",
       "\\PY{c+c1}{\\PYZsh{}}\n",
       "\\PY{c+c1}{\\PYZsh{} Unless required by applicable law or agreed to in writing, software}\n",
       "\\PY{c+c1}{\\PYZsh{} distributed under the License is distributed on an \\PYZdq{}AS IS\\PYZdq{} BASIS,}\n",
       "\\PY{c+c1}{\\PYZsh{} WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.}\n",
       "\\PY{c+c1}{\\PYZsh{} See the License for the specific language governing permissions and}\n",
       "\\PY{c+c1}{\\PYZsh{} limitations under the License.}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\n",
       "\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{typing} \\PY{k+kn}{import} \\PY{n}{List}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{nemo}\\PY{n+nn}{.}\\PY{n+nn}{collections}\\PY{n+nn}{.}\\PY{n+nn}{common}\\PY{n+nn}{.}\\PY{n+nn}{tokenizers} \\PY{k+kn}{import} \\PY{n}{TokenizerSpec}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{bionemo}\\PY{n+nn}{.}\\PY{n+nn}{data}\\PY{n+nn}{.}\\PY{n+nn}{dataloader}\\PY{n+nn}{.}\\PY{n+nn}{collate} \\PY{k+kn}{import} \\PY{p}{(}\n",
       "    \\PY{n}{BertCollate}\\PY{p}{,}\n",
       "    \\PY{n}{BertMasking}\\PY{p}{,}\n",
       "    \\PY{n}{SentencePieceTokenizerAdapter}\\PY{p}{,}\n",
       "\\PY{p}{)}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{bionemo}\\PY{n+nn}{.}\\PY{n+nn}{data}\\PY{n+nn}{.}\\PY{n+nn}{dataloader}\\PY{n+nn}{.}\\PY{n+nn}{protein\\PYZus{}collate} \\PY{k+kn}{import} \\PY{n}{ProteinBertCollate}\n",
       "\n",
       "\n",
       "\\PY{n}{\\PYZus{}\\PYZus{}all\\PYZus{}\\PYZus{}} \\PY{o}{=} \\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{CustomProteinBertCollate}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\n",
       "\n",
       "\n",
       "\\PY{k}{class} \\PY{n+nc}{CustomProteinBertCollate}\\PY{p}{(}\\PY{n}{ProteinBertCollate}\\PY{p}{)}\\PY{p}{:}\n",
       "   \\PY{k}{def} \\PY{n+nf}{collate\\PYZus{}fn}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{batch}\\PY{p}{:} \\PY{n}{List}\\PY{p}{[}\\PY{n+nb}{str}\\PY{p}{]}\\PY{p}{,} \\PY{n}{label\\PYZus{}pad}\\PY{p}{:} \\PY{n+nb}{int} \\PY{o}{=} \\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{)}\\PY{p}{:}\n",
       "\\PY{+w}{        }\\PY{l+s+sd}{\\PYZsq{}\\PYZsq{}\\PYZsq{}}\n",
       "\\PY{l+s+sd}{        Parent does things like add padding, mask, onehot transformations, etc.}\n",
       "\\PY{l+s+sd}{        in this case, we do the same thing but we sort our batch on values (strings)}\n",
       "\\PY{l+s+sd}{        does this do anything useful in pratice? maybe not, but thats okay.}\n",
       "\n",
       "\\PY{l+s+sd}{        This method ultimately will get injected into a DataLoader. We do this as a }\n",
       "\\PY{l+s+sd}{        part of the standard dataloader setup method inside our ESM1nv model, by instantiating}\n",
       "\\PY{l+s+sd}{        this class and then injecting the collate\\PYZus{}fn.}\n",
       "\\PY{l+s+sd}{        \\PYZsq{}\\PYZsq{}\\PYZsq{}}\n",
       "        \\PY{n}{extra} \\PY{o}{=} \\PY{p}{[}\\PY{p}{]} \\PY{c+c1}{\\PYZsh{} Handles odd cases}\n",
       "        \\PY{k}{if} \\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{batch}\\PY{p}{)} \\PY{o}{\\PYZpc{}} \\PY{l+m+mi}{2} \\PY{o}{==} \\PY{l+m+mi}{1}\\PY{p}{:}\n",
       "            \\PY{n}{batch}\\PY{p}{,} \\PY{n}{extra} \\PY{o}{=} \\PY{n}{batch}\\PY{p}{[}\\PY{p}{:}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{]}\n",
       "\n",
       "        \\PY{n}{back}\\PY{p}{,} \\PY{n}{front} \\PY{o}{=} \\PY{n}{batch}\\PY{p}{[}\\PY{p}{:}\\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{batch}\\PY{p}{)}\\PY{p}{]}\\PY{p}{,} \\PY{n}{batch}\\PY{p}{[}\\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{batch}\\PY{p}{)}\\PY{p}{:}\\PY{p}{]}\n",
       "        \\PY{n}{batch} \\PY{o}{=} \\PY{n}{back} \\PY{o}{+} \\PY{n}{front} \\PY{o}{+} \\PY{n}{extra}\n",
       "\n",
       "        \\PY{n}{new\\PYZus{}batch} \\PY{o}{=} \\PY{p}{[} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{A}\\PY{l+s+s1}{\\PYZsq{}} \\PY{o}{*} \\PY{n+nb}{len}\\PY{p}{(}\\PY{n}{seq}\\PY{p}{)} \\PY{k}{for} \\PY{n}{seq} \\PY{o+ow}{in} \\PY{n}{batch} \\PY{p}{]}\n",
       "\n",
       "        \\PY{k}{return} \\PY{n+nb}{super}\\PY{p}{(}\\PY{p}{)}\\PY{o}{.}\\PY{n}{collate\\PYZus{}fn}\\PY{p}{(}\\PY{n}{new\\PYZus{}batch}\\PY{p}{,} \\PY{n}{label\\PYZus{}pad}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "# Copyright (c) 2022, NVIDIA CORPORATION.\n",
       "# SPDX-License-Identifier: Apache-2.0\n",
       "\n",
       "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "# you may not use this file except in compliance with the License.\n",
       "# You may obtain a copy of the License at\n",
       "#\n",
       "#     http://www.apache.org/licenses/LICENSE-2.0\n",
       "#\n",
       "# Unless required by applicable law or agreed to in writing, software\n",
       "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "# See the License for the specific language governing permissions and\n",
       "# limitations under the License.\n",
       "import torch\n",
       "\n",
       "from typing import List\n",
       "from nemo.collections.common.tokenizers import TokenizerSpec\n",
       "from bionemo.data.dataloader.collate import (\n",
       "    BertCollate,\n",
       "    BertMasking,\n",
       "    SentencePieceTokenizerAdapter,\n",
       ")\n",
       "from bionemo.data.dataloader.protein_collate import ProteinBertCollate\n",
       "\n",
       "\n",
       "__all__ = ['CustomProteinBertCollate']\n",
       "\n",
       "\n",
       "class CustomProteinBertCollate(ProteinBertCollate):\n",
       "   def collate_fn(self, batch: List[str], label_pad: int = -1):\n",
       "        '''\n",
       "        Parent does things like add padding, mask, onehot transformations, etc.\n",
       "        in this case, we do the same thing but we sort our batch on values (strings)\n",
       "        does this do anything useful in pratice? maybe not, but thats okay.\n",
       "\n",
       "        This method ultimately will get injected into a DataLoader. We do this as a \n",
       "        part of the standard dataloader setup method inside our ESM1nv model, by instantiating\n",
       "        this class and then injecting the collate_fn.\n",
       "        '''\n",
       "        extra = [] # Handles odd cases\n",
       "        if len(batch) % 2 == 1:\n",
       "            batch, extra = batch[:-1]\n",
       "\n",
       "        back, front = batch[:len(batch)], batch[len(batch):]\n",
       "        batch = back + front + extra\n",
       "\n",
       "        new_batch = [ 'A' * len(seq) for seq in batch ]\n",
       "\n",
       "        return super().collate_fn(new_batch, label_pad)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = f'{BIONEMO_WORKSPACE}/bionemo/data/dataloader/custom_protein_collate.py'\n",
    "show_code(filename=filename, language='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01232a11",
   "metadata": {},
   "source": [
    "## Injecting a custom collate object into an existing model.\n",
    "\n",
    "The implemented collate function servers a single purpose, it replaces all characters with the character 'A.' This is both easy to implement and simple to check for correctness. Upon doing so, the batch is passed back into the parent collate function for padding and masking. Next, we will inject this into our esm1nv model to be applied to the dataset. You can see below that this occurs on the `build_pretraining_data_loader` method, which primarily operates on an already existing Dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d0e453c",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"c1\"># Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.</span>\n",
       "<span class=\"c1\">#</span>\n",
       "<span class=\"c1\"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>\n",
       "<span class=\"c1\"># you may not use this file except in compliance with the License.</span>\n",
       "<span class=\"c1\"># You may obtain a copy of the License at</span>\n",
       "<span class=\"c1\">#</span>\n",
       "<span class=\"c1\">#     http://www.apache.org/licenses/LICENSE-2.0</span>\n",
       "<span class=\"c1\">#</span>\n",
       "<span class=\"c1\"># Unless required by applicable law or agreed to in writing, software</span>\n",
       "<span class=\"c1\"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>\n",
       "<span class=\"c1\"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>\n",
       "<span class=\"c1\"># See the License for the specific language governing permissions and</span>\n",
       "<span class=\"c1\"># limitations under the License.</span>\n",
       "\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">typing</span> <span class=\"kn\">import</span> <span class=\"n\">Dict</span><span class=\"p\">,</span> <span class=\"n\">Optional</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">omegaconf.dictconfig</span> <span class=\"kn\">import</span> <span class=\"n\">DictConfig</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">pytorch_lightning.trainer.trainer</span> <span class=\"kn\">import</span> <span class=\"n\">Trainer</span>\n",
       "\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">nemo.core.neural_types</span> <span class=\"kn\">import</span> <span class=\"n\">NeuralType</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">nemo.collections.nlp.models.language_modeling.megatron_bert_model</span> <span class=\"kn\">import</span> <span class=\"n\">MegatronBertModel</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">nemo.collections.nlp.modules.common.megatron.utils</span> <span class=\"kn\">import</span> <span class=\"p\">(</span>\n",
       "    <span class=\"n\">average_losses_across_data_parallel_group</span><span class=\"p\">,</span>\n",
       "<span class=\"p\">)</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">nemo.utils</span> <span class=\"kn\">import</span> <span class=\"n\">logging</span>\n",
       "\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">bionemo.model.protein.esm1nv.esm1nv_model</span> <span class=\"kn\">import</span> <span class=\"n\">ESM1nvModel</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">bionemo.data.molecule</span> <span class=\"kn\">import</span> <span class=\"n\">megamolbart_build_train_valid_test_datasets</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">bionemo.data.dataloader.custom_protein_collate</span> <span class=\"kn\">import</span> <span class=\"n\">CustomProteinBertCollate</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">nemo.collections.nlp.modules.common.tokenizer_utils</span> <span class=\"kn\">import</span> <span class=\"n\">get_nmt_tokenizer</span>\n",
       "\n",
       "<span class=\"k\">try</span><span class=\"p\">:</span>\n",
       "    <span class=\"kn\">from</span> <span class=\"nn\">apex.transformer</span> <span class=\"kn\">import</span> <span class=\"n\">tensor_parallel</span>\n",
       "\n",
       "\n",
       "    <span class=\"n\">HAVE_APEX</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>\n",
       "<span class=\"k\">except</span> <span class=\"p\">(</span><span class=\"ne\">ImportError</span><span class=\"p\">,</span> <span class=\"ne\">ModuleNotFoundError</span><span class=\"p\">):</span>\n",
       "    <span class=\"n\">HAVE_APEX</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n",
       "\n",
       "\n",
       "<span class=\"n\">__all__</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s2\">&quot;CustomESM1nvModel&quot;</span><span class=\"p\">]</span>\n",
       "\n",
       "<span class=\"k\">class</span> <span class=\"nc\">CustomESM1nvModel</span><span class=\"p\">(</span><span class=\"n\">ESM1nvModel</span><span class=\"p\">):</span>\n",
       "<span class=\"w\">    </span><span class=\"sd\">&quot;&quot;&quot; CustomESM1nv model that extends the dataloader function to use our custom collate function.</span>\n",
       "<span class=\"sd\">    Checkout the base class for more information on how it all fits together.</span>\n",
       "<span class=\"sd\">    &quot;&quot;&quot;</span>\n",
       "\n",
       "    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">cfg</span><span class=\"p\">:</span> <span class=\"n\">DictConfig</span><span class=\"p\">,</span> <span class=\"n\">trainer</span><span class=\"p\">:</span> <span class=\"n\">Trainer</span><span class=\"p\">):</span>\n",
       "        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"n\">cfg</span><span class=\"p\">,</span> <span class=\"n\">trainer</span><span class=\"o\">=</span><span class=\"n\">trainer</span><span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"k\">def</span> <span class=\"nf\">build_pretraining_data_loader</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">dataset</span><span class=\"p\">,</span> <span class=\"n\">consumed_samples</span><span class=\"p\">):</span>\n",
       "<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Buld dataloader given an input dataset.&quot;&quot;&quot;</span>\n",
       "\n",
       "        <span class=\"k\">assert</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_cfg</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader_type</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;single&#39;</span><span class=\"p\">,</span> <span class=\"ne\">AssertionError</span><span class=\"p\">(</span>\n",
       "            <span class=\"sa\">f</span><span class=\"s1\">&#39;Only the Megatron sequential (&quot;single&quot;) sampler is currently supported. </span><span class=\"si\">{</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_cfg</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">dataloader_type</span><span class=\"si\">}</span><span class=\"s1\"> was chosen.&#39;</span>\n",
       "            <span class=\"p\">)</span>\n",
       "\n",
       "        <span class=\"n\">dataloader</span> <span class=\"o\">=</span> <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">build_pretraining_data_loader</span><span class=\"p\">(</span><span class=\"n\">dataset</span><span class=\"o\">=</span><span class=\"n\">dataset</span><span class=\"p\">,</span> <span class=\"n\">consumed_samples</span><span class=\"o\">=</span><span class=\"n\">consumed_samples</span><span class=\"p\">)</span>\n",
       "\n",
       "        <span class=\"c1\"># Add collate function and unpin memory to avoid crash with CUDA misaligned address</span>\n",
       "        <span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">pin_memory</span> <span class=\"o\">=</span> <span class=\"kc\">False</span> <span class=\"c1\"># must be False with CSV dataset TODO check with binary</span>\n",
       "        <span class=\"n\">pad_size_divisible_by_8</span> <span class=\"o\">=</span> <span class=\"kc\">True</span> <span class=\"k\">if</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_cfg</span><span class=\"o\">.</span><span class=\"n\">masked_softmax_fusion</span> <span class=\"k\">else</span> <span class=\"kc\">False</span>\n",
       "\n",
       "        <span class=\"n\">dataloader</span><span class=\"o\">.</span><span class=\"n\">collate_fn</span> <span class=\"o\">=</span> <span class=\"n\">CustomProteinBertCollate</span><span class=\"p\">(</span><span class=\"n\">tokenizer</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">tokenizer</span><span class=\"p\">,</span>\n",
       "                                                    <span class=\"n\">seq_length</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_cfg</span><span class=\"o\">.</span><span class=\"n\">seq_length</span><span class=\"p\">,</span>\n",
       "                                                    <span class=\"n\">pad_size_divisible_by_8</span><span class=\"o\">=</span><span class=\"n\">pad_size_divisible_by_8</span><span class=\"p\">,</span>\n",
       "                                                    <span class=\"n\">modify_percent</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_cfg</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">modify_percent</span><span class=\"p\">,</span>\n",
       "                                                    <span class=\"n\">perturb_percent</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_cfg</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">perturb_percent</span><span class=\"p\">,</span>\n",
       "                                                    <span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">collate_fn</span>\n",
       "\n",
       "        <span class=\"k\">return</span> <span class=\"n\">dataloader</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{c+c1}{\\PYZsh{} Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.}\n",
       "\\PY{c+c1}{\\PYZsh{}}\n",
       "\\PY{c+c1}{\\PYZsh{} Licensed under the Apache License, Version 2.0 (the \\PYZdq{}License\\PYZdq{});}\n",
       "\\PY{c+c1}{\\PYZsh{} you may not use this file except in compliance with the License.}\n",
       "\\PY{c+c1}{\\PYZsh{} You may obtain a copy of the License at}\n",
       "\\PY{c+c1}{\\PYZsh{}}\n",
       "\\PY{c+c1}{\\PYZsh{}     http://www.apache.org/licenses/LICENSE\\PYZhy{}2.0}\n",
       "\\PY{c+c1}{\\PYZsh{}}\n",
       "\\PY{c+c1}{\\PYZsh{} Unless required by applicable law or agreed to in writing, software}\n",
       "\\PY{c+c1}{\\PYZsh{} distributed under the License is distributed on an \\PYZdq{}AS IS\\PYZdq{} BASIS,}\n",
       "\\PY{c+c1}{\\PYZsh{} WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.}\n",
       "\\PY{c+c1}{\\PYZsh{} See the License for the specific language governing permissions and}\n",
       "\\PY{c+c1}{\\PYZsh{} limitations under the License.}\n",
       "\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{typing} \\PY{k+kn}{import} \\PY{n}{Dict}\\PY{p}{,} \\PY{n}{Optional}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{omegaconf}\\PY{n+nn}{.}\\PY{n+nn}{dictconfig} \\PY{k+kn}{import} \\PY{n}{DictConfig}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{pytorch\\PYZus{}lightning}\\PY{n+nn}{.}\\PY{n+nn}{trainer}\\PY{n+nn}{.}\\PY{n+nn}{trainer} \\PY{k+kn}{import} \\PY{n}{Trainer}\n",
       "\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{nemo}\\PY{n+nn}{.}\\PY{n+nn}{core}\\PY{n+nn}{.}\\PY{n+nn}{neural\\PYZus{}types} \\PY{k+kn}{import} \\PY{n}{NeuralType}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{nemo}\\PY{n+nn}{.}\\PY{n+nn}{collections}\\PY{n+nn}{.}\\PY{n+nn}{nlp}\\PY{n+nn}{.}\\PY{n+nn}{models}\\PY{n+nn}{.}\\PY{n+nn}{language\\PYZus{}modeling}\\PY{n+nn}{.}\\PY{n+nn}{megatron\\PYZus{}bert\\PYZus{}model} \\PY{k+kn}{import} \\PY{n}{MegatronBertModel}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{nemo}\\PY{n+nn}{.}\\PY{n+nn}{collections}\\PY{n+nn}{.}\\PY{n+nn}{nlp}\\PY{n+nn}{.}\\PY{n+nn}{modules}\\PY{n+nn}{.}\\PY{n+nn}{common}\\PY{n+nn}{.}\\PY{n+nn}{megatron}\\PY{n+nn}{.}\\PY{n+nn}{utils} \\PY{k+kn}{import} \\PY{p}{(}\n",
       "    \\PY{n}{average\\PYZus{}losses\\PYZus{}across\\PYZus{}data\\PYZus{}parallel\\PYZus{}group}\\PY{p}{,}\n",
       "\\PY{p}{)}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{nemo}\\PY{n+nn}{.}\\PY{n+nn}{utils} \\PY{k+kn}{import} \\PY{n}{logging}\n",
       "\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{bionemo}\\PY{n+nn}{.}\\PY{n+nn}{model}\\PY{n+nn}{.}\\PY{n+nn}{protein}\\PY{n+nn}{.}\\PY{n+nn}{esm1nv}\\PY{n+nn}{.}\\PY{n+nn}{esm1nv\\PYZus{}model} \\PY{k+kn}{import} \\PY{n}{ESM1nvModel}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{bionemo}\\PY{n+nn}{.}\\PY{n+nn}{data}\\PY{n+nn}{.}\\PY{n+nn}{molecule} \\PY{k+kn}{import} \\PY{n}{megamolbart\\PYZus{}build\\PYZus{}train\\PYZus{}valid\\PYZus{}test\\PYZus{}datasets}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{bionemo}\\PY{n+nn}{.}\\PY{n+nn}{data}\\PY{n+nn}{.}\\PY{n+nn}{dataloader}\\PY{n+nn}{.}\\PY{n+nn}{custom\\PYZus{}protein\\PYZus{}collate} \\PY{k+kn}{import} \\PY{n}{CustomProteinBertCollate}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{nemo}\\PY{n+nn}{.}\\PY{n+nn}{collections}\\PY{n+nn}{.}\\PY{n+nn}{nlp}\\PY{n+nn}{.}\\PY{n+nn}{modules}\\PY{n+nn}{.}\\PY{n+nn}{common}\\PY{n+nn}{.}\\PY{n+nn}{tokenizer\\PYZus{}utils} \\PY{k+kn}{import} \\PY{n}{get\\PYZus{}nmt\\PYZus{}tokenizer}\n",
       "\n",
       "\\PY{k}{try}\\PY{p}{:}\n",
       "    \\PY{k+kn}{from} \\PY{n+nn}{apex}\\PY{n+nn}{.}\\PY{n+nn}{transformer} \\PY{k+kn}{import} \\PY{n}{tensor\\PYZus{}parallel}\n",
       "\n",
       "\n",
       "    \\PY{n}{HAVE\\PYZus{}APEX} \\PY{o}{=} \\PY{k+kc}{True}\n",
       "\\PY{k}{except} \\PY{p}{(}\\PY{n+ne}{ImportError}\\PY{p}{,} \\PY{n+ne}{ModuleNotFoundError}\\PY{p}{)}\\PY{p}{:}\n",
       "    \\PY{n}{HAVE\\PYZus{}APEX} \\PY{o}{=} \\PY{k+kc}{False}\n",
       "\n",
       "\n",
       "\\PY{n}{\\PYZus{}\\PYZus{}all\\PYZus{}\\PYZus{}} \\PY{o}{=} \\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{CustomESM1nvModel}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\n",
       "\n",
       "\\PY{k}{class} \\PY{n+nc}{CustomESM1nvModel}\\PY{p}{(}\\PY{n}{ESM1nvModel}\\PY{p}{)}\\PY{p}{:}\n",
       "\\PY{+w}{    }\\PY{l+s+sd}{\\PYZdq{}\\PYZdq{}\\PYZdq{} CustomESM1nv model that extends the dataloader function to use our custom collate function.}\n",
       "\\PY{l+s+sd}{    Checkout the base class for more information on how it all fits together.}\n",
       "\\PY{l+s+sd}{    \\PYZdq{}\\PYZdq{}\\PYZdq{}}\n",
       "\n",
       "    \\PY{k}{def} \\PY{n+nf+fm}{\\PYZus{}\\PYZus{}init\\PYZus{}\\PYZus{}}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{cfg}\\PY{p}{:} \\PY{n}{DictConfig}\\PY{p}{,} \\PY{n}{trainer}\\PY{p}{:} \\PY{n}{Trainer}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n+nb}{super}\\PY{p}{(}\\PY{p}{)}\\PY{o}{.}\\PY{n+nf+fm}{\\PYZus{}\\PYZus{}init\\PYZus{}\\PYZus{}}\\PY{p}{(}\\PY{n}{cfg}\\PY{p}{,} \\PY{n}{trainer}\\PY{o}{=}\\PY{n}{trainer}\\PY{p}{)}\n",
       "\n",
       "    \\PY{k}{def} \\PY{n+nf}{build\\PYZus{}pretraining\\PYZus{}data\\PYZus{}loader}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{dataset}\\PY{p}{,} \\PY{n}{consumed\\PYZus{}samples}\\PY{p}{)}\\PY{p}{:}\n",
       "\\PY{+w}{        }\\PY{l+s+sd}{\\PYZdq{}\\PYZdq{}\\PYZdq{}Buld dataloader given an input dataset.\\PYZdq{}\\PYZdq{}\\PYZdq{}}\n",
       "\n",
       "        \\PY{k}{assert} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{\\PYZus{}cfg}\\PY{o}{.}\\PY{n}{data}\\PY{o}{.}\\PY{n}{dataloader\\PYZus{}type} \\PY{o}{==} \\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{single}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{,} \\PY{n+ne}{AssertionError}\\PY{p}{(}\n",
       "            \\PY{l+s+sa}{f}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{Only the Megatron sequential (}\\PY{l+s+s1}{\\PYZdq{}}\\PY{l+s+s1}{single}\\PY{l+s+s1}{\\PYZdq{}}\\PY{l+s+s1}{) sampler is currently supported. }\\PY{l+s+si}{\\PYZob{}}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{\\PYZus{}cfg}\\PY{o}{.}\\PY{n}{data}\\PY{o}{.}\\PY{n}{dataloader\\PYZus{}type}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s1}{ was chosen.}\\PY{l+s+s1}{\\PYZsq{}}\n",
       "            \\PY{p}{)}\n",
       "\n",
       "        \\PY{n}{dataloader} \\PY{o}{=} \\PY{n+nb}{super}\\PY{p}{(}\\PY{p}{)}\\PY{o}{.}\\PY{n}{build\\PYZus{}pretraining\\PYZus{}data\\PYZus{}loader}\\PY{p}{(}\\PY{n}{dataset}\\PY{o}{=}\\PY{n}{dataset}\\PY{p}{,} \\PY{n}{consumed\\PYZus{}samples}\\PY{o}{=}\\PY{n}{consumed\\PYZus{}samples}\\PY{p}{)}\n",
       "\n",
       "        \\PY{c+c1}{\\PYZsh{} Add collate function and unpin memory to avoid crash with CUDA misaligned address}\n",
       "        \\PY{n}{dataloader}\\PY{o}{.}\\PY{n}{pin\\PYZus{}memory} \\PY{o}{=} \\PY{k+kc}{False} \\PY{c+c1}{\\PYZsh{} must be False with CSV dataset TODO check with binary}\n",
       "        \\PY{n}{pad\\PYZus{}size\\PYZus{}divisible\\PYZus{}by\\PYZus{}8} \\PY{o}{=} \\PY{k+kc}{True} \\PY{k}{if} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{\\PYZus{}cfg}\\PY{o}{.}\\PY{n}{masked\\PYZus{}softmax\\PYZus{}fusion} \\PY{k}{else} \\PY{k+kc}{False}\n",
       "\n",
       "        \\PY{n}{dataloader}\\PY{o}{.}\\PY{n}{collate\\PYZus{}fn} \\PY{o}{=} \\PY{n}{CustomProteinBertCollate}\\PY{p}{(}\\PY{n}{tokenizer}\\PY{o}{=}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{tokenizer}\\PY{p}{,}\n",
       "                                                    \\PY{n}{seq\\PYZus{}length}\\PY{o}{=}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{\\PYZus{}cfg}\\PY{o}{.}\\PY{n}{seq\\PYZus{}length}\\PY{p}{,}\n",
       "                                                    \\PY{n}{pad\\PYZus{}size\\PYZus{}divisible\\PYZus{}by\\PYZus{}8}\\PY{o}{=}\\PY{n}{pad\\PYZus{}size\\PYZus{}divisible\\PYZus{}by\\PYZus{}8}\\PY{p}{,}\n",
       "                                                    \\PY{n}{modify\\PYZus{}percent}\\PY{o}{=}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{\\PYZus{}cfg}\\PY{o}{.}\\PY{n}{data}\\PY{o}{.}\\PY{n}{modify\\PYZus{}percent}\\PY{p}{,}\n",
       "                                                    \\PY{n}{perturb\\PYZus{}percent}\\PY{o}{=}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{\\PYZus{}cfg}\\PY{o}{.}\\PY{n}{data}\\PY{o}{.}\\PY{n}{perturb\\PYZus{}percent}\\PY{p}{,}\n",
       "                                                    \\PY{p}{)}\\PY{o}{.}\\PY{n}{collate\\PYZus{}fn}\n",
       "\n",
       "        \\PY{k}{return} \\PY{n}{dataloader}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "# Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.\n",
       "#\n",
       "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "# you may not use this file except in compliance with the License.\n",
       "# You may obtain a copy of the License at\n",
       "#\n",
       "#     http://www.apache.org/licenses/LICENSE-2.0\n",
       "#\n",
       "# Unless required by applicable law or agreed to in writing, software\n",
       "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "# See the License for the specific language governing permissions and\n",
       "# limitations under the License.\n",
       "\n",
       "import torch\n",
       "from typing import Dict, Optional\n",
       "from omegaconf.dictconfig import DictConfig\n",
       "from pytorch_lightning.trainer.trainer import Trainer\n",
       "\n",
       "from nemo.core.neural_types import NeuralType\n",
       "from nemo.collections.nlp.models.language_modeling.megatron_bert_model import MegatronBertModel\n",
       "from nemo.collections.nlp.modules.common.megatron.utils import (\n",
       "    average_losses_across_data_parallel_group,\n",
       ")\n",
       "from nemo.utils import logging\n",
       "\n",
       "from bionemo.model.protein.esm1nv.esm1nv_model import ESM1nvModel\n",
       "from bionemo.data.molecule import megamolbart_build_train_valid_test_datasets\n",
       "from bionemo.data.dataloader.custom_protein_collate import CustomProteinBertCollate\n",
       "from nemo.collections.nlp.modules.common.tokenizer_utils import get_nmt_tokenizer\n",
       "\n",
       "try:\n",
       "    from apex.transformer import tensor_parallel\n",
       "\n",
       "\n",
       "    HAVE_APEX = True\n",
       "except (ImportError, ModuleNotFoundError):\n",
       "    HAVE_APEX = False\n",
       "\n",
       "\n",
       "__all__ = [\"CustomESM1nvModel\"]\n",
       "\n",
       "class CustomESM1nvModel(ESM1nvModel):\n",
       "    \"\"\" CustomESM1nv model that extends the dataloader function to use our custom collate function.\n",
       "    Checkout the base class for more information on how it all fits together.\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(self, cfg: DictConfig, trainer: Trainer):\n",
       "        super().__init__(cfg, trainer=trainer)\n",
       "\n",
       "    def build_pretraining_data_loader(self, dataset, consumed_samples):\n",
       "        \"\"\"Buld dataloader given an input dataset.\"\"\"\n",
       "\n",
       "        assert self._cfg.data.dataloader_type == 'single', AssertionError(\n",
       "            f'Only the Megatron sequential (\"single\") sampler is currently supported. {self._cfg.data.dataloader_type} was chosen.'\n",
       "            )\n",
       "\n",
       "        dataloader = super().build_pretraining_data_loader(dataset=dataset, consumed_samples=consumed_samples)\n",
       "\n",
       "        # Add collate function and unpin memory to avoid crash with CUDA misaligned address\n",
       "        dataloader.pin_memory = False # must be False with CSV dataset TODO check with binary\n",
       "        pad_size_divisible_by_8 = True if self._cfg.masked_softmax_fusion else False\n",
       "\n",
       "        dataloader.collate_fn = CustomProteinBertCollate(tokenizer=self.tokenizer,\n",
       "                                                    seq_length=self._cfg.seq_length,\n",
       "                                                    pad_size_divisible_by_8=pad_size_divisible_by_8,\n",
       "                                                    modify_percent=self._cfg.data.modify_percent,\n",
       "                                                    perturb_percent=self._cfg.data.perturb_percent,\n",
       "                                                    ).collate_fn\n",
       "\n",
       "        return dataloader"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = f'{BIONEMO_WORKSPACE}/bionemo/model/protein/esm1nv/custom_esm1nv_model.py'\n",
    "show_code(filename=filename, language='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ffa5a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-08-25 18:46:43 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-08-25 18:46:44 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-08-25 18:46:44 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'pretrain_oas': Defaults list is missing `_self_`. See https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "      warnings.warn(msg, UserWarning)\n",
      "    \n",
      "[NeMo W 2023-08-25 18:46:44 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n",
      "[NeMo I 2023-08-25 18:46:44 pretrain_oas:12] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2023-08-25 18:46:44 pretrain_oas:13] \n",
      "    name: esm1nv-oas\n",
      "    do_training: true\n",
      "    do_testing: false\n",
      "    restore_from_path: null\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      num_nodes: 1\n",
      "      accelerator: gpu\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      replace_sampler_ddp: false\n",
      "      max_epochs: null\n",
      "      max_steps: 101\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 100\n",
      "      limit_val_batches: 10\n",
      "      limit_test_batches: 500\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "      benchmark: false\n",
      "    exp_manager:\n",
      "      name: ${name}\n",
      "      exp_dir: /result/nemo_experiments/${.name}/${.wandb_logger_kwargs.name}\n",
      "      explicit_log_dir: ${.exp_dir}\n",
      "      create_wandb_logger: false\n",
      "      create_tensorboard_logger: true\n",
      "      wandb_logger_kwargs:\n",
      "        project: ${name}_pretraining\n",
      "        name: ${name}_pretraining\n",
      "        group: ${name}\n",
      "        job_type: Localhost_nodes_${trainer.num_nodes}_gpus_${trainer.devices}\n",
      "        notes: 'date: ${now:%y%m%d-%H%M%S}'\n",
      "        tags:\n",
      "        - ${name}\n",
      "        offline: false\n",
      "      resume_if_exists: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: val_loss\n",
      "        save_top_k: 10\n",
      "        mode: min\n",
      "        always_save_nemo: false\n",
      "        filename: megatron_bert--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "    model:\n",
      "      micro_batch_size: 8\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      seq_length: 512\n",
      "      max_position_embeddings: ${.seq_length}\n",
      "      encoder_seq_length: ${.seq_length}\n",
      "      num_layers: 6\n",
      "      hidden_size: 768\n",
      "      ffn_hidden_size: 3072\n",
      "      num_attention_heads: 12\n",
      "      init_method_std: 0.02\n",
      "      hidden_dropout: 0.1\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: true\n",
      "      bert_binary_head: false\n",
      "      resume_from_checkpoint: null\n",
      "      masked_softmax_fusion: true\n",
      "      tokenizer:\n",
      "        library: sentencepiece\n",
      "        type: null\n",
      "        model: /tokenizers/protein/esm1nv/vocab/protein_sequence_sentencepiece.model\n",
      "        vocab_file: /tokenizers/vocab/protein_sequence_sentencepiece.vocab\n",
      "        merge_file: null\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      seed: 1234\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: 1\n",
      "      data:\n",
      "        ngc_registry_target: uniref50_2022_05\n",
      "        ngc_registry_version: v23.06\n",
      "        data_prefix: ''\n",
      "        num_workers: 10\n",
      "        dataloader_type: single\n",
      "        reset_position_ids: false\n",
      "        reset_attention_mask: false\n",
      "        eod_mask_loss: false\n",
      "        masked_lm_prob: 0.15\n",
      "        short_seq_prob: 0.1\n",
      "        skip_lines: 0\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "        data_impl: csv_mmap\n",
      "        data_impl_kwargs:\n",
      "          csv_mmap:\n",
      "            header_lines: 1\n",
      "            newline_int: 10\n",
      "            workers: ${model.data.num_workers}\n",
      "            sort_dataset_paths: true\n",
      "            data_sep: ','\n",
      "            data_col: 1\n",
      "        use_upsampling: true\n",
      "        seed: ${model.seed}\n",
      "        max_seq_length: ${model.seq_length}\n",
      "        dataset_path: /data/OASpaired/processed/heavy\n",
      "        dataset:\n",
      "          train: x[000..005]\n",
      "          test: x[000..001]\n",
      "          val: x[000..001]\n",
      "        micro_batch_size: ${model.micro_batch_size}\n",
      "        modify_percent: 0.1\n",
      "        perturb_percent: 0.5\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0002\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 500\n",
      "          constant_steps: 50000\n",
      "          min_lr: 2.0e-05\n",
      "    do_dataloader: false\n",
      "    \n",
      "[NeMo W 2023-08-25 18:46:44 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/plugins/precision/native_amp.py:131: LightningDeprecationWarning: The `NativeMixedPrecisionPlugin` class has been renamed in v1.9.0 and will be removed in v2.0.0. Please use `pytorch_lightning.plugins.MixedPrecisionPlugin` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo I 2023-08-25 18:46:44 utils:168] Selected Callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo E 2023-08-25 18:46:44 exp_manager:646] exp_manager received explicit_log_dir: /result/nemo_experiments/esm1nv-oas/esm1nv-oas_pretraining and at least one of exp_dir: /result/nemo_experiments/esm1nv-oas/esm1nv-oas_pretraining, or version: None. Please note that exp_dir, name, and version will be ignored.\n",
      "[NeMo W 2023-08-25 18:46:44 exp_manager:651] Exp_manager is logging to /result/nemo_experiments/esm1nv-oas/esm1nv-oas_pretraining, but it already exists.\n",
      "[NeMo I 2023-08-25 18:46:44 exp_manager:374] Experiments will be logged at /result/nemo_experiments/esm1nv-oas/esm1nv-oas_pretraining\n",
      "[NeMo I 2023-08-25 18:46:44 exp_manager:797] TensorboardLogger has been set up\n",
      "[NeMo W 2023-08-25 18:46:44 exp_manager:893] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 101. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo I 2023-08-25 18:46:44 utils:191] Resuming training from checkpoint: None\n",
      "[NeMo I 2023-08-25 18:46:44 utils:234] \n",
      "    \n",
      "    ************** Trainer configuration ***********\n",
      "[NeMo I 2023-08-25 18:46:44 utils:235] \n",
      "    name: esm1nv-oas\n",
      "    do_training: true\n",
      "    do_testing: false\n",
      "    restore_from_path: null\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      num_nodes: 1\n",
      "      accelerator: gpu\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      replace_sampler_ddp: false\n",
      "      max_epochs: null\n",
      "      max_steps: 101\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 100\n",
      "      limit_val_batches: 10\n",
      "      limit_test_batches: 500\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "      benchmark: false\n",
      "    exp_manager:\n",
      "      name: ${name}\n",
      "      exp_dir: /result/nemo_experiments/${.name}/${.wandb_logger_kwargs.name}\n",
      "      explicit_log_dir: ${.exp_dir}\n",
      "      create_wandb_logger: false\n",
      "      create_tensorboard_logger: true\n",
      "      wandb_logger_kwargs:\n",
      "        project: ${name}_pretraining\n",
      "        name: ${name}_pretraining\n",
      "        group: ${name}\n",
      "        job_type: Localhost_nodes_${trainer.num_nodes}_gpus_${trainer.devices}\n",
      "        notes: 'date: ${now:%y%m%d-%H%M%S}'\n",
      "        tags:\n",
      "        - ${name}\n",
      "        offline: false\n",
      "      resume_if_exists: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: val_loss\n",
      "        save_top_k: 10\n",
      "        mode: min\n",
      "        always_save_nemo: false\n",
      "        filename: megatron_bert--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "    model:\n",
      "      micro_batch_size: 8\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      seq_length: 512\n",
      "      max_position_embeddings: ${.seq_length}\n",
      "      encoder_seq_length: ${.seq_length}\n",
      "      num_layers: 6\n",
      "      hidden_size: 768\n",
      "      ffn_hidden_size: 3072\n",
      "      num_attention_heads: 12\n",
      "      init_method_std: 0.02\n",
      "      hidden_dropout: 0.1\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: true\n",
      "      bert_binary_head: false\n",
      "      resume_from_checkpoint: null\n",
      "      masked_softmax_fusion: true\n",
      "      tokenizer:\n",
      "        library: sentencepiece\n",
      "        type: null\n",
      "        model: /tokenizers/protein/esm1nv/vocab/protein_sequence_sentencepiece.model\n",
      "        vocab_file: /tokenizers/vocab/protein_sequence_sentencepiece.vocab\n",
      "        merge_file: null\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      seed: 1234\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: 1\n",
      "      data:\n",
      "        ngc_registry_target: uniref50_2022_05\n",
      "        ngc_registry_version: v23.06\n",
      "        data_prefix: ''\n",
      "        num_workers: 10\n",
      "        dataloader_type: single\n",
      "        reset_position_ids: false\n",
      "        reset_attention_mask: false\n",
      "        eod_mask_loss: false\n",
      "        masked_lm_prob: 0.15\n",
      "        short_seq_prob: 0.1\n",
      "        skip_lines: 0\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "        data_impl: csv_mmap\n",
      "        data_impl_kwargs:\n",
      "          csv_mmap:\n",
      "            header_lines: 1\n",
      "            newline_int: 10\n",
      "            workers: ${model.data.num_workers}\n",
      "            sort_dataset_paths: true\n",
      "            data_sep: ','\n",
      "            data_col: 1\n",
      "        use_upsampling: true\n",
      "        seed: ${model.seed}\n",
      "        max_seq_length: ${model.seq_length}\n",
      "        dataset_path: /data/OASpaired/processed/heavy\n",
      "        dataset:\n",
      "          train: x[000..005]\n",
      "          test: x[000..001]\n",
      "          val: x[000..001]\n",
      "        micro_batch_size: ${model.micro_batch_size}\n",
      "        modify_percent: 0.1\n",
      "        perturb_percent: 0.5\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0002\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 500\n",
      "          constant_steps: 50000\n",
      "          min_lr: 2.0e-05\n",
      "      global_batch_size: 8\n",
      "      precision: 16\n",
      "    do_dataloader: false\n",
      "    \n",
      "[NeMo I 2023-08-25 18:46:44 pretrain_oas:19] ************** Starting Training ***********\n",
      "[NeMo I 2023-08-25 18:46:44 megatron_init:231] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2023-08-25 18:46:44 megatron_init:234] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2023-08-25 18:46:44 megatron_init:235] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-08-25 18:46:44 megatron_init:243] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-08-25 18:46:44 megatron_init:244] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-08-25 18:46:44 megatron_init:254] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-08-25 18:46:44 megatron_init:258] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-08-25 18:46:44 megatron_init:259] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-08-25 18:46:44 megatron_init:273] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-08-25 18:46:44 megatron_init:285] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-08-25 18:46:44 megatron_init:291] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-08-25 18:46:44 megatron_init:292] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-08-25 18:46:44 megatron_init:293] All embedding group ranks: [[0]]\n",
      "[NeMo I 2023-08-25 18:46:44 megatron_init:294] Rank 0 has embedding rank: 0\n",
      "23-08-25 18:46:44 - PID:2297 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 1\n",
      "[NeMo I 2023-08-25 18:46:44 tokenizer_utils:191] Getting SentencePiece with model: /tokenizers/protein/esm1nv/vocab/protein_sequence_sentencepiece.model\n",
      "[NeMo I 2023-08-25 18:46:44 megatron_base_model:229] Padded vocab_size: 128, original vocab_size: 30, dummy tokens: 98.\n",
      "[NeMo W 2023-08-25 18:46:44 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/configuration_validator.py:175: UserWarning: The `batch_idx` argument in `CustomESM1nvModel.on_train_batch_start` hook may not match with the actual batch index when using a `dataloader_iter` argument in your `training_step`.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2023-08-25 18:46:44 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/configuration_validator.py:175: UserWarning: The `batch_idx` argument in `CustomESM1nvModel.on_train_batch_end` hook may not match with the actual batch index when using a `dataloader_iter` argument in your `training_step`.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.\n",
      "Added key: store_based_barrier_key:3 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 1 nodes.\n",
      "Added key: store_based_barrier_key:4 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 1 nodes.\n",
      "Added key: store_based_barrier_key:5 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 1 nodes.\n",
      "Added key: store_based_barrier_key:6 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 1 nodes.\n",
      "Added key: store_based_barrier_key:7 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 1 nodes.\n",
      "[NeMo W 2023-08-25 18:46:45 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /result/nemo_experiments/esm1nv-oas/esm1nv-oas_pretraining/checkpoints exists and is not empty.\n",
      "      rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "    \n",
      "[NeMo I 2023-08-25 18:46:45 megatron_bert_model:563] Pipeline model parallel rank: 0, Tensor model parallel rank: 0, Number of model parameters on device: 4.36e+07. Total number of model parameters: 4.36e+07.\n",
      "[NeMo I 2023-08-25 18:46:45 esm1nv_model:96] Building Bert datasets.\n",
      "train:808\n",
      "Loading data from /data/OASpaired/processed/heavy/train/x000.csv, /data/OASpaired/processed/heavy/train/x001.csv, /data/OASpaired/processed/heavy/train/x002.csv, /data/OASpaired/processed/heavy/train/x003.csv, /data/OASpaired/processed/heavy/train/x004.csv, /data/OASpaired/processed/heavy/train/x005.csv\n",
      "[NeMo I 2023-08-25 18:46:45 text_memmap_dataset:104] Building data files\n",
      "[NeMo I 2023-08-25 18:46:45 text_memmap_dataset:343] Processing 6 data files using 10 workers\n",
      "[NeMo I 2023-08-25 18:46:45 text_memmap_dataset:349] Time building 0 / 6 mem-mapped files: 0:00:00.235856\n",
      "[NeMo I 2023-08-25 18:46:45 text_memmap_dataset:114] Loading data files\n",
      "[NeMo I 2023-08-25 18:46:45 text_memmap_dataset:205] Loading /data/OASpaired/processed/heavy/train/x000.csv\n",
      "[NeMo I 2023-08-25 18:46:45 text_memmap_dataset:205] Loading /data/OASpaired/processed/heavy/train/x001.csv\n",
      "[NeMo I 2023-08-25 18:46:45 text_memmap_dataset:205] Loading /data/OASpaired/processed/heavy/train/x002.csv\n",
      "[NeMo I 2023-08-25 18:46:45 text_memmap_dataset:205] Loading /data/OASpaired/processed/heavy/train/x003.csv\n",
      "[NeMo I 2023-08-25 18:46:45 text_memmap_dataset:205] Loading /data/OASpaired/processed/heavy/train/x004.csv\n",
      "[NeMo I 2023-08-25 18:46:45 text_memmap_dataset:205] Loading /data/OASpaired/processed/heavy/train/x005.csv\n",
      "[NeMo I 2023-08-25 18:46:45 text_memmap_dataset:117] Time loading 6 mem-mapped files: 0:00:00.002752\n",
      "[NeMo I 2023-08-25 18:46:45 text_memmap_dataset:121] Computing global indices\n",
      "[NeMo I 2023-08-25 18:46:45 dataset_utils:1341]  > loading indexed mapping from /data/OASpaired/processed/heavy/train/__indexmap_808mns_512msl_0.00ssp_1234s.npy\n",
      "[NeMo I 2023-08-25 18:46:45 dataset_utils:1344]     loaded indexed file in 0.001 seconds\n",
      "[NeMo I 2023-08-25 18:46:45 dataset_utils:1345]     total number of samples: 21129\n",
      "val:160\n",
      "Loading data from /data/OASpaired/processed/heavy/val/x000.csv, /data/OASpaired/processed/heavy/val/x001.csv\n",
      "[NeMo I 2023-08-25 18:46:45 text_memmap_dataset:104] Building data files\n",
      "[NeMo I 2023-08-25 18:46:45 text_memmap_dataset:343] Processing 2 data files using 10 workers\n",
      "[NeMo I 2023-08-25 18:46:46 text_memmap_dataset:349] Time building 0 / 2 mem-mapped files: 0:00:00.231405\n",
      "[NeMo I 2023-08-25 18:46:46 text_memmap_dataset:114] Loading data files\n",
      "[NeMo I 2023-08-25 18:46:46 text_memmap_dataset:205] Loading /data/OASpaired/processed/heavy/val/x000.csv\n",
      "[NeMo I 2023-08-25 18:46:46 text_memmap_dataset:205] Loading /data/OASpaired/processed/heavy/val/x001.csv\n",
      "[NeMo I 2023-08-25 18:46:46 text_memmap_dataset:117] Time loading 2 mem-mapped files: 0:00:00.001060\n",
      "[NeMo I 2023-08-25 18:46:46 text_memmap_dataset:121] Computing global indices\n",
      "[NeMo I 2023-08-25 18:46:46 dataset_utils:1341]  > loading indexed mapping from /data/OASpaired/processed/heavy/val/__indexmap_160mns_512msl_0.00ssp_1234s.npy\n",
      "[NeMo I 2023-08-25 18:46:46 dataset_utils:1344]     loaded indexed file in 0.000 seconds\n",
      "[NeMo I 2023-08-25 18:46:46 dataset_utils:1345]     total number of samples: 294\n",
      "test:4000\n",
      "Loading data from /data/OASpaired/processed/heavy/test/x000.csv, /data/OASpaired/processed/heavy/test/x001.csv\n",
      "[NeMo I 2023-08-25 18:46:46 text_memmap_dataset:104] Building data files\n",
      "[NeMo I 2023-08-25 18:46:46 text_memmap_dataset:343] Processing 2 data files using 10 workers\n",
      "[NeMo I 2023-08-25 18:46:46 text_memmap_dataset:349] Time building 0 / 2 mem-mapped files: 0:00:00.246505\n",
      "[NeMo I 2023-08-25 18:46:46 text_memmap_dataset:114] Loading data files\n",
      "[NeMo I 2023-08-25 18:46:46 text_memmap_dataset:205] Loading /data/OASpaired/processed/heavy/test/x000.csv\n",
      "[NeMo I 2023-08-25 18:46:46 text_memmap_dataset:205] Loading /data/OASpaired/processed/heavy/test/x001.csv\n",
      "[NeMo I 2023-08-25 18:46:46 text_memmap_dataset:117] Time loading 2 mem-mapped files: 0:00:00.001123\n",
      "[NeMo I 2023-08-25 18:46:46 text_memmap_dataset:121] Computing global indices\n",
      "[NeMo I 2023-08-25 18:46:46 dataset_utils:1341]  > loading indexed mapping from /data/OASpaired/processed/heavy/test/__indexmap_4000mns_512msl_0.00ssp_1234s.npy\n",
      "[NeMo I 2023-08-25 18:46:46 dataset_utils:1344]     loaded indexed file in 0.000 seconds\n",
      "[NeMo I 2023-08-25 18:46:46 dataset_utils:1345]     total number of samples: 6499\n",
      "[NeMo I 2023-08-25 18:46:46 esm1nv_model:114] Length of train dataset: 808\n",
      "[NeMo I 2023-08-25 18:46:46 esm1nv_model:115] Length of val dataset: 160\n",
      "[NeMo I 2023-08-25 18:46:46 esm1nv_model:116] Length of test dataset: 4000\n",
      "[NeMo I 2023-08-25 18:46:46 esm1nv_model:117] Finished building Bert datasets.\n",
      "[NeMo I 2023-08-25 18:46:46 megatron_bert_model:662] Setting up train dataloader with len(len(self._train_ds)): 808 and consumed samples: 0\n",
      "[NeMo I 2023-08-25 18:46:46 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 808 and consumed_samples: 0\n",
      "[NeMo I 2023-08-25 18:46:46 megatron_bert_model:670] Setting up validation dataloader with len(len(self._validation_ds)): 160 and consumed samples: 0\n",
      "[NeMo I 2023-08-25 18:46:46 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 160 and consumed_samples: 0\n",
      "[NeMo I 2023-08-25 18:46:46 megatron_bert_model:678] Setting up test dataloader with len(len(self._test_ds)): 4000 and consumed samples: 0\n",
      "[NeMo I 2023-08-25 18:46:46 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 4000 and consumed_samples: 0\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[NeMo I 2023-08-25 18:46:46 nlp_overrides:124] Configuring DDP for model parallelism.\n",
      "[NeMo I 2023-08-25 18:46:46 modelPT:722] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0002\n",
      "        weight_decay: 0.01\n",
      "    \n",
      "    Parameter Group 1\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0002\n",
      "        weight_decay: 0.0\n",
      "    )\n",
      "[NeMo I 2023-08-25 18:46:46 lr_scheduler:910] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f39a83d3400>\" \n",
      "    will be used during training (effective maximum steps = 101) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 500\n",
      "    constant_steps: 50000\n",
      "    min_lr: 2.0e-05\n",
      "    max_steps: 101\n",
      "    )\n",
      "\n",
      "  | Name                           | Type                     | Params\n",
      "----------------------------------------------------------------------------\n",
      "0 | model                          | BertModel                | 43.6 M\n",
      "1 | model.language_model           | TransformerLanguageModel | 43.0 M\n",
      "2 | model.language_model.embedding | Embedding                | 491 K \n",
      "3 | model.language_model.encoder   | ParallelTransformer      | 42.5 M\n",
      "4 | model.lm_head                  | BertLMHead               | 592 K \n",
      "5 | model.lm_head.dense            | Linear                   | 590 K \n",
      "6 | model.lm_head.layernorm        | MixedFusedLayerNorm      | 1.5 K \n",
      "----------------------------------------------------------------------------\n",
      "43.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "43.6 M    Total params\n",
      "87.225    Total estimated model params size (MB)\n",
      "\n",
      "Sanity Checking: 0it [00:00, ?it/s][NeMo W 2023-08-25 18:46:46 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:401: UserWarning: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "\n",
      "Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:01<00:01,  1.56s/it]\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:01<00:00,  1.26it/s][NeMo W 2023-08-25 18:46:48 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:232: UserWarning: You called `self.log('consumed_samples', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-08-25 18:46:48 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-08-25 18:46:48 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('val_loss_ECE', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-08-25 18:46:48 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:536: PossibleUserWarning: It is recommended to use `self.log('consumed_samples', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "\n",
      "                                                                           [NeMo W 2023-08-25 18:46:48 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/loops/fit_loop.py:344: UserWarning: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "\n",
      "\n",
      "Training: 0it [00:00, ?it/s]\n",
      "Training:   0%|          | 0/111 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/111 [00:00<?, ?it/s] [NeMo W 2023-08-25 18:46:50 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:232: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-08-25 18:46:50 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:232: UserWarning: You called `self.log('consumed_samples', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-08-25 18:46:50 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "      warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "    \n",
      "\n",
      "Epoch 0:   1%|          | 1/111 [00:01<03:36,  1.97s/it]\n",
      "Epoch 0:   1%|          | 1/111 [00:01<03:36,  1.97s/it, loss=4.82, v_num=, reduced_train_loss=4.820, global_step=0.000, consumed_samples=0.000]\n",
      "Epoch 0:   2%|▏         | 2/111 [00:02<01:49,  1.01s/it, loss=4.82, v_num=, reduced_train_loss=4.820, global_step=0.000, consumed_samples=0.000]\n",
      "Epoch 0:   2%|▏         | 2/111 [00:02<01:49,  1.01s/it, loss=4.83, v_num=, reduced_train_loss=4.840, global_step=1.000, consumed_samples=8.000]\n",
      "Epoch 0:   3%|▎         | 3/111 [00:02<01:15,  1.42it/s, loss=4.83, v_num=, reduced_train_loss=4.840, global_step=1.000, consumed_samples=8.000]\n",
      "Epoch 0:   3%|▎         | 3/111 [00:02<01:15,  1.42it/s, loss=4.83, v_num=, reduced_train_loss=4.830, global_step=2.000, consumed_samples=16.00]\n",
      "Epoch 0:   4%|▎         | 4/111 [00:02<00:57,  1.86it/s, loss=4.83, v_num=, reduced_train_loss=4.830, global_step=2.000, consumed_samples=16.00]\n",
      "Epoch 0:   4%|▎         | 4/111 [00:02<00:57,  1.86it/s, loss=4.84, v_num=, reduced_train_loss=4.860, global_step=3.000, consumed_samples=24.00]\n",
      "Epoch 0:   5%|▍         | 5/111 [00:02<00:46,  2.28it/s, loss=4.84, v_num=, reduced_train_loss=4.860, global_step=3.000, consumed_samples=24.00]\n",
      "Epoch 0:   5%|▍         | 5/111 [00:02<00:46,  2.28it/s, loss=4.84, v_num=, reduced_train_loss=4.840, global_step=4.000, consumed_samples=32.00]\n",
      "Epoch 0:   5%|▌         | 6/111 [00:02<00:39,  2.68it/s, loss=4.84, v_num=, reduced_train_loss=4.840, global_step=4.000, consumed_samples=32.00]\n",
      "Epoch 0:   5%|▌         | 6/111 [00:02<00:39,  2.68it/s, loss=4.84, v_num=, reduced_train_loss=4.840, global_step=5.000, consumed_samples=40.00]\n",
      "Epoch 0:   6%|▋         | 7/111 [00:02<00:33,  3.06it/s, loss=4.84, v_num=, reduced_train_loss=4.840, global_step=5.000, consumed_samples=40.00]\n",
      "Epoch 0:   6%|▋         | 7/111 [00:02<00:33,  3.06it/s, loss=4.84, v_num=, reduced_train_loss=4.810, global_step=6.000, consumed_samples=48.00]\n",
      "Epoch 0:   7%|▋         | 8/111 [00:02<00:29,  3.44it/s, loss=4.84, v_num=, reduced_train_loss=4.810, global_step=6.000, consumed_samples=48.00]\n",
      "Epoch 0:   7%|▋         | 8/111 [00:02<00:29,  3.44it/s, loss=4.84, v_num=, reduced_train_loss=4.840, global_step=7.000, consumed_samples=56.00]\n",
      "Epoch 0:   8%|▊         | 9/111 [00:02<00:26,  3.80it/s, loss=4.84, v_num=, reduced_train_loss=4.840, global_step=7.000, consumed_samples=56.00]\n",
      "Epoch 0:   8%|▊         | 9/111 [00:02<00:26,  3.80it/s, loss=4.84, v_num=, reduced_train_loss=4.840, global_step=8.000, consumed_samples=64.00]\n",
      "Epoch 0:   9%|▉         | 10/111 [00:02<00:24,  4.14it/s, loss=4.84, v_num=, reduced_train_loss=4.840, global_step=8.000, consumed_samples=64.00]\n",
      "Epoch 0:   9%|▉         | 10/111 [00:02<00:24,  4.14it/s, loss=4.84, v_num=, reduced_train_loss=4.850, global_step=9.000, consumed_samples=72.00]\n",
      "Epoch 0:  10%|▉         | 11/111 [00:02<00:22,  4.46it/s, loss=4.84, v_num=, reduced_train_loss=4.850, global_step=9.000, consumed_samples=72.00]\n",
      "Epoch 0:  10%|▉         | 11/111 [00:02<00:22,  4.46it/s, loss=4.84, v_num=, reduced_train_loss=4.840, global_step=10.00, consumed_samples=80.00]\n",
      "Epoch 0:  11%|█         | 12/111 [00:02<00:20,  4.78it/s, loss=4.84, v_num=, reduced_train_loss=4.840, global_step=10.00, consumed_samples=80.00]\n",
      "Epoch 0:  11%|█         | 12/111 [00:02<00:20,  4.78it/s, loss=4.84, v_num=, reduced_train_loss=4.850, global_step=11.00, consumed_samples=88.00]\n",
      "Epoch 0:  12%|█▏        | 13/111 [00:02<00:19,  5.07it/s, loss=4.84, v_num=, reduced_train_loss=4.850, global_step=11.00, consumed_samples=88.00]\n",
      "Epoch 0:  12%|█▏        | 13/111 [00:02<00:19,  5.07it/s, loss=4.84, v_num=, reduced_train_loss=4.820, global_step=12.00, consumed_samples=96.00]\n",
      "Epoch 0:  13%|█▎        | 14/111 [00:02<00:18,  5.37it/s, loss=4.84, v_num=, reduced_train_loss=4.820, global_step=12.00, consumed_samples=96.00]\n",
      "Epoch 0:  13%|█▎        | 14/111 [00:02<00:18,  5.37it/s, loss=4.84, v_num=, reduced_train_loss=4.800, global_step=13.00, consumed_samples=104.0]\n",
      "Epoch 0:  14%|█▎        | 15/111 [00:02<00:16,  5.66it/s, loss=4.84, v_num=, reduced_train_loss=4.800, global_step=13.00, consumed_samples=104.0]\n",
      "Epoch 0:  14%|█▎        | 15/111 [00:02<00:16,  5.66it/s, loss=4.84, v_num=, reduced_train_loss=4.840, global_step=14.00, consumed_samples=112.0]\n",
      "Epoch 0:  14%|█▍        | 16/111 [00:02<00:16,  5.93it/s, loss=4.84, v_num=, reduced_train_loss=4.840, global_step=14.00, consumed_samples=112.0]\n",
      "Epoch 0:  14%|█▍        | 16/111 [00:02<00:16,  5.93it/s, loss=4.83, v_num=, reduced_train_loss=4.810, global_step=15.00, consumed_samples=120.0]\n",
      "Epoch 0:  15%|█▌        | 17/111 [00:02<00:15,  6.20it/s, loss=4.83, v_num=, reduced_train_loss=4.810, global_step=15.00, consumed_samples=120.0]\n",
      "Epoch 0:  15%|█▌        | 17/111 [00:02<00:15,  6.20it/s, loss=4.83, v_num=, reduced_train_loss=4.820, global_step=16.00, consumed_samples=128.0]\n",
      "Epoch 0:  16%|█▌        | 18/111 [00:02<00:14,  6.46it/s, loss=4.83, v_num=, reduced_train_loss=4.820, global_step=16.00, consumed_samples=128.0]\n",
      "Epoch 0:  16%|█▌        | 18/111 [00:02<00:14,  6.46it/s, loss=4.83, v_num=, reduced_train_loss=4.830, global_step=17.00, consumed_samples=136.0]\n",
      "Epoch 0:  17%|█▋        | 19/111 [00:02<00:13,  6.71it/s, loss=4.83, v_num=, reduced_train_loss=4.830, global_step=17.00, consumed_samples=136.0]\n",
      "Epoch 0:  17%|█▋        | 19/111 [00:02<00:13,  6.71it/s, loss=4.83, v_num=, reduced_train_loss=4.830, global_step=18.00, consumed_samples=144.0]\n",
      "Epoch 0:  18%|█▊        | 20/111 [00:02<00:13,  6.96it/s, loss=4.83, v_num=, reduced_train_loss=4.830, global_step=18.00, consumed_samples=144.0]\n",
      "Epoch 0:  18%|█▊        | 20/111 [00:02<00:13,  6.96it/s, loss=4.83, v_num=, reduced_train_loss=4.820, global_step=19.00, consumed_samples=152.0]\n",
      "Epoch 0:  19%|█▉        | 21/111 [00:02<00:12,  7.19it/s, loss=4.83, v_num=, reduced_train_loss=4.820, global_step=19.00, consumed_samples=152.0]\n",
      "Epoch 0:  19%|█▉        | 21/111 [00:02<00:12,  7.18it/s, loss=4.83, v_num=, reduced_train_loss=4.850, global_step=20.00, consumed_samples=160.0]\n",
      "Epoch 0:  20%|█▉        | 22/111 [00:02<00:12,  7.41it/s, loss=4.83, v_num=, reduced_train_loss=4.850, global_step=20.00, consumed_samples=160.0]\n",
      "Epoch 0:  20%|█▉        | 22/111 [00:02<00:12,  7.41it/s, loss=4.83, v_num=, reduced_train_loss=4.860, global_step=21.00, consumed_samples=168.0]\n",
      "Epoch 0:  21%|██        | 23/111 [00:03<00:11,  7.63it/s, loss=4.83, v_num=, reduced_train_loss=4.860, global_step=21.00, consumed_samples=168.0]\n",
      "Epoch 0:  21%|██        | 23/111 [00:03<00:11,  7.63it/s, loss=4.83, v_num=, reduced_train_loss=4.650, global_step=22.00, consumed_samples=176.0]\n",
      "Epoch 0:  22%|██▏       | 24/111 [00:03<00:11,  7.83it/s, loss=4.83, v_num=, reduced_train_loss=4.650, global_step=22.00, consumed_samples=176.0]\n",
      "Epoch 0:  22%|██▏       | 24/111 [00:03<00:11,  7.83it/s, loss=4.8, v_num=, reduced_train_loss=4.350, global_step=23.00, consumed_samples=184.0] \n",
      "Epoch 0:  23%|██▎       | 25/111 [00:03<00:10,  8.02it/s, loss=4.8, v_num=, reduced_train_loss=4.350, global_step=23.00, consumed_samples=184.0]\n",
      "Epoch 0:  23%|██▎       | 25/111 [00:03<00:10,  8.02it/s, loss=4.75, v_num=, reduced_train_loss=3.900, global_step=24.00, consumed_samples=192.0]\n",
      "Epoch 0:  23%|██▎       | 26/111 [00:03<00:10,  8.21it/s, loss=4.75, v_num=, reduced_train_loss=3.900, global_step=24.00, consumed_samples=192.0]\n",
      "Epoch 0:  23%|██▎       | 26/111 [00:03<00:10,  8.21it/s, loss=4.68, v_num=, reduced_train_loss=3.310, global_step=25.00, consumed_samples=200.0]\n",
      "Epoch 0:  24%|██▍       | 27/111 [00:03<00:10,  8.39it/s, loss=4.68, v_num=, reduced_train_loss=3.310, global_step=25.00, consumed_samples=200.0]\n",
      "Epoch 0:  24%|██▍       | 27/111 [00:03<00:10,  8.39it/s, loss=4.57, v_num=, reduced_train_loss=2.680, global_step=26.00, consumed_samples=208.0]\n",
      "Epoch 0:  25%|██▌       | 28/111 [00:03<00:09,  8.56it/s, loss=4.57, v_num=, reduced_train_loss=2.680, global_step=26.00, consumed_samples=208.0]\n",
      "Epoch 0:  25%|██▌       | 28/111 [00:03<00:09,  8.56it/s, loss=4.43, v_num=, reduced_train_loss=2.000, global_step=27.00, consumed_samples=216.0]\n",
      "Epoch 0:  26%|██▌       | 29/111 [00:03<00:09,  8.75it/s, loss=4.43, v_num=, reduced_train_loss=2.000, global_step=27.00, consumed_samples=216.0]\n",
      "Epoch 0:  26%|██▌       | 29/111 [00:03<00:09,  8.74it/s, loss=4.26, v_num=, reduced_train_loss=1.400, global_step=28.00, consumed_samples=224.0]\n",
      "Epoch 0:  27%|██▋       | 30/111 [00:03<00:09,  8.92it/s, loss=4.26, v_num=, reduced_train_loss=1.400, global_step=28.00, consumed_samples=224.0]\n",
      "Epoch 0:  27%|██▋       | 30/111 [00:03<00:09,  8.92it/s, loss=4.06, v_num=, reduced_train_loss=0.983, global_step=29.00, consumed_samples=232.0]\n",
      "Epoch 0:  28%|██▊       | 31/111 [00:03<00:08,  9.09it/s, loss=4.06, v_num=, reduced_train_loss=0.983, global_step=29.00, consumed_samples=232.0]\n",
      "Epoch 0:  28%|██▊       | 31/111 [00:03<00:08,  9.09it/s, loss=3.85, v_num=, reduced_train_loss=0.624, global_step=30.00, consumed_samples=240.0]\n",
      "Epoch 0:  29%|██▉       | 32/111 [00:03<00:08,  9.26it/s, loss=3.85, v_num=, reduced_train_loss=0.624, global_step=30.00, consumed_samples=240.0]\n",
      "Epoch 0:  29%|██▉       | 32/111 [00:03<00:08,  9.26it/s, loss=3.63, v_num=, reduced_train_loss=0.420, global_step=31.00, consumed_samples=248.0]\n",
      "Epoch 0:  30%|██▉       | 33/111 [00:03<00:08,  9.43it/s, loss=3.63, v_num=, reduced_train_loss=0.420, global_step=31.00, consumed_samples=248.0]\n",
      "Epoch 0:  30%|██▉       | 33/111 [00:03<00:08,  9.43it/s, loss=3.4, v_num=, reduced_train_loss=0.256, global_step=32.00, consumed_samples=256.0] \n",
      "Epoch 0:  31%|███       | 34/111 [00:03<00:08,  9.57it/s, loss=3.4, v_num=, reduced_train_loss=0.256, global_step=32.00, consumed_samples=256.0]\n",
      "Epoch 0:  31%|███       | 34/111 [00:03<00:08,  9.57it/s, loss=3.17, v_num=, reduced_train_loss=0.180, global_step=33.00, consumed_samples=264.0]\n",
      "Epoch 0:  32%|███▏      | 35/111 [00:03<00:07,  9.72it/s, loss=3.17, v_num=, reduced_train_loss=0.180, global_step=33.00, consumed_samples=264.0]\n",
      "Epoch 0:  32%|███▏      | 35/111 [00:03<00:07,  9.72it/s, loss=2.93, v_num=, reduced_train_loss=0.121, global_step=34.00, consumed_samples=272.0]\n",
      "Epoch 0:  32%|███▏      | 36/111 [00:03<00:07,  9.86it/s, loss=2.93, v_num=, reduced_train_loss=0.121, global_step=34.00, consumed_samples=272.0]\n",
      "Epoch 0:  32%|███▏      | 36/111 [00:03<00:07,  9.86it/s, loss=2.7, v_num=, reduced_train_loss=0.0845, global_step=35.00, consumed_samples=280.0]\n",
      "Epoch 0:  33%|███▎      | 37/111 [00:03<00:07,  9.99it/s, loss=2.7, v_num=, reduced_train_loss=0.0845, global_step=35.00, consumed_samples=280.0]\n",
      "Epoch 0:  33%|███▎      | 37/111 [00:03<00:07,  9.99it/s, loss=2.46, v_num=, reduced_train_loss=0.081, global_step=36.00, consumed_samples=288.0]\n",
      "Epoch 0:  34%|███▍      | 38/111 [00:03<00:07, 10.14it/s, loss=2.46, v_num=, reduced_train_loss=0.081, global_step=36.00, consumed_samples=288.0]\n",
      "Epoch 0:  34%|███▍      | 38/111 [00:03<00:07, 10.14it/s, loss=2.22, v_num=, reduced_train_loss=0.0499, global_step=37.00, consumed_samples=296.0]\n",
      "Epoch 0:  35%|███▌      | 39/111 [00:03<00:06, 10.29it/s, loss=2.22, v_num=, reduced_train_loss=0.0499, global_step=37.00, consumed_samples=296.0]\n",
      "Epoch 0:  35%|███▌      | 39/111 [00:03<00:06, 10.29it/s, loss=1.98, v_num=, reduced_train_loss=0.042, global_step=38.00, consumed_samples=304.0] \n",
      "Epoch 0:  36%|███▌      | 40/111 [00:03<00:06, 10.42it/s, loss=1.98, v_num=, reduced_train_loss=0.042, global_step=38.00, consumed_samples=304.0]\n",
      "Epoch 0:  36%|███▌      | 40/111 [00:03<00:06, 10.42it/s, loss=1.74, v_num=, reduced_train_loss=0.0138, global_step=39.00, consumed_samples=312.0]\n",
      "Epoch 0:  37%|███▋      | 41/111 [00:03<00:06, 10.54it/s, loss=1.74, v_num=, reduced_train_loss=0.0138, global_step=39.00, consumed_samples=312.0]\n",
      "Epoch 0:  37%|███▋      | 41/111 [00:03<00:06, 10.54it/s, loss=1.5, v_num=, reduced_train_loss=0.0586, global_step=40.00, consumed_samples=320.0] \n",
      "Epoch 0:  38%|███▊      | 42/111 [00:03<00:06, 10.67it/s, loss=1.5, v_num=, reduced_train_loss=0.0586, global_step=40.00, consumed_samples=320.0]\n",
      "Epoch 0:  38%|███▊      | 42/111 [00:03<00:06, 10.67it/s, loss=1.26, v_num=, reduced_train_loss=0.00855, global_step=41.00, consumed_samples=328.0]\n",
      "Epoch 0:  39%|███▊      | 43/111 [00:03<00:06, 10.79it/s, loss=1.26, v_num=, reduced_train_loss=0.00855, global_step=41.00, consumed_samples=328.0]\n",
      "Epoch 0:  39%|███▊      | 43/111 [00:03<00:06, 10.79it/s, loss=1.03, v_num=, reduced_train_loss=0.0298, global_step=42.00, consumed_samples=336.0] \n",
      "Epoch 0:  40%|███▉      | 44/111 [00:04<00:06, 10.91it/s, loss=1.03, v_num=, reduced_train_loss=0.0298, global_step=42.00, consumed_samples=336.0]\n",
      "Epoch 0:  40%|███▉      | 44/111 [00:04<00:06, 10.91it/s, loss=0.813, v_num=, reduced_train_loss=0.0272, global_step=43.00, consumed_samples=344.0]\n",
      "Epoch 0:  41%|████      | 45/111 [00:04<00:05, 11.04it/s, loss=0.813, v_num=, reduced_train_loss=0.0272, global_step=43.00, consumed_samples=344.0]\n",
      "Epoch 0:  41%|████      | 45/111 [00:04<00:05, 11.03it/s, loss=0.619, v_num=, reduced_train_loss=0.00627, global_step=44.00, consumed_samples=352.0]\n",
      "Epoch 0:  41%|████▏     | 46/111 [00:04<00:05, 11.15it/s, loss=0.619, v_num=, reduced_train_loss=0.00627, global_step=44.00, consumed_samples=352.0]\n",
      "Epoch 0:  41%|████▏     | 46/111 [00:04<00:05, 11.15it/s, loss=0.454, v_num=, reduced_train_loss=0.0058, global_step=45.00, consumed_samples=360.0] \n",
      "Epoch 0:  42%|████▏     | 47/111 [00:04<00:05, 11.26it/s, loss=0.454, v_num=, reduced_train_loss=0.0058, global_step=45.00, consumed_samples=360.0]\n",
      "Epoch 0:  42%|████▏     | 47/111 [00:04<00:05, 11.26it/s, loss=0.322, v_num=, reduced_train_loss=0.0466, global_step=46.00, consumed_samples=368.0]\n",
      "Epoch 0:  43%|████▎     | 48/111 [00:04<00:05, 11.38it/s, loss=0.322, v_num=, reduced_train_loss=0.0466, global_step=46.00, consumed_samples=368.0]\n",
      "Epoch 0:  43%|████▎     | 48/111 [00:04<00:05, 11.38it/s, loss=0.222, v_num=, reduced_train_loss=0.00506, global_step=47.00, consumed_samples=376.0]\n",
      "Epoch 0:  44%|████▍     | 49/111 [00:04<00:05, 11.49it/s, loss=0.222, v_num=, reduced_train_loss=0.00506, global_step=47.00, consumed_samples=376.0]\n",
      "Epoch 0:  44%|████▍     | 49/111 [00:04<00:05, 11.49it/s, loss=0.152, v_num=, reduced_train_loss=0.00475, global_step=48.00, consumed_samples=384.0]\n",
      "Epoch 0:  45%|████▌     | 50/111 [00:04<00:05, 11.57it/s, loss=0.152, v_num=, reduced_train_loss=0.00475, global_step=48.00, consumed_samples=384.0]\n",
      "Epoch 0:  45%|████▌     | 50/111 [00:04<00:05, 11.57it/s, loss=0.104, v_num=, reduced_train_loss=0.00427, global_step=49.00, consumed_samples=392.0]\n",
      "Epoch 0:  46%|████▌     | 51/111 [00:04<00:05, 11.65it/s, loss=0.104, v_num=, reduced_train_loss=0.00427, global_step=49.00, consumed_samples=392.0]\n",
      "Epoch 0:  46%|████▌     | 51/111 [00:04<00:05, 11.65it/s, loss=0.0735, v_num=, reduced_train_loss=0.0238, global_step=50.00, consumed_samples=400.0]\n",
      "Epoch 0:  47%|████▋     | 52/111 [00:04<00:05, 11.72it/s, loss=0.0735, v_num=, reduced_train_loss=0.0238, global_step=50.00, consumed_samples=400.0]\n",
      "Epoch 0:  47%|████▋     | 52/111 [00:04<00:05, 11.72it/s, loss=0.0536, v_num=, reduced_train_loss=0.0226, global_step=51.00, consumed_samples=408.0]\n",
      "Epoch 0:  48%|████▊     | 53/111 [00:04<00:04, 11.82it/s, loss=0.0536, v_num=, reduced_train_loss=0.0226, global_step=51.00, consumed_samples=408.0]\n",
      "Epoch 0:  48%|████▊     | 53/111 [00:04<00:04, 11.82it/s, loss=0.0419, v_num=, reduced_train_loss=0.0215, global_step=52.00, consumed_samples=416.0]\n",
      "Epoch 0:  49%|████▊     | 54/111 [00:04<00:04, 11.93it/s, loss=0.0419, v_num=, reduced_train_loss=0.0215, global_step=52.00, consumed_samples=416.0]\n",
      "Epoch 0:  49%|████▊     | 54/111 [00:04<00:04, 11.93it/s, loss=0.0339, v_num=, reduced_train_loss=0.022, global_step=53.00, consumed_samples=424.0] \n",
      "Epoch 0:  50%|████▉     | 55/111 [00:04<00:04, 12.04it/s, loss=0.0339, v_num=, reduced_train_loss=0.022, global_step=53.00, consumed_samples=424.0]\n",
      "Epoch 0:  50%|████▉     | 55/111 [00:04<00:04, 12.04it/s, loss=0.0304, v_num=, reduced_train_loss=0.0491, global_step=54.00, consumed_samples=432.0]\n",
      "Epoch 0:  50%|█████     | 56/111 [00:04<00:04, 12.07it/s, loss=0.0304, v_num=, reduced_train_loss=0.0491, global_step=54.00, consumed_samples=432.0]\n",
      "Epoch 0:  50%|█████     | 56/111 [00:04<00:04, 12.07it/s, loss=0.0263, v_num=, reduced_train_loss=0.00395, global_step=55.00, consumed_samples=440.0]\n",
      "Epoch 0:  51%|█████▏    | 57/111 [00:04<00:04, 12.16it/s, loss=0.0263, v_num=, reduced_train_loss=0.00395, global_step=55.00, consumed_samples=440.0]\n",
      "Epoch 0:  51%|█████▏    | 57/111 [00:04<00:04, 12.16it/s, loss=0.0225, v_num=, reduced_train_loss=0.00413, global_step=56.00, consumed_samples=448.0]\n",
      "Epoch 0:  52%|█████▏    | 58/111 [00:04<00:04, 12.25it/s, loss=0.0225, v_num=, reduced_train_loss=0.00413, global_step=56.00, consumed_samples=448.0]\n",
      "Epoch 0:  52%|█████▏    | 58/111 [00:04<00:04, 12.25it/s, loss=0.0217, v_num=, reduced_train_loss=0.0349, global_step=57.00, consumed_samples=456.0] \n",
      "Epoch 0:  53%|█████▎    | 59/111 [00:04<00:04, 12.32it/s, loss=0.0217, v_num=, reduced_train_loss=0.0349, global_step=57.00, consumed_samples=456.0]\n",
      "Epoch 0:  53%|█████▎    | 59/111 [00:04<00:04, 12.32it/s, loss=0.0214, v_num=, reduced_train_loss=0.0356, global_step=58.00, consumed_samples=464.0]\n",
      "Epoch 0:  54%|█████▍    | 60/111 [00:04<00:04, 12.40it/s, loss=0.0214, v_num=, reduced_train_loss=0.0356, global_step=58.00, consumed_samples=464.0]\n",
      "Epoch 0:  54%|█████▍    | 60/111 [00:04<00:04, 12.40it/s, loss=0.0217, v_num=, reduced_train_loss=0.0194, global_step=59.00, consumed_samples=472.0]\n",
      "Epoch 0:  55%|█████▍    | 61/111 [00:04<00:04, 12.46it/s, loss=0.0217, v_num=, reduced_train_loss=0.0194, global_step=59.00, consumed_samples=472.0]\n",
      "Epoch 0:  55%|█████▍    | 61/111 [00:04<00:04, 12.46it/s, loss=0.0198, v_num=, reduced_train_loss=0.0199, global_step=60.00, consumed_samples=480.0]\n",
      "Epoch 0:  56%|█████▌    | 62/111 [00:04<00:03, 12.52it/s, loss=0.0198, v_num=, reduced_train_loss=0.0199, global_step=60.00, consumed_samples=480.0]\n",
      "Epoch 0:  56%|█████▌    | 62/111 [00:04<00:03, 12.52it/s, loss=0.0203, v_num=, reduced_train_loss=0.0202, global_step=61.00, consumed_samples=488.0]\n",
      "Epoch 0:  57%|█████▋    | 63/111 [00:05<00:03, 12.56it/s, loss=0.0203, v_num=, reduced_train_loss=0.0202, global_step=61.00, consumed_samples=488.0]\n",
      "Epoch 0:  57%|█████▋    | 63/111 [00:05<00:03, 12.56it/s, loss=0.0204, v_num=, reduced_train_loss=0.0319, global_step=62.00, consumed_samples=496.0]\n",
      "Epoch 0:  58%|█████▊    | 64/111 [00:05<00:03, 12.64it/s, loss=0.0204, v_num=, reduced_train_loss=0.0319, global_step=62.00, consumed_samples=496.0]\n",
      "Epoch 0:  58%|█████▊    | 64/111 [00:05<00:03, 12.64it/s, loss=0.0194, v_num=, reduced_train_loss=0.00546, global_step=63.00, consumed_samples=504.0]\n",
      "Epoch 0:  59%|█████▊    | 65/111 [00:05<00:03, 12.72it/s, loss=0.0194, v_num=, reduced_train_loss=0.00546, global_step=63.00, consumed_samples=504.0]\n",
      "Epoch 0:  59%|█████▊    | 65/111 [00:05<00:03, 12.72it/s, loss=0.0199, v_num=, reduced_train_loss=0.0178, global_step=64.00, consumed_samples=512.0] \n",
      "Epoch 0:  59%|█████▉    | 66/111 [00:05<00:03, 12.77it/s, loss=0.0199, v_num=, reduced_train_loss=0.0178, global_step=64.00, consumed_samples=512.0]\n",
      "Epoch 0:  59%|█████▉    | 66/111 [00:05<00:03, 12.76it/s, loss=0.0205, v_num=, reduced_train_loss=0.018, global_step=65.00, consumed_samples=520.0] \n",
      "Epoch 0:  60%|██████    | 67/111 [00:05<00:03, 12.85it/s, loss=0.0205, v_num=, reduced_train_loss=0.018, global_step=65.00, consumed_samples=520.0]\n",
      "Epoch 0:  60%|██████    | 67/111 [00:05<00:03, 12.85it/s, loss=0.0183, v_num=, reduced_train_loss=0.00246, global_step=66.00, consumed_samples=528.0]\n",
      "Epoch 0:  61%|██████▏   | 68/111 [00:05<00:03, 12.94it/s, loss=0.0183, v_num=, reduced_train_loss=0.00246, global_step=66.00, consumed_samples=528.0]\n",
      "Epoch 0:  61%|██████▏   | 68/111 [00:05<00:03, 12.94it/s, loss=0.0195, v_num=, reduced_train_loss=0.0292, global_step=67.00, consumed_samples=536.0] \n",
      "Epoch 0:  62%|██████▏   | 69/111 [00:05<00:03, 13.02it/s, loss=0.0195, v_num=, reduced_train_loss=0.0292, global_step=67.00, consumed_samples=536.0]\n",
      "Epoch 0:  62%|██████▏   | 69/111 [00:05<00:03, 13.01it/s, loss=0.0211, v_num=, reduced_train_loss=0.0361, global_step=68.00, consumed_samples=544.0]\n",
      "Epoch 0:  63%|██████▎   | 70/111 [00:05<00:03, 13.08it/s, loss=0.0211, v_num=, reduced_train_loss=0.0361, global_step=68.00, consumed_samples=544.0]\n",
      "Epoch 0:  63%|██████▎   | 70/111 [00:05<00:03, 13.08it/s, loss=0.0218, v_num=, reduced_train_loss=0.0176, global_step=69.00, consumed_samples=552.0]\n",
      "Epoch 0:  64%|██████▍   | 71/111 [00:05<00:03, 13.15it/s, loss=0.0218, v_num=, reduced_train_loss=0.0176, global_step=69.00, consumed_samples=552.0]\n",
      "Epoch 0:  64%|██████▍   | 71/111 [00:05<00:03, 13.15it/s, loss=0.0207, v_num=, reduced_train_loss=0.00187, global_step=70.00, consumed_samples=560.0]\n",
      "Epoch 0:  65%|██████▍   | 72/111 [00:05<00:02, 13.21it/s, loss=0.0207, v_num=, reduced_train_loss=0.00187, global_step=70.00, consumed_samples=560.0]\n",
      "Epoch 0:  65%|██████▍   | 72/111 [00:05<00:02, 13.21it/s, loss=0.0212, v_num=, reduced_train_loss=0.0326, global_step=71.00, consumed_samples=568.0] \n",
      "Epoch 0:  66%|██████▌   | 73/111 [00:05<00:02, 13.29it/s, loss=0.0212, v_num=, reduced_train_loss=0.0326, global_step=71.00, consumed_samples=568.0]\n",
      "Epoch 0:  66%|██████▌   | 73/111 [00:05<00:02, 13.29it/s, loss=0.0209, v_num=, reduced_train_loss=0.0162, global_step=72.00, consumed_samples=576.0]\n",
      "Epoch 0:  67%|██████▋   | 74/111 [00:05<00:02, 13.36it/s, loss=0.0209, v_num=, reduced_train_loss=0.0162, global_step=72.00, consumed_samples=576.0]\n",
      "Epoch 0:  67%|██████▋   | 74/111 [00:05<00:02, 13.35it/s, loss=0.0206, v_num=, reduced_train_loss=0.0163, global_step=73.00, consumed_samples=584.0]\n",
      "Epoch 0:  68%|██████▊   | 75/111 [00:05<00:02, 13.42it/s, loss=0.0206, v_num=, reduced_train_loss=0.0163, global_step=73.00, consumed_samples=584.0]\n",
      "Epoch 0:  68%|██████▊   | 75/111 [00:05<00:02, 13.42it/s, loss=0.019, v_num=, reduced_train_loss=0.0169, global_step=74.00, consumed_samples=592.0] \n",
      "Epoch 0:  68%|██████▊   | 76/111 [00:05<00:02, 13.49it/s, loss=0.019, v_num=, reduced_train_loss=0.0169, global_step=74.00, consumed_samples=592.0]\n",
      "Epoch 0:  68%|██████▊   | 76/111 [00:05<00:02, 13.48it/s, loss=0.0192, v_num=, reduced_train_loss=0.00711, global_step=75.00, consumed_samples=600.0]\n",
      "Epoch 0:  69%|██████▉   | 77/111 [00:05<00:02, 13.52it/s, loss=0.0192, v_num=, reduced_train_loss=0.00711, global_step=75.00, consumed_samples=600.0]\n",
      "Epoch 0:  69%|██████▉   | 77/111 [00:05<00:02, 13.51it/s, loss=0.0197, v_num=, reduced_train_loss=0.015, global_step=76.00, consumed_samples=608.0]  \n",
      "Epoch 0:  70%|███████   | 78/111 [00:05<00:02, 13.58it/s, loss=0.0197, v_num=, reduced_train_loss=0.015, global_step=76.00, consumed_samples=608.0]\n",
      "Epoch 0:  70%|███████   | 78/111 [00:05<00:02, 13.58it/s, loss=0.0187, v_num=, reduced_train_loss=0.0147, global_step=77.00, consumed_samples=616.0]\n",
      "Epoch 0:  71%|███████   | 79/111 [00:05<00:02, 13.65it/s, loss=0.0187, v_num=, reduced_train_loss=0.0147, global_step=77.00, consumed_samples=616.0]\n",
      "Epoch 0:  71%|███████   | 79/111 [00:05<00:02, 13.65it/s, loss=0.017, v_num=, reduced_train_loss=0.00149, global_step=78.00, consumed_samples=624.0]\n",
      "Epoch 0:  72%|███████▏  | 80/111 [00:05<00:02, 13.71it/s, loss=0.017, v_num=, reduced_train_loss=0.00149, global_step=78.00, consumed_samples=624.0]\n",
      "Epoch 0:  72%|███████▏  | 80/111 [00:05<00:02, 13.71it/s, loss=0.0169, v_num=, reduced_train_loss=0.0169, global_step=79.00, consumed_samples=632.0]\n",
      "Epoch 0:  73%|███████▎  | 81/111 [00:05<00:02, 13.77it/s, loss=0.0169, v_num=, reduced_train_loss=0.0169, global_step=79.00, consumed_samples=632.0]\n",
      "Epoch 0:  73%|███████▎  | 81/111 [00:05<00:02, 13.77it/s, loss=0.0183, v_num=, reduced_train_loss=0.0474, global_step=80.00, consumed_samples=640.0]\n",
      "Epoch 0:  74%|███████▍  | 82/111 [00:05<00:02, 13.83it/s, loss=0.0183, v_num=, reduced_train_loss=0.0474, global_step=80.00, consumed_samples=640.0]\n",
      "Epoch 0:  74%|███████▍  | 82/111 [00:05<00:02, 13.83it/s, loss=0.0173, v_num=, reduced_train_loss=0.00144, global_step=81.00, consumed_samples=648.0]\n",
      "Epoch 0:  75%|███████▍  | 83/111 [00:05<00:02, 13.85it/s, loss=0.0173, v_num=, reduced_train_loss=0.00144, global_step=81.00, consumed_samples=648.0]\n",
      "Epoch 0:  75%|███████▍  | 83/111 [00:05<00:02, 13.85it/s, loss=0.0168, v_num=, reduced_train_loss=0.0219, global_step=82.00, consumed_samples=656.0] \n",
      "Epoch 0:  76%|███████▌  | 84/111 [00:06<00:01, 13.89it/s, loss=0.0168, v_num=, reduced_train_loss=0.0219, global_step=82.00, consumed_samples=656.0]\n",
      "Epoch 0:  76%|███████▌  | 84/111 [00:06<00:01, 13.89it/s, loss=0.0177, v_num=, reduced_train_loss=0.0229, global_step=83.00, consumed_samples=664.0]\n",
      "Epoch 0:  77%|███████▋  | 85/111 [00:06<00:01, 13.94it/s, loss=0.0177, v_num=, reduced_train_loss=0.0229, global_step=83.00, consumed_samples=664.0]\n",
      "Epoch 0:  77%|███████▋  | 85/111 [00:06<00:01, 13.94it/s, loss=0.0184, v_num=, reduced_train_loss=0.031, global_step=84.00, consumed_samples=672.0] \n",
      "Epoch 0:  77%|███████▋  | 86/111 [00:06<00:01, 13.99it/s, loss=0.0184, v_num=, reduced_train_loss=0.031, global_step=84.00, consumed_samples=672.0]\n",
      "Epoch 0:  77%|███████▋  | 86/111 [00:06<00:01, 13.99it/s, loss=0.0186, v_num=, reduced_train_loss=0.022, global_step=85.00, consumed_samples=680.0]\n",
      "Epoch 0:  78%|███████▊  | 87/111 [00:06<00:01, 14.02it/s, loss=0.0186, v_num=, reduced_train_loss=0.022, global_step=85.00, consumed_samples=680.0]\n",
      "Epoch 0:  78%|███████▊  | 87/111 [00:06<00:01, 14.02it/s, loss=0.0186, v_num=, reduced_train_loss=0.00275, global_step=86.00, consumed_samples=688.0]\n",
      "Epoch 0:  79%|███████▉  | 88/111 [00:06<00:01, 14.06it/s, loss=0.0186, v_num=, reduced_train_loss=0.00275, global_step=86.00, consumed_samples=688.0]\n",
      "Epoch 0:  79%|███████▉  | 88/111 [00:06<00:01, 14.05it/s, loss=0.0174, v_num=, reduced_train_loss=0.00636, global_step=87.00, consumed_samples=696.0]\n",
      "Epoch 0:  80%|████████  | 89/111 [00:06<00:01, 14.10it/s, loss=0.0174, v_num=, reduced_train_loss=0.00636, global_step=87.00, consumed_samples=696.0]\n",
      "Epoch 0:  80%|████████  | 89/111 [00:06<00:01, 14.10it/s, loss=0.0168, v_num=, reduced_train_loss=0.0227, global_step=88.00, consumed_samples=704.0] \n",
      "Epoch 0:  81%|████████  | 90/111 [00:06<00:01, 14.14it/s, loss=0.0168, v_num=, reduced_train_loss=0.0227, global_step=88.00, consumed_samples=704.0]\n",
      "Epoch 0:  81%|████████  | 90/111 [00:06<00:01, 14.14it/s, loss=0.0159, v_num=, reduced_train_loss=0.000591, global_step=89.00, consumed_samples=712.0]\n",
      "Epoch 0:  82%|████████▏ | 91/111 [00:06<00:01, 14.19it/s, loss=0.0159, v_num=, reduced_train_loss=0.000591, global_step=89.00, consumed_samples=712.0]\n",
      "Epoch 0:  82%|████████▏ | 91/111 [00:06<00:01, 14.18it/s, loss=0.0158, v_num=, reduced_train_loss=0.000612, global_step=90.00, consumed_samples=720.0]\n",
      "Epoch 0:  83%|████████▎ | 92/111 [00:06<00:01, 14.23it/s, loss=0.0158, v_num=, reduced_train_loss=0.000612, global_step=90.00, consumed_samples=720.0]\n",
      "Epoch 0:  83%|████████▎ | 92/111 [00:06<00:01, 14.23it/s, loss=0.0147, v_num=, reduced_train_loss=0.0107, global_step=91.00, consumed_samples=728.0]  \n",
      "Epoch 0:  84%|████████▍ | 93/111 [00:06<00:01, 14.28it/s, loss=0.0147, v_num=, reduced_train_loss=0.0107, global_step=91.00, consumed_samples=728.0]\n",
      "Epoch 0:  84%|████████▍ | 93/111 [00:06<00:01, 14.27it/s, loss=0.014, v_num=, reduced_train_loss=0.000628, global_step=92.00, consumed_samples=736.0]\n",
      "Epoch 0:  85%|████████▍ | 94/111 [00:06<00:01, 14.32it/s, loss=0.014, v_num=, reduced_train_loss=0.000628, global_step=92.00, consumed_samples=736.0]\n",
      "Epoch 0:  85%|████████▍ | 94/111 [00:06<00:01, 14.32it/s, loss=0.0132, v_num=, reduced_train_loss=0.000621, global_step=93.00, consumed_samples=744.0]\n",
      "Epoch 0:  86%|████████▌ | 95/111 [00:06<00:01, 14.36it/s, loss=0.0132, v_num=, reduced_train_loss=0.000621, global_step=93.00, consumed_samples=744.0]\n",
      "Epoch 0:  86%|████████▌ | 95/111 [00:06<00:01, 14.36it/s, loss=0.0124, v_num=, reduced_train_loss=0.000607, global_step=94.00, consumed_samples=752.0]\n",
      "Epoch 0:  86%|████████▋ | 96/111 [00:06<00:01, 14.40it/s, loss=0.0124, v_num=, reduced_train_loss=0.000607, global_step=94.00, consumed_samples=752.0]\n",
      "Epoch 0:  86%|████████▋ | 96/111 [00:06<00:01, 14.40it/s, loss=0.012, v_num=, reduced_train_loss=0.000597, global_step=95.00, consumed_samples=760.0] \n",
      "Epoch 0:  87%|████████▋ | 97/111 [00:06<00:00, 14.44it/s, loss=0.012, v_num=, reduced_train_loss=0.000597, global_step=95.00, consumed_samples=760.0]\n",
      "Epoch 0:  87%|████████▋ | 97/111 [00:06<00:00, 14.44it/s, loss=0.0113, v_num=, reduced_train_loss=0.000654, global_step=96.00, consumed_samples=768.0]\n",
      "Epoch 0:  88%|████████▊ | 98/111 [00:06<00:00, 14.48it/s, loss=0.0113, v_num=, reduced_train_loss=0.000654, global_step=96.00, consumed_samples=768.0]\n",
      "Epoch 0:  88%|████████▊ | 98/111 [00:06<00:00, 14.48it/s, loss=0.0106, v_num=, reduced_train_loss=0.000605, global_step=97.00, consumed_samples=776.0]\n",
      "Epoch 0:  89%|████████▉ | 99/111 [00:06<00:00, 14.52it/s, loss=0.0106, v_num=, reduced_train_loss=0.000605, global_step=97.00, consumed_samples=776.0]\n",
      "Epoch 0:  89%|████████▉ | 99/111 [00:06<00:00, 14.52it/s, loss=0.0106, v_num=, reduced_train_loss=0.000655, global_step=98.00, consumed_samples=784.0]\n",
      "Epoch 0:  90%|█████████ | 100/111 [00:06<00:00, 14.56it/s, loss=0.0106, v_num=, reduced_train_loss=0.000655, global_step=98.00, consumed_samples=784.0]\n",
      "Epoch 0:  90%|█████████ | 100/111 [00:06<00:00, 14.56it/s, loss=0.00978, v_num=, reduced_train_loss=0.000861, global_step=99.00, consumed_samples=792.0]\n",
      "\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "\n",
      "Validation:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:00, 11.68it/s]\u001b[A\n",
      "Epoch 0:  91%|█████████ | 101/111 [00:06<00:00, 14.48it/s, loss=0.00978, v_num=, reduced_train_loss=0.000861, global_step=99.00, consumed_samples=792.0]\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:00, 17.98it/s]\u001b[A\n",
      "Epoch 0:  92%|█████████▏| 102/111 [00:06<00:00, 14.57it/s, loss=0.00978, v_num=, reduced_train_loss=0.000861, global_step=99.00, consumed_samples=792.0]\n",
      "\n",
      "Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:00, 24.02it/s]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 103/111 [00:07<00:00, 14.69it/s, loss=0.00978, v_num=, reduced_train_loss=0.000861, global_step=99.00, consumed_samples=792.0]\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 4/10 [00:00<00:00, 28.93it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▎| 104/111 [00:07<00:00, 14.80it/s, loss=0.00978, v_num=, reduced_train_loss=0.000861, global_step=99.00, consumed_samples=792.0]\n",
      "\n",
      "Validation DataLoader 0:  50%|█████     | 5/10 [00:00<00:00, 33.06it/s]\u001b[A\n",
      "Epoch 0:  95%|█████████▍| 105/111 [00:07<00:00, 14.92it/s, loss=0.00978, v_num=, reduced_train_loss=0.000861, global_step=99.00, consumed_samples=792.0]\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 6/10 [00:00<00:00, 32.88it/s]\u001b[A\n",
      "Epoch 0:  95%|█████████▌| 106/111 [00:07<00:00, 14.99it/s, loss=0.00978, v_num=, reduced_train_loss=0.000861, global_step=99.00, consumed_samples=792.0]\n",
      "\n",
      "Validation DataLoader 0:  70%|███████   | 7/10 [00:00<00:00, 34.25it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▋| 107/111 [00:07<00:00, 15.09it/s, loss=0.00978, v_num=, reduced_train_loss=0.000861, global_step=99.00, consumed_samples=792.0]\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 8/10 [00:00<00:00, 36.78it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 108/111 [00:07<00:00, 15.20it/s, loss=0.00978, v_num=, reduced_train_loss=0.000861, global_step=99.00, consumed_samples=792.0]\n",
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 9/10 [00:00<00:00, 39.04it/s]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 109/111 [00:07<00:00, 15.31it/s, loss=0.00978, v_num=, reduced_train_loss=0.000861, global_step=99.00, consumed_samples=792.0]\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 10/10 [00:00<00:00, 41.05it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 110/111 [00:07<00:00, 15.43it/s, loss=0.00978, v_num=, reduced_train_loss=0.000861, global_step=99.00, consumed_samples=792.0]\n",
      "Epoch 0:  99%|█████████▉| 110/111 [00:07<00:00, 15.42it/s, loss=0.00978, v_num=, reduced_train_loss=0.000861, global_step=99.00, consumed_samples=792.0, val_loss=0.000501]\n",
      "\n",
      "                                                                        \u001b[AEpoch 0, global step 100: 'val_loss' reached 0.00050 (best 0.00050), saving model to '/result/nemo_experiments/esm1nv-oas/esm1nv-oas_pretraining/checkpoints/megatron_bert--val_loss=0.00-step=100-consumed_samples=800.0-v11.ckpt' as top 10\n",
      "\n",
      "Epoch 0: 100%|██████████| 111/111 [00:08<00:00, 13.07it/s, loss=0.00978, v_num=, reduced_train_loss=0.000861, global_step=99.00, consumed_samples=792.0, val_loss=0.000501]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:08<00:00, 13.07it/s, loss=0.00745, v_num=, reduced_train_loss=0.000713, global_step=100.0, consumed_samples=800.0, val_loss=0.000501]\n",
      "Epoch 0: 100%|██████████| 111/111 [00:08<00:00, 13.07it/s, loss=0.00745, v_num=, reduced_train_loss=0.000713, global_step=100.0, consumed_samples=800.0, val_loss=0.000501]`Trainer.fit` stopped: `max_steps=101` reached.\n",
      "\n",
      "Epoch 0: 100%|██████████| 111/111 [00:08<00:00, 13.07it/s, loss=0.00745, v_num=, reduced_train_loss=0.000713, global_step=100.0, consumed_samples=800.0, val_loss=0.000501]\n",
      "[NeMo I 2023-08-25 18:46:57 nlp_overrides:226] Removing checkpoint: /result/nemo_experiments/esm1nv-oas/esm1nv-oas_pretraining/checkpoints/megatron_bert--val_loss=0.00-step=100-consumed_samples=800.0-last.ckpt\n",
      "[NeMo I 2023-08-25 18:46:58 pretrain_oas:24] ************** Finished Training ***********\n"
     ]
    }
   ],
   "source": [
    "std_out = ! cd {BIONEMO_WORKSPACE}/examples/protein/esm1nv && python pretrain_oas.py ++trainer.max_steps=101\n",
    "print('\\n'.join(std_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd63b80f",
   "metadata": {},
   "source": [
    "## Creating the Dataset object\n",
    "\n",
    "Underneath the abstractions we provide, ultimately the dataset is constructed by invoking the relevant NeMo object, specified with `model.data.data_impl` in the config file. Additionally we provide the requisite keyword arguments, specified with `model.data.data_impl_kwargs` field. Look around in NeMo for additional dataset types, or implement your own!\n",
    "\n",
    "We can do this manually as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6877acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-08-25 18:47:09 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-08-25 18:47:10 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-08-25 18:47:10 text_memmap_dataset:104] Building data files\n",
      "[NeMo I 2023-08-25 18:47:10 text_memmap_dataset:343] Processing 3 data files using 1 workers\n",
      "[NeMo I 2023-08-25 18:47:10 text_memmap_dataset:349] Time building 0 / 3 mem-mapped files: 0:00:00.051294\n",
      "[NeMo I 2023-08-25 18:47:10 text_memmap_dataset:114] Loading data files\n",
      "[NeMo I 2023-08-25 18:47:10 text_memmap_dataset:205] Loading /data/OASpaired/processed/heavy/train/x000.csv\n",
      "[NeMo I 2023-08-25 18:47:10 text_memmap_dataset:205] Loading /data/OASpaired/processed/heavy/train/x001.csv\n",
      "[NeMo I 2023-08-25 18:47:10 text_memmap_dataset:205] Loading /data/OASpaired/processed/heavy/train/x002.csv\n",
      "[NeMo I 2023-08-25 18:47:10 text_memmap_dataset:117] Time loading 3 mem-mapped files: 0:00:00.005260\n",
      "[NeMo I 2023-08-25 18:47:10 text_memmap_dataset:121] Computing global indices\n",
      "GGGAGAGGAGGCCTGTCCTGGATTCGATTCCCAGTTCCTCACATTCAGTCAGCACTGAACACGGACCCCTCACCATGAACTTCGGGCTCAGCTTGATTTTCCTTGTCCTTGTTTTAAAAGGTGTCCAGTGTGAAGTGATGCTGGTGGAGTCTGGGGGAGGCTTAGTGAAGCCTGGAGGGTCCCTGAAACTCTCCTGTGCAGCCTCTGGATTCACTTTCAGTAGCTATGCCATGTCTTGGGTTCGCCAGACTCCGGAGAAGAGGCTGGAGTGGGTCGCAACCATTAGTAGTGGTGGTAGTTACACCTACTATCCAGACAGTGTGAAGGGGCGATTCACCATCTCCAGAGACAATGCCAAGAACACCCTGTACCTGCAAATGAGCAGTCTGAGGTCTGAGGACACGGCCATGTATTACTGTGCAAGACGGGGGAATGATGGTTACTACGAAGACTACTGGGGCCAAGGCACCACTCTCACAGTCTCCTCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG\n",
      "GAGCTCTGACAGAGGAGGCCAGTCCTGGAATTGATTCCCAGTTCCTCACGTTCAGTGATGAGCACTGAACACAGACACCTCACCATGAACTTTGGGCTCAGATTGATTTTCCTTGTCCTTACTTTAAAAGGTGTGAAGTGTGAAGTGCAGCTGGTGGAGTCTGGGGGAGGCTTAGTGAAGCCTGGAGGGTCCCTGAAACTCTCCTGTGCAGCCTCTGGATTCGCTTTCAGTAGCTATGACATGTCTTGGGTTCGCCAGACTCCGGAGAAGAGGCTGGAGTGGGTCGCATACATTAGTAGTGGTGGTGGTATCACCTACTATCCAGACACTGTGAAGGGCCGATTCACCATCTCCAGAGACAATGCCAAGAACACCCTGTACCTGCAAATGAGCAGTCTGAAGTCTGAGGACACAGCCATGTATTACTGTGCAAGGCCCCCGGGACGGGGCTACTGGTACTTCGATGTCTGGGGCGCAGGGACCACGGTCACCGTCTCCTCAGCCAAAACAACAGCCCCATCGGTCTATCCACTGGCCCCTGTGTGTGGAGATACAACTGGCTCCTCGGTGACTCTAGGGTGCCTGGTCAAGGATTATT\n",
      "AACATATGTCCAATGTCCTCTCCACAGACACTGAACACACTGACTCTAACCATGGGATGGAGCTGGATCTTTCTCTTCCTCCTGTCAGGAACTGCAGGCGTCCACTCTGAGGTCCAGCTTCAGCAGTCAGGACCTGAGCTGGTGAAACCTGGGGCCTCAGTGAAGATATCCTGCAAGGCTTCTGGATACACATTCACTGACTACAACATGCACTGGGTGAAGCAGAGCCATGGAAAGAGCCTTGAGTGGATTGGATATATTTATCCTTACAATGGTGGTACTGGCTACAACCAGAAGTTCAAGAGCAAGGCCACATTGACTGTAGACAATTCCTCCAGCACAGCCTACATGGAGCTCCGCAGCCTGACATCTGAGGACTCTGCAGTCTATTACTGTGCAAGATGGGGGCTAACTGGTGATGCTATGGACTACTGGGGTCAAGGAACCTCAGTCACCGTCTCCTCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG\n",
      "GACATAACAGCAAGAGAGTGTCCGGTTAGTCTCAAGGAAGACTGAGACACAGTCTTAGATATCATGGAATGGCTGTGGAACTTGCTATTTCTCATGGCAGCAGCTCAAAGTATCCAAGCACAGATCCAGTTGGTGCAGTCTGGACCTGAGCTGAAGAAGCCTGGAGAGACAGTCAGGATCTCCTGCAAGGCTTCTGGGTATACCTTCACAACTGCTGGAATGCAGTGGGTGCAAAAGATGCCAGGAAAGGGTTTGAAGTGGATTGGCTGGATAAACACCCACTCTGGAGTGCCAAAATATGCAGAAGACTTCAAGGGACGGTTTGCCTTCTCTTTGGAAACCTCTGCCAGCACTGCATATTTACAGATAAGCAACCTCAAAAATGAGGACACGGCTACGTATTTCTGTGCGAGATCAGGTTACGACGCCTTTGACTACTGGGGCCAAGGCACCACTCTCACAGTCTCCTCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG\n",
      "GGGGAGCATATGATCAGTGTCCTCTCCAAAGTCCTTGAACATAGACTCTAACCATGGAATGGACCTGGGTCTTTCTCTTCCTCCTGTCAGTAACTGCAGGTGTCCACTCCCAGGTTCAGCTGCAGCAGTCTGGAGTTGAGCTGATGAAGCCTGGGGCCTCAGTGAAGATATCCTGCAAGGCTACTGGCTACACACTCAGTAACTACTGGATAGAGTGGGTAAAGCAGAGGCCTGGACATGGCCTTGAGTGGATTGGAGAGATTTTACCTGGAGATGTTATTACTAACTACAATGAGAGGTTCAAGGACAAGGCCACATTCACTGCAGATACATCCTCCAACACAGCCTACATGCAACTCAGCAGCCTGACATCTGAGGATTCTGCCGTCTATTACTGTGCAAGAAGGGTTATTAAGGGGGGGTTTGCTTACTGGGGCCAAGGGACTCTGGTCACTGTCTCTGCAGCCAAAACAACAGCCCCATCGGTCTATCCACTGGCCCCTGTGTGTGGAGATACAACTGGCTCCTCGGTGACTCTAGGATGCCTGGTCAAGG\n",
      "ACATCGCTCTCACTGGAGGCTGATCTCTGAAGATAAGGAGGTGTAGCCTAAAAGATGAGAGTGCTGATTCTTTTGTGGCTGTTCACAGCCTTTCCTGGTATCCTGTCTGATGTGCAGCTTCAGGAGTCGGGACCTGGCCTGGTGAAACCTTCTCAGTCTCTGTCCCTCACCTGCACTGTCACTGGCTACTCAATCACCAGTGATTATGCCTGGAACTGGATCCGGCAGTTTCCAGGAAACAAACTGGAGTGGATGGGCTACATAAGCTACAGTGGTAGCACTAGCTACAACCCATCTCTCAAAAGTCGAATCTCTATCACTCGAGACACATCCAAGAACCAGTTCTTCCTGCAGTTGAATTCTGTGACTACTGAGGACACAGCCACATATTACTGTGCAAGAAGGTACTTCGATGTCTGGGGCGCAGGGACCACGGTCACCGTCTCCTCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG\n",
      "ATCGCTCTCACTGGAGGCTGATCTCTGAAGATAAGGAGGTGTAGCCTAAAAGATGAGAGTGCTGATTCTTTTGTGGCTGTTCACAGCCTTTCCTGGTATCCTGTCTGATGTGCAGCTTCAGGAGTCGGGACCTGGCCTGGTGAAACCTTCTCAGTCTCTGTCCCTCACCTGCACTGTCACTGGCTACTCAATCACCAGTGATTATGCCTGGAACTGGATCCGGCAGTTTCCAGGAAACAAACTGGAGTGGATGGGCTACATAAGCTACAGTGGTAGCACTAGCTACAACCCATCTCTCAAAAGTCGAATCTCTATCACTCGAGACACATCCAAGAACCAGTTCTTCCTGCAGTTGAATTCTGTGACTACTGAGGACACAGCCACATATTACTGTGCAAGGAGGTACTTCGATGTCTGGGGCGCAGGGACCACGGTCACCGTCTCCTCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG\n",
      "GGGGAAAAACATGAGATCACAGTTCTCTCTACAGTTACTGAGCACACAGGAACTCACCATGGGATGGAGCTATATCATCCTCTTTTTGGTAGCAACAGCTACAGGTGTCCACTCCCAGGTCCAACTGCAGCAGCCTGGGGCTGAACTGGTGAAGCCTGGGGCTTCAGTGAAGTTGTCCTGCAAGGCTTCTGGCTACACCTTCACCAGCTACTATATGTACTGGGTGAAGCAGAGGCCTGGACAAGGCCTTGAGTGGATTGGGGGGATTAATCCTAGCAATGGTGGTACTAACTTCAATGAGAAGTTCAAGAGCAAGGCCACACTGACTGTAGACAAATCCTCCAGCACAGCCTACATGCAACTCAGCAGCCTGACATCTGAGGACTCTGCGGTCTATTACTGTACAAGATACGGCCTCTATGCTATGGACTACTGGGGTCAAGGAACCTCAGTCACCGTCTCCTCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG\n",
      "ACTAGTGTGCAGATATGGACAGGCTTACTTCCTCATTGCTGCTGCTGATTGTCCCTGCATATGTCCTGTCCCAGGTTACTCTGAAAGAGTCTGGCCCTGGGATATTGCAGCCCTCCCAGACCCTCAGTCTGACTTGTTCTTTCTCTGGGTTTTCACTGACCACTTCTGGTATGGGTGTGACCTGGATTCGTCAGCCTTCAGGAAAGGGTCTGGAGTGGCTGGCACACATTTACTGGGATAATGACAAGCGCTATAATACATCCCTGAAGAGCCGGCTCACAATCTCCAAGGATACCTCCAGCAACCGGGTATTCCTCAAGATAACCAGTGTGGACACTGCAGATACTGCCACATACTACTGTACTCGAGTTACTACGTTGGTGGGCTACTTTGACCAATGGGGCCAAGGCACCACTCTCACAGTCTCCTCAGCCAAAACGACACCCCCATCTGTCTATCCACTGGCCCCTGGATCTGCTGCCCAAACTAACTCCATGGTGACCCTGGGATGCCTGGTCAAGGG\n",
      "AACACCACCAACAACGACATCGACAATCATTCCCTACACAAAGCTCTTCCGATCTAAACGGGAGAATAGAGTCAATGATTTATTCTTATATGAGGAGAAAAACATGAGATCACAGTTCTCTCTACAGTTACTGAGCACACAGGACCTCACCATGGGATGGAGCTATATCATTTTCTTTTTGGTAGCAACAGCTACAGGTGTCCACTCCCAGGTCCAACTCCAGCAGCCTGGGGCTGAACTGGTGAAGCCTGGGGCTTCAGTGAAGTTGTCCTGCAAGGCTTCTGGCTACACCTTCACCAGCTACTGGATGCACTGGGTGAAGCTGAGGCCTGGACAAGGCTTTGAGTGGATTGGAGAGATTAATCCTAGCAATGGTGGTACTAACTACAATGAGAAGTTCAAGAGAAAGGCCACACTGACTGTAGACAAATCCTCCAGCACAGCCTACATGCAACTCAGCAGCCTGACATCTGAGGACTCTGCGGTCTATTACTGTACAATACGGAATTACTACGGTAGTAGCTACGAGGACTACTGGGGCCAAGGCACCACTCTCACAGTCTCCTCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG\n",
      "GTCTATGGCAGTTCCTATCTCTCTCACTGGAGGCTGATTTTTGAAGAAAGGGGTTGTAGCCTAAAAGATGATGGTGTTAAGTCTTCTGTACCTGTTGACAGCCCTTCCGGGTATCCTGTCAGAGGTGCAGCTTCAGGAGTCAGGACCTAGCCTCGTGAAACCTTCTCAGACTCTGTCCCTCACCTGTTCTGTCACTGGCGACTCCATCACCAGTGGTTACTGGAACTGGATCCGGAAATTCCCAGGGAATAAACTTGAGTACATGGGGTACATAAGCTACAGTGGTAGCACTTACTACAATCCATCTCTCAAAAGTCGAATCTCCATCACTCGAGACACATCCAAGAACCAGTACTACCTGCAGTTGAATTCTGTGACTACTGAGGACACAGCCACATATTACTGTGCAAGATGGGACTATGACTGGGGTCAAGGAACCTCAGTCACCGTCTCCTCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG\n"
     ]
    }
   ],
   "source": [
    "dataset_paths = [ \n",
    "    '/data/OASpaired/processed/heavy/train/x000.csv' ,\n",
    "    '/data/OASpaired/processed/heavy/train/x001.csv' ,\n",
    "    '/data/OASpaired/processed/heavy/train/x002.csv' ,\n",
    "]\n",
    "# Checkout nemo for examples of other dataset types, or add your own!\n",
    "from nemo.collections.nlp.data.language_modeling.text_memmap_dataset import CSVMemMapDataset\n",
    "# The kwargs here are taken from our yaml file.\n",
    "dataset = CSVMemMapDataset(dataset_paths=dataset_paths, header_lines=1, newline_int=10, workers=1, sort_dataset_paths=True, data_sep=',', data_col=1)\n",
    "\n",
    "for i, item in enumerate(iter(dataset)):\n",
    "    if i > 10: break\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86def78",
   "metadata": {},
   "source": [
    "## Testing our new collate function\n",
    "\n",
    "Before we inject our collate function into a dataloader, lets first take a look at what it actually does. As we saw previously, it simply replaces every character with 'A', this should be visually obvious! To do this, we must also include a tokenizer. This is required by the default language modeling collate function, which tokenizes the input, applies padding, and aligns it for distributed training. We call `collate_fn` with some dummy data and then watch the output transformed to `AAAA..`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dedaf1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-08-25 18:47:10 tokenizer_utils:191] Getting SentencePiece with model: /tokenizers/protein/esm1nv/vocab/protein_sequence_sentencepiece.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': tensor([[1, 6, 6, 6, 6, 6, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "         [1, 6, 6, 4, 6, 6, 6, 6, 6, 2, 3, 3, 3, 3, 3, 3]]),\n",
       " 'types': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'is_random': tensor([0, 1]),\n",
       " 'loss_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'labels': tensor([[1, 6, 6, 6, 6, 6, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "         [1, 6, 6, 6, 6, 6, 6, 6, 6, 2, 3, 3, 3, 3, 3, 3]]),\n",
       " 'padding_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]),\n",
       " 'batch': ['AAAAA', 'AAAAAAAA']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bionemo.data.dataloader.custom_protein_collate import CustomProteinBertCollate\n",
    "\n",
    "# Some magic to get our NeMo tokenizer, filled with arguments from our config file.\n",
    "from nemo.collections.nlp.modules.common.tokenizer_utils import get_nmt_tokenizer\n",
    "tokenizer = get_nmt_tokenizer(\n",
    "            library='sentencepiece',\n",
    "            tokenizer_model= '/tokenizers/protein/esm1nv/vocab/protein_sequence_sentencepiece.model',\n",
    "            vocab_file='/tokenizers/vocab/protein_sequence_sentencepiece.vocab',\n",
    "            legacy=False,\n",
    ")\n",
    "\n",
    "# Extra kwargs are again taken from our config file.\n",
    "collate_fn = CustomProteinBertCollate(tokenizer=tokenizer,\n",
    "                                                    seq_length=512,\n",
    "                                                    pad_size_divisible_by_8=True,\n",
    "                                                    modify_percent=.1, # Fraction of tokens to mask or perturb\n",
    "                                                    perturb_percent=.5, # Fraction of modified tokens to perturb, 1-perturb_percent is masking probability\n",
    "                                                    ).collate_fn\n",
    "collate_fn(['ACTGT', 'ADFASDFA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d247880d",
   "metadata": {},
   "source": [
    "# DataLoader!\n",
    "Lastly, we must construct a dataloader composed of our collate function and our dataset object. From here, we can iterate over the reuslt and ensure it changed the data in the same way as manually calling the collate function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0651efb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "['GGGAGAGGAGGCCTGTCCTGGATTCGATTCCCAGTTCCTCACATTCAGTCAGCACTGAACACGGACCCCTCACCATGAACTTCGGGCTCAGCTTGATTTTCCTTGTCCTTGTTTTAAAAGGTGTCCAGTGTGAAGTGATGCTGGTGGAGTCTGGGGGAGGCTTAGTGAAGCCTGGAGGGTCCCTGAAACTCTCCTGTGCAGCCTCTGGATTCACTTTCAGTAGCTATGCCATGTCTTGGGTTCGCCAGACTCCGGAGAAGAGGCTGGAGTGGGTCGCAACCATTAGTAGTGGTGGTAGTTACACCTACTATCCAGACAGTGTGAAGGGGCGATTCACCATCTCCAGAGACAATGCCAAGAACACCCTGTACCTGCAAATGAGCAGTCTGAGGTCTGAGGACACGGCCATGTATTACTGTGCAAGACGGGGGAATGATGGTTACTACGAAGACTACTGGGGCCAAGGCACCACTCTCACAGTCTCCTCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG', 'GAGCTCTGACAGAGGAGGCCAGTCCTGGAATTGATTCCCAGTTCCTCACGTTCAGTGATGAGCACTGAACACAGACACCTCACCATGAACTTTGGGCTCAGATTGATTTTCCTTGTCCTTACTTTAAAAGGTGTGAAGTGTGAAGTGCAGCTGGTGGAGTCTGGGGGAGGCTTAGTGAAGCCTGGAGGGTCCCTGAAACTCTCCTGTGCAGCCTCTGGATTCGCTTTCAGTAGCTATGACATGTCTTGGGTTCGCCAGACTCCGGAGAAGAGGCTGGAGTGGGTCGCATACATTAGTAGTGGTGGTGGTATCACCTACTATCCAGACACTGTGAAGGGCCGATTCACCATCTCCAGAGACAATGCCAAGAACACCCTGTACCTGCAAATGAGCAGTCTGAAGTCTGAGGACACAGCCATGTATTACTGTGCAAGGCCCCCGGGACGGGGCTACTGGTACTTCGATGTCTGGGGCGCAGGGACCACGGTCACCGTCTCCTCAGCCAAAACAACAGCCCCATCGGTCTATCCACTGGCCCCTGTGTGTGGAGATACAACTGGCTCCTCGGTGACTCTAGGGTGCCTGGTCAAGGATTATT']\n",
      "['AACATATGTCCAATGTCCTCTCCACAGACACTGAACACACTGACTCTAACCATGGGATGGAGCTGGATCTTTCTCTTCCTCCTGTCAGGAACTGCAGGCGTCCACTCTGAGGTCCAGCTTCAGCAGTCAGGACCTGAGCTGGTGAAACCTGGGGCCTCAGTGAAGATATCCTGCAAGGCTTCTGGATACACATTCACTGACTACAACATGCACTGGGTGAAGCAGAGCCATGGAAAGAGCCTTGAGTGGATTGGATATATTTATCCTTACAATGGTGGTACTGGCTACAACCAGAAGTTCAAGAGCAAGGCCACATTGACTGTAGACAATTCCTCCAGCACAGCCTACATGGAGCTCCGCAGCCTGACATCTGAGGACTCTGCAGTCTATTACTGTGCAAGATGGGGGCTAACTGGTGATGCTATGGACTACTGGGGTCAAGGAACCTCAGTCACCGTCTCCTCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG', 'GACATAACAGCAAGAGAGTGTCCGGTTAGTCTCAAGGAAGACTGAGACACAGTCTTAGATATCATGGAATGGCTGTGGAACTTGCTATTTCTCATGGCAGCAGCTCAAAGTATCCAAGCACAGATCCAGTTGGTGCAGTCTGGACCTGAGCTGAAGAAGCCTGGAGAGACAGTCAGGATCTCCTGCAAGGCTTCTGGGTATACCTTCACAACTGCTGGAATGCAGTGGGTGCAAAAGATGCCAGGAAAGGGTTTGAAGTGGATTGGCTGGATAAACACCCACTCTGGAGTGCCAAAATATGCAGAAGACTTCAAGGGACGGTTTGCCTTCTCTTTGGAAACCTCTGCCAGCACTGCATATTTACAGATAAGCAACCTCAAAAATGAGGACACGGCTACGTATTTCTGTGCGAGATCAGGTTACGACGCCTTTGACTACTGGGGCCAAGGCACCACTCTCACAGTCTCCTCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG']\n",
      "['GGGGAGCATATGATCAGTGTCCTCTCCAAAGTCCTTGAACATAGACTCTAACCATGGAATGGACCTGGGTCTTTCTCTTCCTCCTGTCAGTAACTGCAGGTGTCCACTCCCAGGTTCAGCTGCAGCAGTCTGGAGTTGAGCTGATGAAGCCTGGGGCCTCAGTGAAGATATCCTGCAAGGCTACTGGCTACACACTCAGTAACTACTGGATAGAGTGGGTAAAGCAGAGGCCTGGACATGGCCTTGAGTGGATTGGAGAGATTTTACCTGGAGATGTTATTACTAACTACAATGAGAGGTTCAAGGACAAGGCCACATTCACTGCAGATACATCCTCCAACACAGCCTACATGCAACTCAGCAGCCTGACATCTGAGGATTCTGCCGTCTATTACTGTGCAAGAAGGGTTATTAAGGGGGGGTTTGCTTACTGGGGCCAAGGGACTCTGGTCACTGTCTCTGCAGCCAAAACAACAGCCCCATCGGTCTATCCACTGGCCCCTGTGTGTGGAGATACAACTGGCTCCTCGGTGACTCTAGGATGCCTGGTCAAGG', 'ACATCGCTCTCACTGGAGGCTGATCTCTGAAGATAAGGAGGTGTAGCCTAAAAGATGAGAGTGCTGATTCTTTTGTGGCTGTTCACAGCCTTTCCTGGTATCCTGTCTGATGTGCAGCTTCAGGAGTCGGGACCTGGCCTGGTGAAACCTTCTCAGTCTCTGTCCCTCACCTGCACTGTCACTGGCTACTCAATCACCAGTGATTATGCCTGGAACTGGATCCGGCAGTTTCCAGGAAACAAACTGGAGTGGATGGGCTACATAAGCTACAGTGGTAGCACTAGCTACAACCCATCTCTCAAAAGTCGAATCTCTATCACTCGAGACACATCCAAGAACCAGTTCTTCCTGCAGTTGAATTCTGTGACTACTGAGGACACAGCCACATATTACTGTGCAAGAAGGTACTTCGATGTCTGGGGCGCAGGGACCACGGTCACCGTCTCCTCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG']\n",
      "['ATCGCTCTCACTGGAGGCTGATCTCTGAAGATAAGGAGGTGTAGCCTAAAAGATGAGAGTGCTGATTCTTTTGTGGCTGTTCACAGCCTTTCCTGGTATCCTGTCTGATGTGCAGCTTCAGGAGTCGGGACCTGGCCTGGTGAAACCTTCTCAGTCTCTGTCCCTCACCTGCACTGTCACTGGCTACTCAATCACCAGTGATTATGCCTGGAACTGGATCCGGCAGTTTCCAGGAAACAAACTGGAGTGGATGGGCTACATAAGCTACAGTGGTAGCACTAGCTACAACCCATCTCTCAAAAGTCGAATCTCTATCACTCGAGACACATCCAAGAACCAGTTCTTCCTGCAGTTGAATTCTGTGACTACTGAGGACACAGCCACATATTACTGTGCAAGGAGGTACTTCGATGTCTGGGGCGCAGGGACCACGGTCACCGTCTCCTCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG', 'GGGGAAAAACATGAGATCACAGTTCTCTCTACAGTTACTGAGCACACAGGAACTCACCATGGGATGGAGCTATATCATCCTCTTTTTGGTAGCAACAGCTACAGGTGTCCACTCCCAGGTCCAACTGCAGCAGCCTGGGGCTGAACTGGTGAAGCCTGGGGCTTCAGTGAAGTTGTCCTGCAAGGCTTCTGGCTACACCTTCACCAGCTACTATATGTACTGGGTGAAGCAGAGGCCTGGACAAGGCCTTGAGTGGATTGGGGGGATTAATCCTAGCAATGGTGGTACTAACTTCAATGAGAAGTTCAAGAGCAAGGCCACACTGACTGTAGACAAATCCTCCAGCACAGCCTACATGCAACTCAGCAGCCTGACATCTGAGGACTCTGCGGTCTATTACTGTACAAGATACGGCCTCTATGCTATGGACTACTGGGGTCAAGGAACCTCAGTCACCGTCTCCTCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG']\n",
      "['ACTAGTGTGCAGATATGGACAGGCTTACTTCCTCATTGCTGCTGCTGATTGTCCCTGCATATGTCCTGTCCCAGGTTACTCTGAAAGAGTCTGGCCCTGGGATATTGCAGCCCTCCCAGACCCTCAGTCTGACTTGTTCTTTCTCTGGGTTTTCACTGACCACTTCTGGTATGGGTGTGACCTGGATTCGTCAGCCTTCAGGAAAGGGTCTGGAGTGGCTGGCACACATTTACTGGGATAATGACAAGCGCTATAATACATCCCTGAAGAGCCGGCTCACAATCTCCAAGGATACCTCCAGCAACCGGGTATTCCTCAAGATAACCAGTGTGGACACTGCAGATACTGCCACATACTACTGTACTCGAGTTACTACGTTGGTGGGCTACTTTGACCAATGGGGCCAAGGCACCACTCTCACAGTCTCCTCAGCCAAAACGACACCCCCATCTGTCTATCCACTGGCCCCTGGATCTGCTGCCCAAACTAACTCCATGGTGACCCTGGGATGCCTGGTCAAGGG', 'AACACCACCAACAACGACATCGACAATCATTCCCTACACAAAGCTCTTCCGATCTAAACGGGAGAATAGAGTCAATGATTTATTCTTATATGAGGAGAAAAACATGAGATCACAGTTCTCTCTACAGTTACTGAGCACACAGGACCTCACCATGGGATGGAGCTATATCATTTTCTTTTTGGTAGCAACAGCTACAGGTGTCCACTCCCAGGTCCAACTCCAGCAGCCTGGGGCTGAACTGGTGAAGCCTGGGGCTTCAGTGAAGTTGTCCTGCAAGGCTTCTGGCTACACCTTCACCAGCTACTGGATGCACTGGGTGAAGCTGAGGCCTGGACAAGGCTTTGAGTGGATTGGAGAGATTAATCCTAGCAATGGTGGTACTAACTACAATGAGAAGTTCAAGAGAAAGGCCACACTGACTGTAGACAAATCCTCCAGCACAGCCTACATGCAACTCAGCAGCCTGACATCTGAGGACTCTGCGGTCTATTACTGTACAATACGGAATTACTACGGTAGTAGCTACGAGGACTACTGGGGCCAAGGCACCACTCTCACAGTCTCCTCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG']\n",
      "['GTCTATGGCAGTTCCTATCTCTCTCACTGGAGGCTGATTTTTGAAGAAAGGGGTTGTAGCCTAAAAGATGATGGTGTTAAGTCTTCTGTACCTGTTGACAGCCCTTCCGGGTATCCTGTCAGAGGTGCAGCTTCAGGAGTCAGGACCTAGCCTCGTGAAACCTTCTCAGACTCTGTCCCTCACCTGTTCTGTCACTGGCGACTCCATCACCAGTGGTTACTGGAACTGGATCCGGAAATTCCCAGGGAATAAACTTGAGTACATGGGGTACATAAGCTACAGTGGTAGCACTTACTACAATCCATCTCTCAAAAGTCGAATCTCCATCACTCGAGACACATCCAAGAACCAGTACTACCTGCAGTTGAATTCTGTGACTACTGAGGACACAGCCACATATTACTGTGCAAGATGGGACTATGACTGGGGTCAAGGAACCTCAGTCACCGTCTCCTCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG', 'AGTTGCGTCTTTTCTTATATGGGATCCTCTTCTCATAGAGCCTCCATCAGAGCATGGCTGTCTTGGGGCTGCTCTTCTGCCTGGTGACATTCCCAAGCTGTGTCCTATCCCAGGTGCAGCTGAAGCAGTCAGGACCTGGCCTAGTGCAGCCCTCACAGAGCCTGTCCATCACCTGCACAGTCTCTGGTTTCTCATTAACTAGCTATGGTGTACACTGGGTTCGCCAGTCTCCAGGAAAGGGTCTGGAGTGGCTGGGAGTGATATGGAGTGGTGGAAGCACAGACTATAATGCAGCTTTCATATCCAGACTGAGCATCAGCAAGGACAATTCCAAGAGCCAAGTTTTCTTTAAAATGAACAGTCTGCAAGCTAATGACACAGCCATATATTACTGTGCCAGAAATTCGGGGGGGTATGGTAACTACGCCCTTTTTGCTTACTGGGGCCAAGGGACTCTGGTCACTGTCTCTGCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG']\n",
      "['GGGGAACAGACACACAAACCTGGACTCACAAGTTTTTCTCTTCAGTGACAGACACAGACATAGAACATTCACGATGTACTTGGGACTGAACTATGTATTCATAGTTTTTCTCTTAAATGGTGTCCAGAGTGAAGTGAAGCTTGAGGAGTCTGGAGGAGGCTTGGTGCAACCTGGAGGATCCATGAAACTCTCTTGTGCTGCCTCTGGATTCACTTTTAGTGACGCCTGGATGGACTGGGTCCGCCAGTCTCCAGAGAAGGGGCTTGAGTGGGTTGCTGAAATTAGAAGCAAAGCTAATAATCATGCAACATACTATGCTGAGTCTGTGAAAGGGAGGTTCACCATCTCAAGAGATGATTCCAAAAGTAGTGTCTACCTGCAAATGAACAGCTTAAGAGCTGAAGACACTGGCATTTATTACTGTACCCGGTATGGTAACTGGCGGTACTTCGATGTCTGGGGCGCAGGGACCACGGTCACCGTCTCCTCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG', 'GGAGCTCTGACAGAGGAGGCAGGTCCTGGATTCGATTCCCAGTTCCTCACATTCAGTCAGCACTGAACACGGACCCCTCACCATGAACTTTGTGCTCAGCTTGATTTTCCTTGCCCTCATTTTAAAAGGTGTCCAGTGTGAAGTGCAGCTGGTGGAGTCTGGGGGAGGCTTAGTGAAGCCTGGAGGGTCCCTGAAACTCTCCTGTGCAGCCTCTGGATTCACTTTCAGTAGCTATGCCATGTCTTGGGTTCGCCAGACTCCGGAGAAGAGGCTGGAGTGGGTCGCAACCATTAGTAGTGGTGGTAGTTACACCTACTATCCAGACAGTGTGAAGGGTCGATTCACCATCTCCAGAGACAATGCCAAGAACACCCTGTACCTGCAAATGAGCAGTCTGAGGTCTGAGGACACGGCCATGTATTACTGTGCAAGACTAGACCCAACTGGGAACTATGCTATGGACTACTGGGGTCAAGGAACCTCAGTCACCGTCTCCTCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG']\n",
      "['ATCTCCTCACTAGAGCCCCCATCAGAGCATGGCTGTCCTGGTGCTGTTCCTCTGCCTGGTTGCATTTCCAAGCTGTGTCCTGTCCCAGGTGCAACTGAAGGAGTCAGGACCTGGCCTGGTGGCGCCCTCACAGAGCCTGTCCATCACTTGCACTGTCTCTGGGTTTTCCTTAACCAGCTATGGTGTACACTGGGTTCGCCAGCCTCCAGGAAAGGGTCTGGAGTGGCTGGGAGTAATATGGGCTGGTGGAATCACAAATTATAATTCGGCTCTCATGTCCAGACTGAGCATCAGCAAAGACAACTCCAAGAGCCAAGTTTTCTTAAAAATGAACAGTCTGCAAACTGTTGACACAGCCATGTACTACTGTGCCAGAGATAGGGCCGGCTACTATGGTAACTACTTTGACTACTGGGGCCAAGGCACCACTCTCACAGTCTCCTCAGCCAAAACGACACCCCCATCTGTCTATCCACTGGCCCCTGGATCTGCTGCCCAAACTAACTCCATGGTGACCCTGGGATGCCTGGTCAAGGG', 'GACATACCAGCAAGGGAGTGACCAGTTTGTCTTAAGGCACCACTGAGCCCAAGTCTTAGACATCATGGATTGGCTGTGGAACTTGCTATTCCTGATGGCAGCTGCCCAAAGTGCCCAAGCACAGATCCAGTTGGTGCAGTCTGGACCTGAGCTGAAGAAGCCTGGAGAGACAGTCAAGATCTCCTGCAAGGCTTCTGGGTATACCTTCACAAACTATGGAATGAACTGGGTGAAGCAGGCTCCAGGAAAGGGTTTAAAGTGGATGGGCTGGATAAACACCTACACTGGAGAGCCAACATATGCTGATGACTTCAAGGGACGGTTTGCCTTCTCTTTGGAAACCTCTGCCAGCACTGCCTATTTGCAGATCAACAACCTCAAAAATGAGGACATGGCTACATATTTCTGTGCAAGAGGGGGTAGTAGCTACAGGGACTGGTACTTCGATGTCTGGGGCGCAGGGACCACGGTCACCGTCTCCTCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG']\n",
      "['GAGCTCTGACAGAGGAGGCCAGTCCTGGAATTGATTCCCAGTTCCTCACGTTCAGTGATGAGCAGTGAACACAGACCCCTCACCATGAACTTCGGGCTCAGATTGATTTTCCTTGTCCTTACTTTAAAAGGTGTCCAGTGTGACGTGAAGCTGGTGGAGTCTGGGGGAGGCTTAGTGAAGCCTGGAGGGTCCCTGAAACTCTCCTGTGCAGCCTCTGGATTCACTTTCAGTAGCTATACCATGTCTTGGGTTCGCCAGACTCCGGAGAAGAGGCTGGAGTGGGTCGCAACCATTAGTAGTGGTGGTAGTTACACCTACTATCCAGACAGTGTGAAGGGCCGATTCACCATCTCCAGAGACAATGCCAAGAACACCCTGTACCTGCAAATGAGCAGTCTGAAGTCTGAGGACACAGCCATGTATTACTGTACAAGCTCCCACCCTGATTGGGGAGCTATGGACTACTGGGGTCAAGGAACCTCAGTCACCGTCTCCTCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG', 'TGGGGAGCTCTGACAGAGGAGGCCGGTCCTGGATTCGATTCCCAGTTCCTCACATTCAGTCAGCACTGAACACAGACACCTCACCATGAACTTCGGGCTCAGCTTGATTTTCCTTGTCCTTATTTTAAAAGGTGTCCAGTGTGAAGTGCAGCTGGTGGAGTCTGGGGGAGGCTTAGTGAAGCCTGGAGGGTCCCTGAAACTCTCCTGTGCAGCCTCTGGATTCACTTTCAGTAGCTATGCCATGTCTTGGGTTCGCCAGTCTCCAGAGAAGAGGCTGGAGTGGGTCGCAGAAATTAGTAGTGGTGGTAGTTACACCTACTATCCAGACACTGTGACGGGCCGATTCACCATCTCCAGAGACAATGCCAAGAACACCCTGTACCTGGAAATGAGCAGTCTGAGGTCTGAGGACACGGCCATGTATTACTGTGCAAGGGATCAGTCTACTATGATTACGTCGTTTGCTTACTGGGGCCAAGGGACTCTGGTCACTGTCTCTGCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG']\n",
      "['TCATCTCCTCACTAGAGCCCCCATCAGAGCATGGCTGTCCTGGTGCTGTTCCTCTGCCTGGTTGCATTTCCAAGCTGTGTCCTGTCCCAGGTGCAGCTGAAGGAGTCAGGACCTGGCCTGGTGGCGCCCTCACAGAGCCTGTCCATCACTTGCACTGTCTCTGGGTTTTCATTAACCAGCTATGGTGTACACTGGGTTCGCCAGCCTCCAGGAAAGGGTCTGGAGTGGCTGGGAGTAATATGGGCTGGTGGAAGCACAAATTATAATTCGGCTCTCATGTCCAGACTGAGCATCAGCAAAGACAACTCCAAGAGCCAAGTTTTCTTAAAAATGAACAGTCTGCAAACTGATGACACAGCCATGTACTACTGTGCCAGAGATCTATTCTATGCTATGGACTACTGGGGTCAAGGAACCTCAGTCACCGTCTCCTCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG', 'TATTTTTCTTATATGGGGATCCTCTTCTCATAGAGCCTCCATCAGAGCATGGCTGTCCTGGTGCTGCTCTTCTGCCTGGTGACATTCCCAAGCTGTGTCCTATCCCAGGTGCAGCTGAAGCAGTCAGGACCTGGCCTAGTGCAGCCCTCACAGAGCCTGTCCATCACCTGCACAGTCTCTGGTTTCTCATTAACTAGCTATGGTGTACACTGGGTTCGCCAGCCTCCAGGAAAGGGTCTGGAGTGGCTGGGAGTGATATGGAGTGGTGGAAGCACAGACTATAATGCTGCTTTCATATCCAGACTGAGCATCAGCAAGGACAACTCCAAGAGCCAAGTTTTCTTTAAAATGAACAGTCTGCAAGCTGATGACACAGCCATATACTACTGTGCCAGAAAGGCCCCCTATGCTATGGACTACTGGGGTCAAGGAACCTCAGTCACCGTCTCCTCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG']\n",
      "['CTGAGCTTTCTTATATGGGGAGCTCTGACAGAGGAGGCCTGTCCTGGATTCGATTCCCAGTTCCTCACATTCAGTCAGCACTGAACACGGACCCCTCACCATGAACTTCGGGCTCAGCTTGATTTTCCTTGTCCTTGTTTTAAAAGGTGTCCAGTGTGAAGTGATGCTGGTGGAGTCTGGGGGAGGCTTAGTGAAGCCTGGAGGGTCCCTGAAACTCTCCTGTGCAGCCTCTGGATTCACTTTCAGTAGCTATGCCATGTCTTGGGTTCGCCAGACTCCGGAGAAGAGGCTGGAGTGGGTCGCAACCATTAGTAGTGGTGGTAGTTACACCTACTATCCAGACAGTGTGAAGGGGCGATTCACCATCTCCAGAGACAATGCCAAGAACACCCTGTACCTGCAAATGAGCAGTCTGAGGTCTGAGGACACGGCCATGTATTACTGTGCAAGAGGAGGGGACTATGGTAACGGGTACTTCGATGTCTGGGGCGCAGGGACCACGGTCACCGTCTCCTCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG', 'ACAGTCATTGAAAACACTGACTCTAATCATGGAATGTAACTGGATACTTCCTTTTATTCTGTCGGTAATTTCAGGGGTCTACTCAGAGGTTCAGCTCCAGCAGTCTGGGACTGTGCTGGCAAGGCCTGGGGCTTCCGTGAAGATGTCCTGCAAGGCTTCTGGCTACAGCTTTACCAGCTACTGGATGCACTGGGTAAAACAGAGGCCTGGACAGGGTCTAGAATGGATTGGTGCTATTTATCCTGGAAATAGTGATACTAGCTACAACCAGAAGTTCAAGGGCAAGGCCAAACTGACTGCAGTCACATCCGCCAGCACTGCCTACATGGAGCTCAGCAGCCTGACAAATGAGGACTCTGCGGTCTATTACTGTACCCTTATGATTACGACGACGGTTTTTGCTTACTGGGGCCAAGGGACTCTGGTCACTGTCTCTGCAGAGAGTCAGTCCTTCCCAAATGTCTTCCCCCTCGTCTCCTGCGAGAGCCCCCTGTCTGATAAGAATCTGGTGGCCATGGGCTGCCTGG']\n",
      "\n",
      "\n",
      "\n",
      "After:\n",
      "{'text': tensor([[ 1,  6, 13,  ...,  6,  6,  6],\n",
      "        [16,  6,  6,  ...,  4,  9,  6]]), 'types': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_random': tensor([0, 1]), 'loss_mask': tensor([[0, 0, 1,  ..., 0, 0, 0],\n",
      "        [1, 0, 0,  ..., 1, 1, 0]]), 'labels': tensor([[1, 6, 6,  ..., 6, 6, 6],\n",
      "        [1, 6, 6,  ..., 6, 6, 6]]), 'padding_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'batch': ['AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA', 'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA']}\n",
      "{'text': tensor([[1, 6, 6,  ..., 6, 6, 6],\n",
      "        [1, 6, 6,  ..., 6, 6, 6]]), 'types': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_random': tensor([0, 1]), 'loss_mask': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'labels': tensor([[1, 6, 6,  ..., 6, 6, 6],\n",
      "        [1, 6, 6,  ..., 6, 6, 6]]), 'padding_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'batch': ['AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA', 'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA']}\n",
      "{'text': tensor([[1, 6, 6,  ..., 6, 6, 6],\n",
      "        [1, 6, 6,  ..., 6, 6, 6]]), 'types': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_random': tensor([0, 1]), 'loss_mask': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'labels': tensor([[1, 6, 6,  ..., 6, 6, 6],\n",
      "        [1, 6, 6,  ..., 6, 6, 6]]), 'padding_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'batch': ['AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA', 'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA']}\n",
      "{'text': tensor([[ 1, 25,  6,  ...,  6,  6,  6],\n",
      "        [ 1,  6,  6,  ...,  6,  6,  6]]), 'types': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_random': tensor([0, 1]), 'loss_mask': tensor([[0, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'labels': tensor([[1, 6, 6,  ..., 6, 6, 6],\n",
      "        [1, 6, 6,  ..., 6, 6, 6]]), 'padding_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'batch': ['AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA', 'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA']}\n",
      "{'text': tensor([[1, 6, 6,  ..., 6, 4, 6],\n",
      "        [1, 6, 4,  ..., 6, 6, 6]]), 'types': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_random': tensor([0, 1]), 'loss_mask': tensor([[0, 0, 0,  ..., 0, 1, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0]]), 'labels': tensor([[1, 6, 6,  ..., 6, 6, 6],\n",
      "        [1, 6, 6,  ..., 6, 6, 6]]), 'padding_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'batch': ['AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA', 'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA']}\n",
      "{'text': tensor([[ 1,  6, 26,  ...,  6,  6,  6],\n",
      "        [ 1,  6,  4,  ...,  6,  6,  6]]), 'types': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_random': tensor([0, 1]), 'loss_mask': tensor([[0, 0, 1,  ..., 0, 0, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0]]), 'labels': tensor([[1, 6, 6,  ..., 6, 6, 6],\n",
      "        [1, 6, 6,  ..., 6, 6, 6]]), 'padding_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'batch': ['AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA', 'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA']}\n",
      "{'text': tensor([[ 1,  6,  6,  ..., 22,  6,  6],\n",
      "        [ 4,  6,  6,  ...,  6,  6,  6]]), 'types': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_random': tensor([0, 1]), 'loss_mask': tensor([[0, 0, 0,  ..., 1, 0, 0],\n",
      "        [1, 0, 0,  ..., 0, 0, 0]]), 'labels': tensor([[1, 6, 6,  ..., 6, 6, 6],\n",
      "        [1, 6, 6,  ..., 6, 6, 6]]), 'padding_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'batch': ['AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA', 'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA']}\n",
      "{'text': tensor([[ 1,  4,  6,  ...,  6,  6,  6],\n",
      "        [ 1, 26,  6,  ...,  6,  6,  6]]), 'types': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_random': tensor([0, 1]), 'loss_mask': tensor([[0, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 1, 0,  ..., 0, 0, 0]]), 'labels': tensor([[1, 6, 6,  ..., 6, 6, 6],\n",
      "        [1, 6, 6,  ..., 6, 6, 6]]), 'padding_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'batch': ['AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA', 'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA']}\n",
      "{'text': tensor([[1, 6, 6,  ..., 6, 6, 6],\n",
      "        [1, 6, 6,  ..., 6, 6, 6]]), 'types': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_random': tensor([0, 1]), 'loss_mask': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'labels': tensor([[1, 6, 6,  ..., 6, 6, 6],\n",
      "        [1, 6, 6,  ..., 6, 6, 6]]), 'padding_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'batch': ['AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA', 'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA']}\n",
      "{'text': tensor([[1, 6, 6,  ..., 6, 6, 6],\n",
      "        [1, 6, 6,  ..., 6, 6, 6]]), 'types': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_random': tensor([0, 1]), 'loss_mask': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'labels': tensor([[1, 6, 6,  ..., 6, 6, 6],\n",
      "        [1, 6, 6,  ..., 6, 6, 6]]), 'padding_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'batch': ['AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA', 'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA']}\n",
      "{'text': tensor([[ 1,  4,  6,  ...,  6,  6,  6],\n",
      "        [ 1,  6,  6,  ...,  6, 19,  6]]), 'types': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'is_random': tensor([0, 1]), 'loss_mask': tensor([[0, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 1, 0]]), 'labels': tensor([[1, 6, 6,  ..., 6, 6, 6],\n",
      "        [1, 6, 6,  ..., 6, 6, 6]]), 'padding_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'batch': ['AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA', 'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA']}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "print(\"Before:\")\n",
    "dl = DataLoader(dataset, batch_size=2, shuffle=False)\n",
    "for i, item in enumerate(dl):\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(item)\n",
    "print(\"\\n\\n\\nAfter:\")\n",
    "dl = DataLoader(dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "for i, item in enumerate(dl):\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d43797",
   "metadata": {},
   "source": [
    "# Conclusion and further reading.\n",
    "\n",
    "This concludes our tutorial on including custom data in the BioNeMo framework. Throughout these tutorials we described how to manually update a model with a new dataset, and how those changes propagate throughout the framework. Checkout other Dataset classes and tokenizers in NeMo to learn about further customization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd444479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
