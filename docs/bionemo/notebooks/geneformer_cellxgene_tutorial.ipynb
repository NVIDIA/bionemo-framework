{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a88d40b-64de-4661-b3b0-7a4b87ba0162",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BioNeMo - Geneformer inferencing for single cell downstream tasks\n",
    "\n",
    "This tutorial showcases how to run the BioNeMo container, pre-train a geneformer model, and use it for inferencing downstream single cell tasks. At the end of this tutorial, a user will learn:\n",
    "- launching the BioNeMo container\n",
    "- Download data from czi to use for pre-training and inference.\n",
    "- Convert AnnData files into the sparse CSR memmap format used by BioNeMo\n",
    "- Kick-off pretraining with a custom single cell dataset\n",
    "- Restore the pre-trained model and perform inference with the same czi dataset.\n",
    "\n",
    "\n",
    "### Prerequisites:\n",
    "- BioNeMo Framework container is running (refer to the [Quickstart Guide](../quickstart-fw.md))\n",
    "- Familiarity with some components of the BioNeMo framework such as the [Models](../models/megamolbart.md) and [Inferencing](../inference-grpc-fw.md)\n",
    "\n",
    "\n",
    "#### Running the BioNeMo container\n",
    "\n",
    "This example has been built by launching the container in a local machine with 2 x A6000 RTX GPUs. Refer to specific instructions for [remote and multi-node launch]\n",
    "\n",
    "Once the container is launched, navigate to http://0.0.0.0:8888, http://localhost:8888, or the IP address of the workstation/node. A JupyterLab instance should show up.\n",
    "\n",
    "#### Copy this code and input files into JupyterLab\n",
    "\n",
    "In the launched JupyterLab, run the codes in a Jupyter notebook as provided in the code cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e47327a-6549-4ff4-a32a-a32d07a1e706",
   "metadata": {},
   "source": [
    "## Getting example single cell data and setting it up for inference\n",
    "\n",
    "First, we must acquire single cell training data for inference. To do this we will install the cellxgene-census api and download a small dataset. We use the example provided by the czi api examples page to download a single h5ad file. Generally, our workflow expects a collection of h5ad files to be used for pre-training. In this case, we restrict to 100k cells from a single dataset  to keep training time and downloading time small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "056ca29a-ffa1-4381-83b6-cad9a7b0c4d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: cellxgene-census in /workspace/bionemo/.local/lib/python3.10/site-packages (1.11.1)\n",
      "Requirement already satisfied: tiledbsoma~=1.8.0 in /workspace/bionemo/.local/lib/python3.10/site-packages (from cellxgene-census) (1.8.1)\n",
      "Requirement already satisfied: anndata in /workspace/bionemo/.local/lib/python3.10/site-packages (from cellxgene-census) (0.10.5.post1)\n",
      "Requirement already satisfied: numpy>=1.21 in /workspace/bionemo/.local/lib/python3.10/site-packages (from cellxgene-census) (1.24.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from cellxgene-census) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from cellxgene-census) (4.7.1)\n",
      "Requirement already satisfied: s3fs>=2021.06.1 in /workspace/bionemo/.local/lib/python3.10/site-packages (from cellxgene-census) (2024.3.1)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /workspace/bionemo/.local/lib/python3.10/site-packages (from s3fs>=2021.06.1->cellxgene-census) (2.12.1)\n",
      "Requirement already satisfied: fsspec==2024.3.1 in /workspace/bionemo/.local/lib/python3.10/site-packages (from s3fs>=2021.06.1->cellxgene-census) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from s3fs>=2021.06.1->cellxgene-census) (3.9.0)\n",
      "Requirement already satisfied: attrs>=22.2 in /usr/local/lib/python3.10/dist-packages (from tiledbsoma~=1.8.0->cellxgene-census) (23.1.0)\n",
      "Requirement already satisfied: numba>=0.58.0 in /workspace/bionemo/.local/lib/python3.10/site-packages (from tiledbsoma~=1.8.0->cellxgene-census) (0.59.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tiledbsoma~=1.8.0->cellxgene-census) (2.2.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from tiledbsoma~=1.8.0->cellxgene-census) (0.6)\n",
      "Requirement already satisfied: scanpy>=1.9.2 in /workspace/bionemo/.local/lib/python3.10/site-packages (from tiledbsoma~=1.8.0->cellxgene-census) (1.9.8)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from tiledbsoma~=1.8.0->cellxgene-census) (1.11.1)\n",
      "Requirement already satisfied: somacore==1.0.9 in /workspace/bionemo/.local/lib/python3.10/site-packages (from tiledbsoma~=1.8.0->cellxgene-census) (1.0.9)\n",
      "Requirement already satisfied: tiledb~=0.26.0 in /workspace/bionemo/.local/lib/python3.10/site-packages (from tiledbsoma~=1.8.0->cellxgene-census) (0.26.4)\n",
      "Requirement already satisfied: pyarrow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from tiledbsoma~=1.8.0->cellxgene-census) (14.0.1)\n",
      "Requirement already satisfied: array-api-compat in /workspace/bionemo/.local/lib/python3.10/site-packages (from anndata->cellxgene-census) (1.4.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata->cellxgene-census) (1.1.3)\n",
      "Requirement already satisfied: h5py>=3 in /usr/local/lib/python3.10/dist-packages (from anndata->cellxgene-census) (3.10.0)\n",
      "Requirement already satisfied: natsort in /workspace/bionemo/.local/lib/python3.10/site-packages (from anndata->cellxgene-census) (8.4.0)\n",
      "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.10/dist-packages (from anndata->cellxgene-census) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->cellxgene-census) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->cellxgene-census) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->cellxgene-census) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->cellxgene-census) (2023.7.22)\n",
      "Requirement already satisfied: botocore<1.34.52,>=1.34.41 in /workspace/bionemo/.local/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2021.06.1->cellxgene-census) (1.34.51)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2021.06.1->cellxgene-census) (1.14.1)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /workspace/bionemo/.local/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2021.06.1->cellxgene-census) (0.11.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2021.06.1->cellxgene-census) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2021.06.1->cellxgene-census) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2021.06.1->cellxgene-census) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2021.06.1->cellxgene-census) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2021.06.1->cellxgene-census) (4.0.3)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /workspace/bionemo/.local/lib/python3.10/site-packages (from numba>=0.58.0->tiledbsoma~=1.8.0->cellxgene-census) (0.42.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->tiledbsoma~=1.8.0->cellxgene-census) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tiledbsoma~=1.8.0->cellxgene-census) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->tiledbsoma~=1.8.0->cellxgene-census) (2024.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scanpy>=1.9.2->tiledbsoma~=1.8.0->cellxgene-census) (1.2.0)\n",
      "Requirement already satisfied: matplotlib>=3.6 in /workspace/bionemo/.local/lib/python3.10/site-packages (from scanpy>=1.9.2->tiledbsoma~=1.8.0->cellxgene-census) (3.8.3)\n",
      "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.10/dist-packages (from scanpy>=1.9.2->tiledbsoma~=1.8.0->cellxgene-census) (2.6.3)\n",
      "Requirement already satisfied: patsy in /workspace/bionemo/.local/lib/python3.10/site-packages (from scanpy>=1.9.2->tiledbsoma~=1.8.0->cellxgene-census) (0.5.6)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.10/dist-packages (from scanpy>=1.9.2->tiledbsoma~=1.8.0->cellxgene-census) (1.2.0)\n",
      "Requirement already satisfied: seaborn>=0.13.0 in /workspace/bionemo/.local/lib/python3.10/site-packages (from scanpy>=1.9.2->tiledbsoma~=1.8.0->cellxgene-census) (0.13.2)\n",
      "Requirement already satisfied: session-info in /workspace/bionemo/.local/lib/python3.10/site-packages (from scanpy>=1.9.2->tiledbsoma~=1.8.0->cellxgene-census) (1.0.0)\n",
      "Requirement already satisfied: statsmodels>=0.10.0rc2 in /workspace/bionemo/.local/lib/python3.10/site-packages (from scanpy>=1.9.2->tiledbsoma~=1.8.0->cellxgene-census) (0.14.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from scanpy>=1.9.2->tiledbsoma~=1.8.0->cellxgene-census) (4.66.2)\n",
      "Requirement already satisfied: umap-learn>=0.3.10 in /workspace/bionemo/.local/lib/python3.10/site-packages (from scanpy>=1.9.2->tiledbsoma~=1.8.0->cellxgene-census) (0.5.5)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.34.52,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs>=2021.06.1->cellxgene-census) (1.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy>=1.9.2->tiledbsoma~=1.8.0->cellxgene-census) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy>=1.9.2->tiledbsoma~=1.8.0->cellxgene-census) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /workspace/bionemo/.local/lib/python3.10/site-packages (from matplotlib>=3.6->scanpy>=1.9.2->tiledbsoma~=1.8.0->cellxgene-census) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy>=1.9.2->tiledbsoma~=1.8.0->cellxgene-census) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy>=1.9.2->tiledbsoma~=1.8.0->cellxgene-census) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy>=1.9.2->tiledbsoma~=1.8.0->cellxgene-census) (3.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->tiledbsoma~=1.8.0->cellxgene-census) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24->scanpy>=1.9.2->tiledbsoma~=1.8.0->cellxgene-census) (3.2.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /workspace/bionemo/.local/lib/python3.10/site-packages (from umap-learn>=0.3.10->scanpy>=1.9.2->tiledbsoma~=1.8.0->cellxgene-census) (0.5.11)\n",
      "Requirement already satisfied: stdlib-list in /workspace/bionemo/.local/lib/python3.10/site-packages (from session-info->scanpy>=1.9.2->tiledbsoma~=1.8.0->cellxgene-census) (0.10.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install cellxgene-census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f579ac-0207-4e07-8f34-dac0cd955c53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Below are paths required for setting up pre-training and inference.\n",
    "tutorial_data_dir = \"/workspace/bionemo/data/singlecell_tutorial/download_anndata\"\n",
    "tutorial_processed_dir = \"/workspace/bionemo/data/singlecell_tutorial/processed_data\"\n",
    "tutorial_output_dir = \"/workspace/bionemo/data/singlecell_tutorial/inference_output\"\n",
    "tutorial_output_inference_pickle = f\"{tutorial_output_dir}/human_covid19_bcells_inference.pkl\"\n",
    "demo_data_download_path = f\"{tutorial_data_dir}/human_covid19_bcells.h5ad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88bd2844-0251-476e-9e3f-8d4e89474f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p {tutorial_data_dir}\n",
    "!mkdir -p {tutorial_processed_dir}\n",
    "!mkdir -p {tutorial_output_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ccf03f0-1590-434e-a314-8ff0a02f5ef2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/bionemo/.local/lib/python3.10/site-packages/anndata/_core/anndata.py:1301: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  df[key] = c\n",
      "/workspace/bionemo/.local/lib/python3.10/site-packages/anndata/_core/anndata.py:1301: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  df[key] = c\n",
      "/workspace/bionemo/.local/lib/python3.10/site-packages/anndata/_core/anndata.py:1301: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  df[key] = c\n",
      "/workspace/bionemo/.local/lib/python3.10/site-packages/anndata/_core/anndata.py:1301: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  df[key] = c\n",
      "/workspace/bionemo/.local/lib/python3.10/site-packages/anndata/_core/anndata.py:1301: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  df[key] = c\n",
      "/workspace/bionemo/.local/lib/python3.10/site-packages/anndata/_core/anndata.py:1301: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  df[key] = c\n"
     ]
    }
   ],
   "source": [
    "import cellxgene_census\n",
    "\n",
    "with cellxgene_census.open_soma(census_version=\"2023-12-15\") as census:\n",
    "    filter1 = \"cell_type == 'B cell' and tissue_general == 'lung' and disease == 'COVID-19' and is_primary_data == True\"\n",
    "\n",
    "    adata = cellxgene_census.get_anndata(\n",
    "        census = census,\n",
    "        organism = \"Homo sapiens\",\n",
    "        obs_value_filter = filter1,\n",
    "        column_names = {\"obs\": [\"assay\", \"cell_type\", \"tissue\", \"tissue_general\", \"suspension_type\", \"disease\"]},\n",
    "    )\n",
    "\n",
    "    adata[:100000].write(demo_data_download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b060e73b-a3f9-4315-ae91-c23b9a29f490",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 skothenhill skothenhill 27M Apr 26 21:44 /workspace/bionemo/data/singlecell_tutorial/download_anndata/human_covid19_bcells.h5ad\n"
     ]
    }
   ],
   "source": [
    "!ls -laht {demo_data_download_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1812f0e-7179-4140-aa9c-8109813f3e5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 files\n",
      "Starting to create memmap files...\n",
      "Metadata already exists, loading...\n",
      "Writing data into memmaps to /workspace/bionemo/data/singlecell_tutorial/processed_data...\n",
      "Merging AnnData into numpy memaps...: 100%|███████| 1/1 [00:00<00:00,  6.31it/s]\n",
      "Saving dataframe ...\n",
      "Done creating dataset ...\n"
     ]
    }
   ],
   "source": [
    "!python /workspace/bionemo/bionemo/data/singlecell/sc_memmap.py \\\n",
    "  --data-path {tutorial_data_dir} \\\n",
    "  --save-path {tutorial_processed_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd212acb-9695-4ec4-bff6-8669693118f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 18M\n",
      "-rw-r--r-- 1 skothenhill skothenhill 107K Apr 26 21:48 features.csv\n",
      "-rw-r--r-- 1 skothenhill skothenhill 8.5M Apr 26 21:48 gene_expression_ind.npy\n",
      "-rw-r--r-- 1 skothenhill skothenhill  19K Apr 26 21:48 gene_expression_ptr.npy\n",
      "-rw-r--r-- 1 skothenhill skothenhill 8.5M Apr 26 21:48 gene_expression_data.npy\n",
      "drwxr-xr-x 2 skothenhill skothenhill 4.0K Apr 26 21:46 .\n",
      "-rw-r--r-- 1 skothenhill skothenhill 764K Apr 26 21:44 metadata.json\n",
      "drwxr-xr-x 5 skothenhill skothenhill 4.0K Apr 26 21:37 ..\n"
     ]
    }
   ],
   "source": [
    "!ls -laht {tutorial_processed_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a43efdd-8ba5-4a3b-8f70-9b5839ec075c",
   "metadata": {},
   "source": [
    "# Pretraining\n",
    "Now that we have aquired the h5ad files we would like to use for training and converted them to a sparse memmap. We will kickoff training. This involves two distinct steps\n",
    "- preprocessing (indicated with do_training=False), where artifacts are downloaded from huggingface to be used by the model. Importantly, we set the `dataset_path` to be the same place we created the sparse memmap files. This is how BioNeMo knows where to find files for training, including both training data and additional artifacts (such as tokenizers).\n",
    "- pretraining, where the model is actually trained.\n",
    "\n",
    "We set the flag `max_steps` to limit the runtime. Check the full config file in `examples/singlecell/geneformer/conf` for a complete list of arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "424247a7-105b-4bea-b34e-b28a8f5787af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-04-26 21:53:40 nemo_logging:349] /usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "      self.pid = os.fork()\n",
      "    \n",
      "[NeMo I 2024-04-26 21:53:42 megatron_hiddens:110] Registered hidden transform sampled_var_cond_gaussian at bionemo.model.core.hiddens_support.SampledVarGaussianHiddenTransform\n",
      "[NeMo I 2024-04-26 21:53:42 megatron_hiddens:110] Registered hidden transform interp_var_cond_gaussian at bionemo.model.core.hiddens_support.InterpVarGaussianHiddenTransform\n",
      "[NeMo W 2024-04-26 21:53:42 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'geneformer_config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "      warnings.warn(msg, UserWarning)\n",
      "    \n",
      "[NeMo W 2024-04-26 21:53:42 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n",
      "[NeMo I 2024-04-26 21:53:42 pretrain:32] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-04-26 21:53:42 pretrain:33] \n",
      "    name: geneformer_base_config\n",
      "    restore_from_path: null\n",
      "    seed_everything: false\n",
      "    do_training: false\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      num_nodes: 1\n",
      "      accelerator: gpu\n",
      "      precision: bf16-mixed\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 1\n",
      "      max_steps: 400000\n",
      "      log_every_n_steps: 100\n",
      "      val_check_interval: 100\n",
      "      limit_val_batches: 8\n",
      "      limit_test_batches: 500\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "      benchmark: false\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: ${oc.env:BIONEMO_HOME}/results/nemo_experiments/${.name}/${.wandb_logger_kwargs.name}\n",
      "      name: geneformer\n",
      "      create_wandb_logger: true\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: geneformer-pretraining\n",
      "        offline: false\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: val_loss\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        always_save_nemo: true\n",
      "        filename: geneformer--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "    model:\n",
      "      tokenizer:\n",
      "        vocab_file: ${..data.dataset_path}/geneformer.vocab\n",
      "      micro_batch_size: 8\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      use_flash_attention: true\n",
      "      seq_length: 2048\n",
      "      encoder_seq_length: ${.seq_length}\n",
      "      max_position_embeddings: ${.seq_length}\n",
      "      num_layers: 6\n",
      "      hidden_size: 256\n",
      "      ffn_hidden_size: 512\n",
      "      num_attention_heads: 4\n",
      "      init_method_std: 0.02\n",
      "      hidden_dropout: 0.1\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: true\n",
      "      bert_binary_head: false\n",
      "      resume_from_checkpoint: null\n",
      "      masked_softmax_fusion: true\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      seed: 1234\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: 1\n",
      "      data:\n",
      "        data_impl: geneformer\n",
      "        probabilistic_dirichlet_sampling_train: false\n",
      "        dataset_path: /workspace/bionemo/data/singlecell_tutorial/processed_data\n",
      "        data_prefix: ''\n",
      "        shuffle: true\n",
      "        dataset: /\n",
      "        medians_file: ${.dataset_path}/medians.json\n",
      "        index_mapping_dir: null\n",
      "        skip_warmup: true\n",
      "        index_mapping_type: memmap\n",
      "        train_ratio: 0.98\n",
      "        val_ratio: 0.01\n",
      "        num_workers: 12\n",
      "        dataloader_type: single\n",
      "        seq_length: ${model.seq_length}\n",
      "        seed: ${model.seed}\n",
      "        dynamic_padding: false\n",
      "        micro_batch_size: ${model.micro_batch_size}\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0002\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: ${multiply:${trainer.max_steps}, 0.01}\n",
      "          constant_steps: ${multiply:${trainer.max_steps}, 0.05}\n",
      "          max_steps: ${trainer.max_steps}\n",
      "          min_lr: 2.0e-05\n",
      "      precision: bf16-mixed\n",
      "    \n",
      "[NeMo I 2024-04-26 21:53:42 pretrain:50] ************** Starting Preprocessing ***********\n",
      "[NeMo I 2024-04-26 21:53:42 remote:103] Downloading resource: https://huggingface.co/ctheodoris/Geneformer/resolve/main/geneformer/gene_name_id_dict.pkl?download=true\n",
      "[NeMo I 2024-04-26 21:53:43 remote:121] No checksum provided, filename exists. Assuming it is complete.\n",
      "[NeMo I 2024-04-26 21:53:43 remote:103] Downloading resource: https://huggingface.co/ctheodoris/Geneformer/resolve/main/geneformer/gene_median_dictionary.pkl?download=true\n",
      "[NeMo I 2024-04-26 21:53:43 remote:121] No checksum provided, filename exists. Assuming it is complete.\n",
      "[NeMo I 2024-04-26 21:53:43 pretrain:59] *************** Preprocessing Finished ************\n"
     ]
    }
   ],
   "source": [
    "# Run preprocessing to acquire the requisite files for pre-training.\n",
    "!python /workspace/bionemo/examples/singlecell/geneformer/pretrain.py \\\n",
    "  ++model.data.dataset_path={tutorial_processed_dir} \\\n",
    "  ++do_training=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86aeeb46-3733-44b8-ba90-e8fd127d7441",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-04-26 22:02:02 nemo_logging:349] /usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "      self.pid = os.fork()\n",
      "    \n",
      "[NeMo I 2024-04-26 22:02:04 megatron_hiddens:110] Registered hidden transform sampled_var_cond_gaussian at bionemo.model.core.hiddens_support.SampledVarGaussianHiddenTransform\n",
      "[NeMo I 2024-04-26 22:02:04 megatron_hiddens:110] Registered hidden transform interp_var_cond_gaussian at bionemo.model.core.hiddens_support.InterpVarGaussianHiddenTransform\n",
      "[NeMo W 2024-04-26 22:02:04 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'geneformer_config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "      warnings.warn(msg, UserWarning)\n",
      "    \n",
      "[NeMo W 2024-04-26 22:02:04 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n",
      "[NeMo I 2024-04-26 22:02:04 pretrain:32] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-04-26 22:02:04 pretrain:33] \n",
      "    name: geneformer_base_config\n",
      "    restore_from_path: null\n",
      "    seed_everything: false\n",
      "    do_training: true\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      num_nodes: 1\n",
      "      accelerator: gpu\n",
      "      precision: bf16-mixed\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 1\n",
      "      max_steps: 200\n",
      "      log_every_n_steps: 100\n",
      "      val_check_interval: 100\n",
      "      limit_val_batches: 8\n",
      "      limit_test_batches: 500\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "      benchmark: false\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: /workspace/bionemo/data/singlecell_tutorial/inference_output\n",
      "      name: geneformer\n",
      "      create_wandb_logger: true\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: geneformer-pretraining\n",
      "        offline: true\n",
      "      resume_if_exists: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: val_loss\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        always_save_nemo: true\n",
      "        filename: geneformer--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "    model:\n",
      "      tokenizer:\n",
      "        vocab_file: ${..data.dataset_path}/geneformer.vocab\n",
      "      micro_batch_size: 8\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      use_flash_attention: true\n",
      "      seq_length: 2048\n",
      "      encoder_seq_length: ${.seq_length}\n",
      "      max_position_embeddings: ${.seq_length}\n",
      "      num_layers: 6\n",
      "      hidden_size: 256\n",
      "      ffn_hidden_size: 512\n",
      "      num_attention_heads: 4\n",
      "      init_method_std: 0.02\n",
      "      hidden_dropout: 0.1\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: true\n",
      "      bert_binary_head: false\n",
      "      resume_from_checkpoint: null\n",
      "      masked_softmax_fusion: true\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      seed: 1234\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: 1\n",
      "      data:\n",
      "        data_impl: geneformer\n",
      "        probabilistic_dirichlet_sampling_train: false\n",
      "        dataset_path: /workspace/bionemo/data/singlecell_tutorial/processed_data\n",
      "        data_prefix: ''\n",
      "        shuffle: true\n",
      "        dataset: /\n",
      "        medians_file: ${.dataset_path}/medians.json\n",
      "        index_mapping_dir: null\n",
      "        skip_warmup: true\n",
      "        index_mapping_type: memmap\n",
      "        train_ratio: 0.98\n",
      "        val_ratio: 0.01\n",
      "        num_workers: 12\n",
      "        dataloader_type: single\n",
      "        seq_length: ${model.seq_length}\n",
      "        seed: ${model.seed}\n",
      "        dynamic_padding: false\n",
      "        micro_batch_size: ${model.micro_batch_size}\n",
      "        output_fname: /workspace/bionemo/data/singlecell_tutorial/inference_output/human_covid19_bcells_inference.pkl\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0002\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: ${multiply:${trainer.max_steps}, 0.01}\n",
      "          constant_steps: ${multiply:${trainer.max_steps}, 0.05}\n",
      "          max_steps: ${trainer.max_steps}\n",
      "          min_lr: 2.0e-05\n",
      "      precision: bf16-mixed\n",
      "    \n",
      "[NeMo I 2024-04-26 22:02:04 utils:230] Selected Callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo I 2024-04-26 22:02:05 exp_manager:394] Experiments will be logged at /workspace/bionemo/data/singlecell_tutorial/inference_output/geneformer/2024-04-26_22-02-05\n",
      "[NeMo I 2024-04-26 22:02:05 exp_manager:835] TensorboardLogger has been set up\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id 2024-04-26_22-02-05.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
      "[NeMo I 2024-04-26 22:02:06 exp_manager:850] WandBLogger has been set up\n",
      "[NeMo W 2024-04-26 22:02:06 exp_manager:931] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 200. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo I 2024-04-26 22:02:06 utils:306] \n",
      "    \n",
      "    ************** Trainer configuration ***********\n",
      "[NeMo I 2024-04-26 22:02:06 utils:307] \n",
      "    name: geneformer_base_config\n",
      "    restore_from_path: null\n",
      "    seed_everything: false\n",
      "    do_training: true\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      num_nodes: 1\n",
      "      accelerator: gpu\n",
      "      precision: bf16-mixed\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 1\n",
      "      max_steps: 200\n",
      "      log_every_n_steps: 100\n",
      "      val_check_interval: 100\n",
      "      limit_val_batches: 8\n",
      "      limit_test_batches: 500\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "      benchmark: false\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: /workspace/bionemo/data/singlecell_tutorial/inference_output\n",
      "      name: geneformer\n",
      "      create_wandb_logger: true\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: geneformer-pretraining\n",
      "        offline: true\n",
      "      resume_if_exists: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: val_loss\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        always_save_nemo: true\n",
      "        filename: geneformer--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}\n",
      "    model:\n",
      "      tokenizer:\n",
      "        vocab_file: ${..data.dataset_path}/geneformer.vocab\n",
      "      micro_batch_size: 8\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      use_flash_attention: true\n",
      "      seq_length: 2048\n",
      "      encoder_seq_length: ${.seq_length}\n",
      "      max_position_embeddings: ${.seq_length}\n",
      "      num_layers: 6\n",
      "      hidden_size: 256\n",
      "      ffn_hidden_size: 512\n",
      "      num_attention_heads: 4\n",
      "      init_method_std: 0.02\n",
      "      hidden_dropout: 0.1\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: true\n",
      "      bert_binary_head: false\n",
      "      resume_from_checkpoint: null\n",
      "      masked_softmax_fusion: true\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      seed: 1234\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: 1\n",
      "      data:\n",
      "        data_impl: geneformer\n",
      "        probabilistic_dirichlet_sampling_train: false\n",
      "        dataset_path: /workspace/bionemo/data/singlecell_tutorial/processed_data\n",
      "        data_prefix: ''\n",
      "        shuffle: true\n",
      "        dataset: /\n",
      "        medians_file: ${.dataset_path}/medians.json\n",
      "        index_mapping_dir: null\n",
      "        skip_warmup: true\n",
      "        index_mapping_type: memmap\n",
      "        train_ratio: 0.98\n",
      "        val_ratio: 0.01\n",
      "        num_workers: 12\n",
      "        dataloader_type: single\n",
      "        seq_length: ${model.seq_length}\n",
      "        seed: ${model.seed}\n",
      "        dynamic_padding: false\n",
      "        micro_batch_size: ${model.micro_batch_size}\n",
      "        output_fname: /workspace/bionemo/data/singlecell_tutorial/inference_output/human_covid19_bcells_inference.pkl\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0002\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: ${multiply:${trainer.max_steps}, 0.01}\n",
      "          constant_steps: ${multiply:${trainer.max_steps}, 0.05}\n",
      "          max_steps: ${trainer.max_steps}\n",
      "          min_lr: 2.0e-05\n",
      "      precision: bf16-mixed\n",
      "      global_batch_size: 8\n",
      "    \n",
      "[NeMo W 2024-04-26 22:02:06 modelPT:251] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo I 2024-04-26 22:02:06 megatron_init:234] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2024-04-26 22:02:06 megatron_init:237] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2024-04-26 22:02:06 megatron_init:238] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-04-26 22:02:06 megatron_init:246] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-04-26 22:02:06 megatron_init:247] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-04-26 22:02:06 megatron_init:257] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-04-26 22:02:06 megatron_init:261] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-04-26 22:02:06 megatron_init:262] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-04-26 22:02:06 megatron_init:276] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-04-26 22:02:06 megatron_init:288] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-04-26 22:02:06 megatron_init:294] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-04-26 22:02:06 megatron_init:295] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-04-26 22:02:06 megatron_init:296] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-04-26 22:02:06 megatron_init:297] Rank 0 has embedding rank: 0\n",
      "24-04-26 22:02:06 - PID:214862 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 1\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 megatron_base_model:821] The model: GeneformerModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:02:06 modelPT:251] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "[NeMo I 2024-04-26 22:02:06 megatron_base_model:315] Padded vocab_size: 21376, original vocab_size: 21250, dummy tokens: 126.\n",
      "[NeMo I 2024-04-26 22:02:06 pretrain:46] ************** Starting Training ***********\n",
      "[NeMo W 2024-04-26 22:02:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:153: UserWarning: The `batch_idx` argument in `GeneformerModel.on_train_batch_start` hook may not match with the actual batch index when using a `dataloader_iter` argument in your `training_step`.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2024-04-26 22:02:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:153: UserWarning: The `batch_idx` argument in `GeneformerModel.on_train_batch_end` hook may not match with the actual batch index when using a `dataloader_iter` argument in your `training_step`.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Pipeline model parallel rank: 0, Tensor model parallel rank: 0, Number of model parameters on device: 9.25e+06. Total number of model parameters: 9.25e+06.\n",
      "[NeMo I 2024-04-26 22:02:06 core:254] Building Bert datasets.\n",
      "[NeMo W 2024-04-26 22:02:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/data/language_modeling/megatron/dataset_utils.py:1332: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "      counts = torch.cuda.LongTensor([1])\n",
      "    \n",
      "[NeMo I 2024-04-26 22:02:07 dataset_utils:1341]  > loading indexed mapping from backed_backed_indexmap_2313mns_2046msl_0.00ssp_1234s.npy\n",
      "[NeMo I 2024-04-26 22:02:07 dataset_utils:1344]     loaded indexed file in 0.001 seconds\n",
      "[NeMo I 2024-04-26 22:02:07 dataset_utils:1345]     total number of samples: 2313\n",
      " > WARNING: could not find index map file _train_indexmap_1600mns_2046msl_0.00ssp_1234s.npy, building the indices on rank 0 ...\n",
      "[NeMo I 2024-04-26 22:02:07 dataset_utils:1303]  > building samples index mapping for train ...\n",
      "make: Entering directory '/usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "    using uint32 for data mapping...\n",
      "    using:\n",
      "     number of documents:            2266\n",
      "     sentences range:                [0, 2266)\n",
      "     total number of sentences:      2266\n",
      "     number of epochs:               2147483646\n",
      "     maximum number of samples:      1600\n",
      "     maximum sequence length:        2046\n",
      "     short sequence probability:     0\n",
      "     short sequence ration (1/prob): 0\n",
      "     seed:                           1234\n",
      "    reached 1600 samples after 1 epochs ...\n",
      "   number of empty documents: 0\n",
      "   number of documents with one sentence: 2266\n",
      "   number of documents with long sentences: 0\n",
      "   will create mapping for 2266 samples\n",
      "[NeMo I 2024-04-26 22:02:07 dataset_utils:1324]  > done building samples index maping\n",
      "[NeMo I 2024-04-26 22:02:07 dataset_utils:1326]  > saved the index mapping in _train_indexmap_1600mns_2046msl_0.00ssp_1234s.npy\n",
      "[NeMo I 2024-04-26 22:02:07 dataset_utils:1328]  > elasped time to build and save samples mapping (seconds): 0.067438\n",
      "[NeMo I 2024-04-26 22:02:07 core:260] Length of train dataset: 1600\n",
      "[NeMo I 2024-04-26 22:02:07 core:261] Length of val dataset: 23\n",
      "[NeMo I 2024-04-26 22:02:07 core:262] Length of test dataset: 24\n",
      "[NeMo I 2024-04-26 22:02:07 core:263] Finished building Bert datasets.\n",
      "Setting up train dataloader with len(len(self._train_ds)): 1600 and consumed samples: 0\n",
      "[NeMo I 2024-04-26 22:02:07 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 1600 and consumed_samples: 0\n",
      "[NeMo I 2024-04-26 22:02:07 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 23 and consumed_samples: 0\n",
      "[NeMo I 2024-04-26 22:02:07 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 24 and consumed_samples: 0\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[NeMo I 2024-04-26 22:02:07 nlp_overrides:150] Configuring DDP for model parallelism.\n",
      "[NeMo I 2024-04-26 22:02:07 modelPT:728] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0002\n",
      "        weight_decay: 0.01\n",
      "    \n",
      "    Parameter Group 1\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0002\n",
      "        weight_decay: 0.0\n",
      "    )\n",
      "[NeMo I 2024-04-26 22:02:07 lr_scheduler:943] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f7bc2fb00d0>\" \n",
      "    will be used during training (effective maximum steps = 200) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 2.0\n",
      "    constant_steps: 10.0\n",
      "    max_steps: 200\n",
      "    min_lr: 2.0e-05\n",
      "    )\n",
      "\n",
      "  | Name                           | Type                     | Params\n",
      "----------------------------------------------------------------------------\n",
      "0 | model                          | BertModel                | 9.2 M \n",
      "1 | model.language_model           | TransformerLanguageModel | 9.2 M \n",
      "2 | model.language_model.embedding | Embedding                | 6.0 M \n",
      "3 | model.language_model.encoder   | ParallelTransformer      | 3.2 M \n",
      "4 | model.lm_head                  | BertLMHead               | 87.7 K\n",
      "5 | model.lm_head.dense            | Linear                   | 65.8 K\n",
      "6 | model.lm_head.layernorm        | MixedFusedLayerNorm      | 512   \n",
      "----------------------------------------------------------------------------\n",
      "9.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "9.2 M     Total params\n",
      "36.989    Total estimated model params size (MB)\n",
      "Sanity Checking: 0it [00:00, ?it/s][NeMo W 2024-04-26 22:02:07 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2024-04-26 22:02:07 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: UserWarning: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Sanity Checking DataLoader 0:   0%|                       | 0/2 [00:00<?, ?it/s][NeMo W 2024-04-26 22:02:07 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/modules/common/megatron/fused_bias_dropout_add.py:70: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)\n",
      "      return bias_dropout_add_fused_inference_(*args)\n",
      "    \n",
      "Sanity Checking DataLoader 0: : 3it [00:01,  2.68it/s]                          [NeMo W 2024-04-26 22:02:08 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2024-04-26 22:02:08 nemo_logging:349] /usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "      self.pid = os.fork()\n",
      "    \n",
      "[NeMo W 2024-04-26 22:02:08 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: UserWarning: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Epoch 0:   0%|                                          | 0/200 [00:00<?, ?it/s][NeMo W 2024-04-26 22:02:10 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2024-04-26 22:02:10 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('consumed_samples', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "Epoch 0:  50%|▌| 100/200 [00:14<00:14,  7.06it/s, v_num=2-05, reduced_train_loss\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|██████████          | 1/2 [00:00<00:00, 14.80it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 2/2 [00:00<00:00, 15.14it/s]\u001b[A\n",
      "Epoch 0:  50%|▌| 100/200 [00:14<00:14,  6.99it/s, v_num=2-05, reduced_train_loss\u001b[A\n",
      "                                                 \u001b[AEpoch 0, global step 100: 'val_loss' reached 8.69522 (best 8.69522), saving model to '/workspace/bionemo/data/singlecell_tutorial/inference_output/geneformer/2024-04-26_22-02-05/checkpoints/geneformer--val_loss=8.70-step=100-consumed_samples=800.0.ckpt' as top 1\n",
      "[NeMo I 2024-04-26 22:02:23 nemo_model_checkpoint:183] New .nemo model saved to: /workspace/bionemo/data/singlecell_tutorial/inference_output/geneformer/2024-04-26_22-02-05/checkpoints/geneformer.nemo\n",
      "[NeMo I 2024-04-26 22:02:23 nemo_model_checkpoint:183] New .nemo model saved to: /workspace/bionemo/data/singlecell_tutorial/inference_output/geneformer/2024-04-26_22-02-05/checkpoints/geneformer.nemo\n",
      "Epoch 0: 100%|█| 200/200 [00:27<00:00,  7.39it/s, v_num=2-05, reduced_train_loss\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|██████████          | 1/2 [00:00<00:00, 15.03it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 2/2 [00:00<00:00, 15.23it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 200/200 [00:27<00:00,  7.35it/s, v_num=2-05, reduced_train_loss\u001b[A\n",
      "                                                 \u001b[AEpoch 0, global step 200: 'val_loss' reached 8.54715 (best 8.54715), saving model to '/workspace/bionemo/data/singlecell_tutorial/inference_output/geneformer/2024-04-26_22-02-05/checkpoints/geneformer--val_loss=8.55-step=200-consumed_samples=1600.0.ckpt' as top 1\n",
      "[NeMo I 2024-04-26 22:02:35 nemo_model_checkpoint:183] New .nemo model saved to: /workspace/bionemo/data/singlecell_tutorial/inference_output/geneformer/2024-04-26_22-02-05/checkpoints/geneformer.nemo\n",
      "[NeMo I 2024-04-26 22:02:36 nlp_overrides:412] Removing checkpoint: /workspace/bionemo/data/singlecell_tutorial/inference_output/geneformer/2024-04-26_22-02-05/checkpoints/geneformer--val_loss=8.70-step=100-consumed_samples=800.0.ckpt\n",
      "[NeMo I 2024-04-26 22:02:36 nemo_model_checkpoint:183] New .nemo model saved to: /workspace/bionemo/data/singlecell_tutorial/inference_output/geneformer/2024-04-26_22-02-05/checkpoints/geneformer.nemo\n",
      "[NeMo I 2024-04-26 22:02:36 nlp_overrides:412] Removing checkpoint: /workspace/bionemo/data/singlecell_tutorial/inference_output/geneformer/2024-04-26_22-02-05/checkpoints/geneformer--val_loss=8.70-step=100-consumed_samples=800.0-last.ckpt\n",
      "Epoch 0: 100%|█| 200/200 [00:27<00:00,  7.17it/s, v_num=2-05, reduced_train_loss`Trainer.fit` stopped: `max_steps=200` reached.\n",
      "Epoch 0: 100%|█| 200/200 [00:27<00:00,  7.17it/s, v_num=2-05, reduced_train_loss\n",
      "[NeMo I 2024-04-26 22:02:36 pretrain:48] *************** Finish Training ************\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            consumed_samples ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch ▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 global_step ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   grad_norm █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          reduced_train_loss █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_backward_timing in s █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_step_timing in s █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         trainer/global_step ▄▁▁▁▄█▁▁▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    val_loss █▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: validation_step_timing in s ██▁██▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            consumed_samples 1600.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                       epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 global_step 199.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   grad_norm 0.97383\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                          lr 2e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          reduced_train_loss 8.62004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_backward_timing in s 4e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train_step_timing in s 0.11772\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         trainer/global_step 199\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    val_loss 8.54715\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: validation_step_timing in s 0.00018\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /workspace/bionemo/data/singlecell_tutorial/inference_output/wandb/offline-run-20240426_220206-2024-04-26_22-02-05\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m/workspace/bionemo/data/singlecell_tutorial/inference_output/wandb/offline-run-20240426_220206-2024-04-26_22-02-05/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Pretrain the model using\n",
    "!python /workspace/bionemo/examples/singlecell/geneformer/pretrain.py \\\n",
    "  --config-dir /workspace/bionemo/examples/singlecell/geneformer/conf \\\n",
    "  --config-name geneformer_config \\\n",
    "  ++model.data.dataset_path={tutorial_processed_dir} \\\n",
    "  ++trainer.devices=1 \\\n",
    "  ++trainer.max_steps=200 \\\n",
    "  ++exp_manager.exp_dir={tutorial_output_dir} \\\n",
    "  ++model.data.output_fname={tutorial_output_inference_pickle} \\\n",
    "  ++exp_manager.wandb_logger_kwargs.offline=True \\\n",
    "  ++exp_manager.resume_if_exists=False \\\n",
    "  ++do_training=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23771e9-0dd6-414e-8cd0-5cd9a9dd4a6b",
   "metadata": {},
   "source": [
    "# Running inference.\n",
    "\n",
    "We can see from the above training job that the model was trained for a short number of steps. Note the end of the log file the experiment manager leaves a message about where the resulting `.nemo` file is written. This file is used for finetuning, inference, or training from an existing set of model weights. See the example produced below from our run:\n",
    "\n",
    "```text\n",
    "[NeMo I 2024-04-26 22:02:36 nemo_model_checkpoint:183] New .nemo model saved to: /workspace/bionemo/data/singlecell_tutorial/inference_output/geneformer/2024-04-26_22-02-05/checkpoints/geneformer.nemo\n",
    "[NeMo I 2024-04-26 22:02:36 nlp_overrides:412] Removing checkpoint: /workspace/bionemo/data/singlecell_tutorial/inference_output/geneformer/2024-04-26_22-02-05/checkpoints/geneformer--val_loss=8.70-step=100-consumed_samples=800.0-last.ckpt\n",
    "Epoch 0: 100%|█| 200/200 [00:27<00:00,  7.17it/s, v_num=2-05, reduced_train_loss`Trainer.fit` stopped: `max_steps=200` reached.\n",
    "Epoch 0: 100%|█| 200/200 [00:27<00:00,  7.17it/s, v_num=2-05, reduced_train_loss\n",
    "```\n",
    "\n",
    "We will take the `.nemo` file logged:\n",
    "`/workspace/bionemo/data/singlecell_tutorial/inference_output/geneformer/2024-04-26_22-02-05/checkpoints/geneformer.nemo`\n",
    "\n",
    "and use this for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b11a330-b8f6-491a-b608-fe47e78f83ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrained_nemo_file = '/workspace/bionemo/data/singlecell_tutorial/inference_output/geneformer/2024-04-26_22-02-05/checkpoints/geneformer.nemo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "269a327d-a5b1-49c9-96b3-141ae99baeb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-04-26 22:05:17 nemo_logging:349] /usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "      self.pid = os.fork()\n",
      "    \n",
      "[NeMo I 2024-04-26 22:05:19 megatron_hiddens:110] Registered hidden transform sampled_var_cond_gaussian at bionemo.model.core.hiddens_support.SampledVarGaussianHiddenTransform\n",
      "[NeMo I 2024-04-26 22:05:19 megatron_hiddens:110] Registered hidden transform interp_var_cond_gaussian at bionemo.model.core.hiddens_support.InterpVarGaussianHiddenTransform\n",
      "[NeMo W 2024-04-26 22:05:19 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n",
      "[NeMo I 2024-04-26 22:05:19 loading:31] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-04-26 22:05:19 loading:32] \n",
      "    name: geneformer_inference\n",
      "    desc: Minimum configuration for initializing a Geneformer model for inference.\n",
      "    trainer:\n",
      "      precision: 16-mixed\n",
      "      devices: 1\n",
      "      num_nodes: 1\n",
      "      accelerator: gpu\n",
      "      logger: false\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: /workspace/bionemo/data/singlecell_tutorial/inference_output\n",
      "      name: ${name}\n",
      "      create_checkpoint_callback: false\n",
      "    model:\n",
      "      micro_batch_size: ${model.data.batch_size}\n",
      "      downstream_task:\n",
      "        restore_from_path: /workspace/bionemo/data/singlecell_tutorial/inference_output/geneformer/2024-04-26_22-02-05/checkpoints/geneformer.nemo\n",
      "        outputs:\n",
      "        - embeddings\n",
      "        - hiddens\n",
      "      data:\n",
      "        num_workers: 4\n",
      "        batch_size: 128\n",
      "        dataset_path: /workspace/bionemo/data/singlecell_tutorial/processed_data\n",
      "        output_fname: /workspace/bionemo/data/singlecell_tutorial/inference_output/human_covid19_bcells_inference.pkl\n",
      "        index_mapping_dir: null\n",
      "        data_fields_map:\n",
      "          sequence: sequence\n",
      "          id: id\n",
      "        data_impl: geneformer\n",
      "        data_impl_kwargs:\n",
      "          csv_fields_mmap:\n",
      "            newline_int: 10\n",
      "            header_lines: 1\n",
      "            workers: null\n",
      "            sort_dataset_paths: false\n",
      "            data_sep: ','\n",
      "            data_fields:\n",
      "              id: 0\n",
      "              sequence: 1\n",
      "          fasta_fields_mmap:\n",
      "            data_fields:\n",
      "              id: 0\n",
      "              sequence: 1\n",
      "        dynamic_padding: true\n",
      "      post_process: false\n",
      "    target: bionemo.model.singlecell.geneformer.model.GeneformerModel\n",
      "    infer_target: bionemo.model.singlecell.geneformer.infer.GeneformerInference\n",
      "    formatters:\n",
      "      simple:\n",
      "        format: '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'\n",
      "    handlers:\n",
      "      console:\n",
      "        class: logging.StreamHandler\n",
      "        formatter: simple\n",
      "        stream: ext://sys.stdout\n",
      "      file:\n",
      "        class: logging.FileHandler\n",
      "        formatter: simple\n",
      "        filename: /logs/inference.log\n",
      "    root:\n",
      "      level: INFO\n",
      "      handlers:\n",
      "      - console\n",
      "    disable_existing_loggers: false\n",
      "    \n",
      "[NeMo I 2024-04-26 22:05:19 utils:333] Restoring model from /workspace/bionemo/data/singlecell_tutorial/inference_output/geneformer/2024-04-26_22-02-05/checkpoints/geneformer.nemo\n",
      "[NeMo I 2024-04-26 22:05:19 utils:337] Loading model class: bionemo.model.singlecell.geneformer.model.GeneformerModel\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo I 2024-04-26 22:05:19 exp_manager:394] Experiments will be logged at /workspace/bionemo/data/singlecell_tutorial/inference_output/geneformer_inference/2024-04-26_22-05-19\n",
      "[NeMo I 2024-04-26 22:05:19 exp_manager:835] TensorboardLogger has been set up\n",
      "[NeMo I 2024-04-26 22:05:19 utils:306] \n",
      "    \n",
      "    ************** Trainer configuration ***********\n",
      "[NeMo I 2024-04-26 22:05:19 utils:307] \n",
      "    name: geneformer_inference\n",
      "    desc: Minimum configuration for initializing a Geneformer model for inference.\n",
      "    trainer:\n",
      "      precision: 16-mixed\n",
      "      devices: 1\n",
      "      num_nodes: 1\n",
      "      accelerator: gpu\n",
      "      logger: false\n",
      "      accumulate_grad_batches: 1\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: /workspace/bionemo/data/singlecell_tutorial/inference_output\n",
      "      name: ${name}\n",
      "      create_checkpoint_callback: false\n",
      "    model:\n",
      "      tokenizer:\n",
      "        vocab_file: nemo:c8fc0333e4d84d84aa99126d970643b1_geneformer.vocab\n",
      "      micro_batch_size: ${model.data.batch_size}\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      use_flash_attention: true\n",
      "      seq_length: 2048\n",
      "      encoder_seq_length: 2048\n",
      "      max_position_embeddings: 2048\n",
      "      num_layers: 6\n",
      "      hidden_size: 256\n",
      "      ffn_hidden_size: 512\n",
      "      num_attention_heads: 4\n",
      "      init_method_std: 0.02\n",
      "      hidden_dropout: 0.1\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: false\n",
      "      bert_binary_head: false\n",
      "      resume_from_checkpoint: null\n",
      "      masked_softmax_fusion: true\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      seed: 1234\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: 1\n",
      "      data:\n",
      "        data_impl: geneformer\n",
      "        probabilistic_dirichlet_sampling_train: false\n",
      "        dataset_path: /workspace/bionemo/data/singlecell_tutorial/processed_data\n",
      "        data_prefix: ''\n",
      "        shuffle: true\n",
      "        dataset: /\n",
      "        medians_file: nemo:1dcbba07821740a69c00535c4bf4dcdc_medians.json\n",
      "        index_mapping_dir: null\n",
      "        skip_warmup: true\n",
      "        index_mapping_type: memmap\n",
      "        train_ratio: 0.98\n",
      "        val_ratio: 0.01\n",
      "        num_workers: 4\n",
      "        dataloader_type: single\n",
      "        seq_length: 2048\n",
      "        seed: 1234\n",
      "        dynamic_padding: true\n",
      "        micro_batch_size: 8\n",
      "        output_fname: /workspace/bionemo/data/singlecell_tutorial/inference_output/human_covid19_bcells_inference.pkl\n",
      "        batch_size: 128\n",
      "        data_fields_map:\n",
      "          sequence: sequence\n",
      "          id: id\n",
      "        data_impl_kwargs:\n",
      "          csv_fields_mmap:\n",
      "            newline_int: 10\n",
      "            header_lines: 1\n",
      "            workers: null\n",
      "            sort_dataset_paths: false\n",
      "            data_sep: ','\n",
      "            data_fields:\n",
      "              id: 0\n",
      "              sequence: 1\n",
      "          fasta_fields_mmap:\n",
      "            data_fields:\n",
      "              id: 0\n",
      "              sequence: 1\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0002\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 2.0\n",
      "          constant_steps: 10.0\n",
      "          max_steps: 200\n",
      "          min_lr: 2.0e-05\n",
      "      precision: bf16-mixed\n",
      "      global_batch_size: 128\n",
      "      target: bionemo.model.singlecell.geneformer.model.GeneformerModel\n",
      "      nemo_version: 1.22.0\n",
      "      downstream_task:\n",
      "        restore_from_path: /workspace/bionemo/data/singlecell_tutorial/inference_output/geneformer/2024-04-26_22-02-05/checkpoints/geneformer.nemo\n",
      "        outputs:\n",
      "        - embeddings\n",
      "        - hiddens\n",
      "    target: bionemo.model.singlecell.geneformer.model.GeneformerModel\n",
      "    infer_target: bionemo.model.singlecell.geneformer.infer.GeneformerInference\n",
      "    formatters:\n",
      "      simple:\n",
      "        format: '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'\n",
      "    handlers:\n",
      "      console:\n",
      "        class: logging.StreamHandler\n",
      "        formatter: simple\n",
      "        stream: ext://sys.stdout\n",
      "      file:\n",
      "        class: logging.FileHandler\n",
      "        formatter: simple\n",
      "        filename: /logs/inference.log\n",
      "    root:\n",
      "      level: INFO\n",
      "      handlers:\n",
      "      - console\n",
      "    disable_existing_loggers: false\n",
      "    \n",
      "[NeMo W 2024-04-26 22:05:19 modelPT:251] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo I 2024-04-26 22:05:19 megatron_init:234] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2024-04-26 22:05:19 megatron_init:237] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2024-04-26 22:05:19 megatron_init:238] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-04-26 22:05:19 megatron_init:246] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-04-26 22:05:19 megatron_init:247] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-04-26 22:05:19 megatron_init:257] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-04-26 22:05:19 megatron_init:261] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-04-26 22:05:19 megatron_init:262] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-04-26 22:05:19 megatron_init:276] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-04-26 22:05:19 megatron_init:288] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-04-26 22:05:19 megatron_init:294] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-04-26 22:05:19 megatron_init:295] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-04-26 22:05:19 megatron_init:296] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-04-26 22:05:19 megatron_init:297] Rank 0 has embedding rank: 0\n",
      "24-04-26 22:05:19 - PID:217988 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 1\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 megatron_base_model:821] The model: GeneformerModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-04-26 22:05:19 modelPT:251] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "[NeMo I 2024-04-26 22:05:19 megatron_base_model:315] Padded vocab_size: 21376, original vocab_size: 21250, dummy tokens: 126.\n",
      "[NeMo I 2024-04-26 22:05:20 nlp_overrides:752] Model GeneformerModel was successfully restored from /workspace/bionemo/data/singlecell_tutorial/inference_output/geneformer/2024-04-26_22-02-05/checkpoints/geneformer.nemo.\n",
      "[NeMo I 2024-04-26 22:05:20 utils:474] DDP is not initialized. Initializing...\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo W 2024-04-26 22:05:20 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
      "      warnings.warn(\"This function is only for unittest\")\n",
      "    \n",
      "[NeMo W 2024-04-26 22:05:20 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/modules/common/megatron/fused_bias_dropout_add.py:70: UserWarning: nvfuser integration in TorchScript is deprecated. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/jit/codegen/cuda/interface.cpp:235.)\n",
      "      return bias_dropout_add_fused_inference_(*args)\n",
      "    \n",
      "[NeMo I 2024-04-26 22:05:21 loading:43] \n",
      "    \n",
      "    ************** Restored model configuration ***********\n",
      "[NeMo I 2024-04-26 22:05:21 loading:44] \n",
      "    tokenizer:\n",
      "      vocab_file: /tmp/tmp0y0663ih/c8fc0333e4d84d84aa99126d970643b1_geneformer.vocab\n",
      "    micro_batch_size: 128\n",
      "    tensor_model_parallel_size: 1\n",
      "    pipeline_model_parallel_size: 1\n",
      "    use_flash_attention: true\n",
      "    seq_length: 2048\n",
      "    encoder_seq_length: 2048\n",
      "    max_position_embeddings: 2048\n",
      "    num_layers: 6\n",
      "    hidden_size: 256\n",
      "    ffn_hidden_size: 512\n",
      "    num_attention_heads: 4\n",
      "    init_method_std: 0.02\n",
      "    hidden_dropout: 0.1\n",
      "    kv_channels: null\n",
      "    apply_query_key_layer_scaling: true\n",
      "    layernorm_epsilon: 1.0e-05\n",
      "    make_vocab_size_divisible_by: 128\n",
      "    pre_process: true\n",
      "    post_process: false\n",
      "    bert_binary_head: false\n",
      "    resume_from_checkpoint: null\n",
      "    masked_softmax_fusion: true\n",
      "    native_amp_init_scale: 4294967296\n",
      "    native_amp_growth_interval: 1000\n",
      "    fp32_residual_connection: false\n",
      "    fp16_lm_cross_entropy: false\n",
      "    seed: 1234\n",
      "    use_cpu_initialization: false\n",
      "    onnx_safe: false\n",
      "    activations_checkpoint_method: null\n",
      "    activations_checkpoint_num_layers: 1\n",
      "    data:\n",
      "      data_impl: geneformer\n",
      "      probabilistic_dirichlet_sampling_train: false\n",
      "      dataset_path: /workspace/bionemo/data/singlecell_tutorial/processed_data\n",
      "      data_prefix: ''\n",
      "      shuffle: true\n",
      "      dataset: /\n",
      "      medians_file: nemo:1dcbba07821740a69c00535c4bf4dcdc_medians.json\n",
      "      index_mapping_dir: null\n",
      "      skip_warmup: true\n",
      "      index_mapping_type: memmap\n",
      "      train_ratio: 0.98\n",
      "      val_ratio: 0.01\n",
      "      num_workers: 4\n",
      "      dataloader_type: single\n",
      "      seq_length: 2048\n",
      "      seed: 1234\n",
      "      dynamic_padding: true\n",
      "      micro_batch_size: 8\n",
      "      output_fname: /workspace/bionemo/data/singlecell_tutorial/inference_output/human_covid19_bcells_inference.pkl\n",
      "      batch_size: 128\n",
      "      data_fields_map:\n",
      "        sequence: sequence\n",
      "        id: id\n",
      "      data_impl_kwargs:\n",
      "        csv_fields_mmap:\n",
      "          newline_int: 10\n",
      "          header_lines: 1\n",
      "          workers: null\n",
      "          sort_dataset_paths: false\n",
      "          data_sep: ','\n",
      "          data_fields:\n",
      "            id: 0\n",
      "            sequence: 1\n",
      "        fasta_fields_mmap:\n",
      "          data_fields:\n",
      "            id: 0\n",
      "            sequence: 1\n",
      "    optim:\n",
      "      name: fused_adam\n",
      "      lr: 0.0002\n",
      "      weight_decay: 0.01\n",
      "      betas:\n",
      "      - 0.9\n",
      "      - 0.98\n",
      "      sched:\n",
      "        name: CosineAnnealing\n",
      "        warmup_steps: 2.0\n",
      "        constant_steps: 10.0\n",
      "        max_steps: 200\n",
      "        min_lr: 2.0e-05\n",
      "    precision: 16-mixed\n",
      "    global_batch_size: 128\n",
      "    target: bionemo.model.singlecell.geneformer.model.GeneformerModel\n",
      "    nemo_version: 1.22.0\n",
      "    downstream_task:\n",
      "      restore_from_path: /workspace/bionemo/data/singlecell_tutorial/inference_output/geneformer/2024-04-26_22-02-05/checkpoints/geneformer.nemo\n",
      "      outputs:\n",
      "      - embeddings\n",
      "      - hiddens\n",
      "    \n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[NeMo W 2024-04-26 22:05:21 nemo_logging:349] /usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "      self.pid = os.fork()\n",
      "    \n",
      "Predicting DataLoader 0: 100%|██████████████████| 19/19 [00:09<00:00,  2.10it/s]\n",
      "[NeMo I 2024-04-26 22:05:31 run_inference:50] Collecting results from all GPUs...\n",
      "[NeMo I 2024-04-26 22:05:31 infer:73] Saving 2313 samples to /workspace/bionemo/data/singlecell_tutorial/inference_output/human_covid19_bcells_inference.pkl\n"
     ]
    }
   ],
   "source": [
    "!python /workspace/bionemo/bionemo/model/infer.py \\\n",
    "  --config-dir /workspace/bionemo/examples/singlecell/geneformer/conf \\\n",
    "  --config-name infer \\\n",
    "  ++model.downstream_task.restore_from_path={pretrained_nemo_file} \\\n",
    "  ++model.data.dataset_path={tutorial_processed_dir} \\\n",
    "  ++exp_manager.exp_dir={tutorial_output_dir} \\\n",
    "  ++model.data.output_fname={tutorial_output_inference_pickle} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b9a425-850d-4027-9160-5e659c163604",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load inference result and cluster with UMAP.\n",
    "Now we will inspect our result. First, we expect there to be one prediction for each cell, we can compare the shape of the anndata object to the predictions produced by our model. After this, we can simply pass our embeddings into umap, and view the result! In this case its a very poorly trained model with very few cells, so keep expectations low!\n",
    "\n",
    "The inference_results pickle file contains one set of hiddens and embeddings for each cell. The hiddens contain the embedding per-token, whereas the embeddings contain the mean embedding for all gene tokens with special tokens (CLS, MASK, etc) removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2895e979-1652-49f2-96e9-b63b66191ada",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2313,\n",
       " (2313, 60664),\n",
       " dict_keys(['text', 'types', 'padding_mask', 'labels', 'loss_mask', 'is_random', 'hiddens', 'embeddings']))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open(tutorial_output_inference_pickle, 'rb') as inference_handle:\n",
    "    inference_results = pickle.load(inference_handle)\n",
    "len(inference_results), adata.shape, inference_results[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "621decfb-2fd1-44dd-86e5-69e7ddcb8235",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2048, 256), (256,))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_results[0]['embeddings'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d80061f4-3311-4246-9640-177d4ca3b9a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import umap\n",
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform([x['embeddings'] for x in inference_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "704d37ad-ac98-4868-9249-bd2ada17a8d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2313, 2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5ea88b8a-bb3c-4058-93a0-9afb0dfedfa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fd81ddfb130>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    s=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafb8266-7451-495b-bdae-9bd51c6a214a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
