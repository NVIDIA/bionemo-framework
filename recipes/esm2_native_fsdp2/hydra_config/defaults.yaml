# Training config
model_name: ???
micro_batch_size: ???
max_seq_length: 1024
data_path: .
num_train_steps: ???

# WandB config
wandb_init_args:
  name: ???

# Optimizer config
adamw_kwargs:
  lr: 4e-4
  fused: true
  betas: [0.9, 0.98]
  eps: 1e-8
  weight_decay: 0.01

# Learning rate scheduler config
lr_scheduler_kwargs:
  num_warmup_steps: 2_000
  num_training_steps: 500_000
