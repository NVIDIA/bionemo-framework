attention_probs_dropout_prob: 0.02
hidden_act: relu
hidden_dropout_prob: 0.02
hidden_size: 256
initializer_range: 0.02
intermediate_size: 512
layer_norm_eps: 1.0e-12
max_position_embeddings: 2048
micro_batch_size: 36
model_type: bert
num_attention_heads: 4
num_hidden_layers: 6
pad_token_id: 0
seq_length: 2048
use_te_layers: false
vocab_size: 25426
