# @package _global_
defaults:
  - /base
  - _self_

# lepton info
node_group: yo-bom-lepton-001
resource_shape: gpu.2xh100-sxm

# recipe identifiers
recipe_subdir: esm2_native_te_mfsdp_thd
model_type: esm2

# wandb
wandb_init_args:
  project: "bionemo_recipes_convergence_lepton_tests_${sanitize:${branch}}"
  group: "${model_type}_${node_group}_${sanitize:${resource_shape}}"
  job_type: "${recipe_subdir}"
  name: "${commit_sha}"

# train specific commands
train_cmd: train
num_train_steps: 100

# configs to run
products:
  # - config: L1_15B_perf_test
  #   micro_batch_size: 4
  # - config: L1_3B_ddp
  #   micro_batch_size: 32
  - config: L1_650M
    wandb_init_args.name: "${config}-${commit_sha}"
    micro_batch_size: 16

# training script to run
train_script: |
  torchrun ${train_cmd}.py \
    --config-name ${config}.yaml \
    num_train_steps=${num_train_steps} \
    micro_batch_size=${micro_batch_size} \
    +wandb_init_args.mode=${wandb_init_args.mode} \
    wandb_init_args.project=${wandb_init_args.project} \
    +wandb_init_args.group=${wandb_init_args.group} \
    +wandb_init_args.job_type=${wandb_init_args.job_type} \
    wandb_init_args.name=${wandb_init_args.name}
