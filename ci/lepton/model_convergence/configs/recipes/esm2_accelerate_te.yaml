# @package _global_
defaults:
  - /base
  - _self_

# lepton info
node_group: yo-bom-lepton-001
num_nodes: 2
device_type: gpu
num_devices: 2
gpu_type: h100-sxm
total_gpus: ${multiply:${num_devices},${num_nodes}}
resource_shape: "${device_type}.${num_devices}x${gpu_type}"

# recipe identifiers
recipe_subdir: esm2_accelerate_te
model_type: esm2

# wandb
wandb_init_args:
  project: "test_convergence__recipes__${sanitize:${branch}}"
  group: "${model_type}__${task_cmd}__${total_gpus}__${sanitize:${gpu_type}}"
  job_type: "${recipe_subdir}"
  name: null

# config overrides
trainer:
  report_to: "wandb"

# train specific commands
task_cmd: train
stop_after_n_steps: 10

# configs to run
products:
  - config: L0_sanity
    wandb_name: "${config}__${now:%Y%m%d-%H%M%S}__${gitsha:}"

# training script to run
run_script: |
  accelerate launch --config_file accelerate_config/default.yaml \
    ${task_cmd}.py \
    --config-name=${config} \
    stop_after_n_steps=${stop_after_n_steps} \
    wandb_init_args.mode=${wandb_init_args.mode} \
    +wandb_init_args.project=${wandb_init_args.project} \
    +wandb_init_args.group=${wandb_init_args.group} \
    +wandb_init_args.job_type=${wandb_init_args.job_type} \
    wandb_init_args.name=${wandb_name}
