# @package _global_
defaults:
  - /base
  - _self_

job_name: esm2-recipes
resource_shape: gpu.2xh200

dashboard_info:
  model: esm2
  variant: recipes
  repo: recipes

# Base configuration
recipe_subdir: esm2_native_te_nvfsdp_thd
branch: main
wandb_init_args:
  mode: "online" # need online to collect logs; if offline have to manually sync after run
train_cmnd: train
num_train_steps: 100

# Run one for each config
products:
  - model_name: esm2_t48_15B_UR50D
    config: L1_15B_perf_test
    micro_batch_size: 4
  - model_name: esm2_t36_3B_UR50D
    config: L1_3B_ddp
    micro_batch_size: 32
  - model_name: esm2_t33_650M_UR50D
    config: L1_650M
    micro_batch_size: 16

script: |
  git clone https://github.com/NVIDIA/bionemo-framework.git
  if [ "${branch}" != "main" ]; then
    cd bionemo-framework
    git checkout "${branch}"
    cd ..
  fi
  cd bionemo-framework/recipes/${recipe_subdir}
  pip install -r requirements.txt
  torchrun ${train_cmnd}.py \
    --config-name ${config}.yaml \
    num_train_steps=${num_train_steps} \
    micro_batch_size=${micro_batch_size} \
    +wandb_init_args.mode=${wandb_init_args.mode}
