# @package _global_
# #########################################################
# Lepton Job Configuration
# If used in script: below, these must be referenced as $var (no brackets)
# environment variables and mounts are passed as follows:
#   environment_variables:
#     - name: WANDB_API_KEY
#       value_from: JWILBER_WANDB_API_KEY
#
#   mounts:
#     - path: /BioNeMo
#       mount_path: /BioNeMo
#       from_: node-nfs:lepton-shared-fs
# #########################################################
defaults:
  - /base
  - _self_

# job name may only have hypens, no underscores
job_name: bionemo-recipe-esm2-t33-650M-UR50D
resource_shape: gpu.2xh200

# #########################################################
# Dashboard Information
# This is data with log details we need for the dashboard dropdowns.
# For example, for recipe
# #########################################################
dashboard_info:
  # model is the core model name; e.g. evo2, esm2, geneformer, etc.
  model: esm2
  # variant is the type of training; e.g. train, finetune, finetune_lora
  variant: recipe
  # config is the model size, optionally suffixed with -recipes, etc.
  config: 8M
  # repo is the repo name; e.g. bionemo2 or recipes.
  repo: recipes

# script specific variables
# these must be wrapped with ${var}
recipe_subdir: esm2_native_te_mfsdp
model_name: esm2_t6_8M_UR50D
num_train_steps: 250
result_dir: wandb_output
wandb_init_args:
  name: "esm2_t6_8M_UR50D_nvfsdp_sanity"
  mode: "online"

# #########################################################
# Script-specific arguments
# If used in script: below, these must be referenced as ${var} (use brackets)
# They can be whatever you want, but should be referenced in the script
# #########################################################
# Note, in this script:
# lepton-specific vars must be $var
# script-specific vars must be ${var}
script: |
  git clone https://github.com/NVIDIA/bionemo-framework.git
  cd bionemo-framework/recipes/${recipe_subdir}
  pip install -r requirements.txt
  torchrun train_mfsdp.py --config-name L0_sanity num_train_steps=${num_train_steps} wandb_init_args.mode=${wandb_init_args.mode}
