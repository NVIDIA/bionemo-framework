# @package _global_
defaults:
  - /base
  - _self_

resource_shape: gpu.2xh200

recipe_subdir: esm2_native_te_mfsdp
train_cmnd: train_fsdp2
num_train_steps: 100

wandb_init_args:
  project: "lepton_recipe_convergence_{branch}"
  group: "{recipe_subdir}_{resource_shape}"

products:
  - config: L1_650M
    resource_shape: gpu.2xh200
  - config: L1_3B
    resource_shape: gpu.2xh200
  # - config: L1_15B_perf_test

train_script: |
  torchrun ${train_cmnd}.py \
    --config-name ${config}.yaml \
    num_train_steps=${num_train_steps}
