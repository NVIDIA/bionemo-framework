# @package _global_
defaults:
  - /base
  - _self_

############################################################
# lepton job info
############################################################
node_group: yo-bom-lepton-001
mount_from: node-nfs:fs1
num_nodes: 8
device_type: gpu
num_devices: 8
gpu_type: h100-sxm
resource_shape: "${device_type}.${num_devices}x${gpu_type}"

############################################################
# kratos info: where to log data
############################################################
kratos_subject: "convergence_tests_fp8_v0.0.3"

############################################################
# recipe identifiers
# mostly used for logging and observability
############################################################
recipe_subdir: esm2_native_te
model_type: esm2
variant: train # train, finetune

# Core identifiers for filtering
framework: native # native, accelerate
precision: bf16 # likely bf16 or fp8
te_enabled: true
# thd_enabled: false

# Catchall for additional features/configs
extras: [] # e.g. [thd]

############################################################
# wandb info (total_gpus used for group name)
############################################################
# `total_gpus` calculated from lepton job info above
total_gpus: ${multiply:${num_devices},${num_nodes}}

wandb_init_args:
  project: "recipes_fp8_${sanitize:${branch}}"
  group: "esm2_650m"
  job_type: "bhsd"
  name: null

############################################################
# task commands
# shared across all products (if not explicitly overridden)
############################################################

# script overrides
# these should match the keys in the recipe's config file
config: L1_650M
num_train_steps: 40_000
num_warmup_steps: 2_000
task_cmd: train_fsdp2
parallelism_strategy: fsdp2
micro_batch_size: 32
thd_enabled: false # bhsd
use_torch_compile: false # already off in defaults.yaml

model_tag: nvidia/esm2_t36_650M_UR50D
# task_cmd: train_fsdp2 # mfsdp
# dataset commands
load_dataset_kwargs_path: nvidia/esm2_uniref_pretraining_data
load_dataset_kwargs_streaming: true
load_dataset_kwargs_revision: 4ac1d2973567e46b8ca95901f4b4793a21305995 # pragma: allowlist secret
num_workers: 1

# checkpoint controls
ckpt_dir: ""
save_checkpoints: false
save_final_model: false
resume_from_checkpoint: false
use_distributed_checkpoint_fsdp2: false

############################################################
# Each product is a different config to run, alongside
# config-specific arguments. Must have a w`andb_name`.
############################################################

products:
  # TE BHSD baseline (bf16) running
  - wandb_name: "esm2_650m__bhsd__baseline${now:%Y%m%d-%H%M%S}__${gitsha:}"
    job_name: "${sanitize:esm2fp8-650m-bhsd-baseline}"
    fp8_enabled: false
    fp8_recipe: transformer_engine.common.recipe.DelayedScaling
    fp8_format: "HYBRID"
    fp8_model_init_kwargs_enabled: false
  # TE BHSD delayed scaling
  - wandb_name: "esm2_650m__bhsd__delayed_scaling${now:%Y%m%d-%H%M%S}__${gitsha:}"
    job_name: "${sanitize:esm2fp8-650m-bhsd-delayed-scaling}"
    fp8_enabled: true
    fp8_recipe: transformer_engine.common.recipe.DelayedScaling
    fp8_format: "HYBRID"
    fp8_model_init_kwargs_enabled: false
  # TE BHSD current scaling
  - wandb_name: "esm2_650m__bhsd__current_scaling${now:%Y%m%d-%H%M%S}__${gitsha:}"
    job_name: "${sanitize:esm2fp8-650m-bhsd-current-scaling}"
    fp8_enabled: true
    fp8_recipe: transformer_engine.common.recipe.Float8CurrentScaling
    fp8_format: "HYBRID"
    fp8_model_init_kwargs_enabled: false
  # TE BHSD block scaling
  - wandb_name: "esm2_650m__bhsd__block_scaling${now:%Y%m%d-%H%M%S}__${gitsha:}"
    job_name: "${sanitize:esm2fp8-650m-bhsd-block-scaling}"
    fp8_enabled: true
    fp8_recipe: transformer_engine.common.recipe.Float8BlockScaling
    fp8_format: "E4M3"
    fp8_model_init_kwargs_enabled: false
  # TE BHSD delayed scaling (fp8 model init)
  - wandb_name: "esm2_650m__bhsd__delayed_scaling_fp8_model_init${now:%Y%m%d-%H%M%S}__${gitsha:}"
    job_name: "${sanitize:esm2fp8-650m-bhsd-delayedscale-init}"
    fp8_enabled: true
    fp8_recipe: transformer_engine.common.recipe.DelayedScaling
    fp8_format: "HYBRID"
    fp8_model_init_kwargs_enabled: true
  # TE BHSD current scaling (fp8 model init)
  - wandb_name: "esm2_650m__bhsd__current_scaling_fp8_model_init${now:%Y%m%d-%H%M%S}__${gitsha:}"
    job_name: "${sanitize:esm2fp8-650m-bhsd-currentscale-init}"
    fp8_enabled: true
    fp8_recipe: transformer_engine.common.recipe.Float8CurrentScaling
    fp8_format: "HYBRID"
    fp8_model_init_kwargs_enabled: true
  # TE BHSD block scaling (fp8 model init)
  - wandb_name: "esm2_650m__bhsd__block_scaling_fp8_model_init${now:%Y%m%d-%H%M%S}__${gitsha:}"
    job_name: "${sanitize:esm2fp8-650m-bhsd-blockscale-init}"
    fp8_enabled: true
    fp8_recipe: transformer_engine.common.recipe.Float8BlockScaling
    fp8_format: "E4M3"
    fp8_model_init_kwargs_enabled: true

############################################################
# run script
# This gets called right after `checkout_script` in the base config.
############################################################
run_script: |
  wget -O init.sh https://raw.githubusercontent.com/leptonai/scripts/main/lepton_env_to_pytorch.sh;
  chmod +x init.sh;
  source init.sh;

  HYDRA_FULL_ERROR=1 torchrun \
    --nnodes=$NNODES \
    --nproc_per_node=$(nvidia-smi --query-gpu=gpu_name --format=csv,noheader | wc -l) \
    --node_rank=$NODE_RANK \
    --master_addr=$MASTER_ADDR \
    --master_port=$MASTER_PORT \
    ${task_cmd}.py \
    --config-name ${config}.yaml \
    wandb_init_args.mode=${wandb_init_args.mode} \
    wandb_init_args.project=${wandb_init_args.project} \
    +wandb_init_args.group=${wandb_init_args.group} \
    +wandb_init_args.job_type=${wandb_init_args.job_type} \
    wandb_init_args.name=${wandb_name} \
    num_train_steps=${num_train_steps} \
    dataset.micro_batch_size=${micro_batch_size} \
    use_sequence_packing=${thd_enabled} \
    dataset.load_dataset_kwargs.path=${load_dataset_kwargs_path} \
    dataset.load_dataset_kwargs.streaming=${load_dataset_kwargs_streaming} \
    +dataset.load_dataset_kwargs.revision=${load_dataset_kwargs_revision} \
    dataset.num_workers=${num_workers} \
    lr_scheduler_kwargs.num_warmup_steps=${num_warmup_steps} \
    checkpoint.ckpt_dir=${ckpt_dir} \
    checkpoint.save_final_model=${save_final_model} \
    checkpoint.resume_from_checkpoint=${resume_from_checkpoint} \
    +checkpoint.save_checkpoints=${save_checkpoints} \
    +checkpoint.use_distributed_checkpoint_fsdp2=${use_distributed_checkpoint_fsdp2} \
    use_torch_compile=${use_torch_compile} \
    fp8_config.enabled=${fp8_enabled} \
    fp8_config.fp8_recipe=${fp8_recipe} \
    fp8_config.fp8_format=${fp8_format} \
    fp8_config.fp8_model_init_kwargs.enabled=${fp8_model_init_kwargs_enabled}
