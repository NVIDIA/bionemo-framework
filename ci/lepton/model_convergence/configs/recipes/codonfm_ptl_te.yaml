# @package _global_
defaults:
  - /base
  - _self_

############################################################
# lepton job info
############################################################
node_group: yo-bom-lepton-001
mount_from: node-nfs:fs1
num_nodes: 1
device_type: gpu
num_devices: 8
gpu_type: h100-sxm
resource_shape: "${device_type}.${num_devices}x${gpu_type}"

job_name: "${sanitize:codonfm_ptl_test}8gpu"
branch: jwilber/codon-fm-ci

############################################################
# Container Runtime
# Defines the base Docker image and registry auth needed
############################################################
container:
  image: nvcr.io/nvidian/cvai_bnmo_trng/codon_fm_te_bnmo_10292025_multinode
  registry_auth: lepton-nvidia-nvcrio-jonathan

############################################################
# kratos info: where to log data
############################################################
kratos_subject: "codonfm_test"

############################################################
# recipe identifiers
# mostly used for logging and observability
############################################################
recipe_subdir: codonfm_ptl_te # don't need
model_type: codonfm
variant: train # train, finetune

# Core identifiers for filtering
framework: ptl # native, accelerate
precision: bf16 # likely bf16 or fp8
te_enabled: true
fp8_enabled: false
thd_enabled: true

# Catchall for additional features/configs
extras: [] # e.g. [thd]

############################################################
# wandb info (total_gpus used for group name)
############################################################
# `total_gpus` calculated from lepton job info above
total_gpus: ${multiply:${num_devices},${num_nodes}}

wandb_init_args:
  project: "test_convergence__recipes__${sanitize:${branch}}"
  group: "${model_type}__${task_cmd}__${total_gpus}gpus__${sanitize:${gpu_type}}"
  job_type: "${recipe_subdir}"
  name: null

############################################################
# task commands
# shared across all products (if not explicitly overridden)
############################################################

# script overrides
# these should match the keys in the recipe's config file

task_cmd: "train"

num_train_steps: 2_000
# dataset commands
micro_batch_size: 32
num_warmup_steps: 2_000

# checkpoint controls
ckpt_dir: ""
save_checkpoints: false
save_final_model: false
resume_from_checkpoint: false
use_distributed_checkpoint_fsdp2: false

# edit these
max_steps: 1_000

############################################################
# Each product is a different config to run, alongside
# config-specific arguments. Must have a w`andb_name`.
############################################################

############################################################
# Checkout Script
# Standardized script to clone the BioNeMo repository and install
# dependencies before the training run starts. Child configs can
# inherit and reuse this logic without modification.
############################################################
checkout_script: |
  echo "pwd: $(pwd)"
  ls /workspace

  echo "python --version"
  python --version
  echo "nvcc --version"
  nvcc --version
  echo "nvidia-smi"
  nvidia-smi
  echo "ls /data/codonfm"
  ls /data/codonfm

  set -euo pipefail

############################################################
# run script
# This gets called right after `checkout_script` in the base config.
############################################################
run_script: |
  cd /workspace/codonfm

  ls /workspace/codonfm

  git clone https://github.com/NVIDIA/bionemo-framework.git
  cd bionemo-framework
  if [ -n "${commit_sha}" ]; then
    echo "Checking out commit: ${commit_sha}"
    git checkout "${commit_sha}"
  elif [ "${branch}" != "main" ]; then
    echo "Checking out branch: ${branch}"
    git checkout "${branch}"
  fi
  cd bionemo-recipes/recipes/${recipe_subdir}
  pip install -e .

  learning_rate=1e-4
  num_nodes=$LEPTON_JOB_TOTAL_WORKERS
  num_gpus=$LEPTON_RESOURCE_ACCELERATOR_NUM
  train_batch_size=32
  val_batch_size=32
  gradient_accumulation_steps=4
  effective_batch_size=$((train_batch_size * num_gpus * num_nodes*gradient_accumulation_steps))
  num_workers=12


  python -m src.runner pretrain \
      --exp_name ci_test_jwilber_oct28_1 \
      --model_name encodon_80m \
      --data_path /data/jomitchell/codonfmnight/ \
      --process_item mlm_memmap \
      --dataset_name CodonMemmapDataset \
      --lr $learning_rate \
      --num_gpus $num_gpus \
      --num_nodes $num_nodes \
      --train_batch_size $train_batch_size \
      --val_batch_size $val_batch_size \
      --num_workers $num_workers \
      --max_steps ${max_steps} \
      --bf16 \
      --split_name_prefix nopathogen \
      --attn_input_format thd \
      --collate_fn thd \
      --project_name jm_codonfm_bionemo_gitlab_test_oct28_1 \
      --entity clara-discovery \
      --gradient_accumulation_steps $gradient_accumulation_steps \
      --use_transformer_engine \
      --enable_wandb
