# @package _global_
defaults:
  - /base
  - _self_

resource_shape: gpu.2xh100-sxm
node_group: yo-bom-lepton-001
recipe_subdir: esm2_native_te
wandb_init_args:
  project: "lepton_recipe_convergence_${branch}"
  group: "${recipe_subdir}_${resource_shape}"

train_cmd: train_fsdp2 #mfsdp
num_train_steps: 100
micro_batch_size: 2

# products:
# - config: L1_15B_perf_test
#   micro_batch_size: 4
# - config: L1_3B
#   micro_batch_size: 32
# - config: L1_650M
#   micro_batch_size: 16

products:
  - config: L0_sanity
#     resource_shape: gpu.2xh200
#   # - config: L1_3B
#   #   resource_shape: gpu.2xh200
#   # - config: L1_15B_perf_test

train_script: |
  torchrun ${train_cmd}.py \
    --config-name ${config}.yaml \
    num_train_steps=${num_train_steps} \
    micro_batch_size=${micro_batch_size}
