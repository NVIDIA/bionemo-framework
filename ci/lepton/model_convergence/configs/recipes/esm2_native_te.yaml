# @package _global_
defaults:
  - /base
  - _self_

############################################################
# lepton job info
############################################################
node_group: yo-bom-lepton-001
num_nodes: 1
device_type: gpu
num_devices: 2
gpu_type: h100-sxm
resource_shape: "${device_type}.${num_devices}x${gpu_type}"
# `total_gpus` calculated from above
total_gpus: ${multiply:${num_devices},${num_nodes}}

############################################################
# recipe identifiers
# mostly used for logging and observability
############################################################
recipe_subdir: esm2_native_te
model_type: esm2
variant: train # train, finetune

# Core identifiers for filtering
framework: native # native, accelerate
parallelism_strategy: fsdp2 # ddp, fsdp2, mfsdp
precision: fp8 # likely bf16 or fp8
te_enabled: true
fp8_enabled: true

# Catchall for additional features/configs
extras: [] # e.g. [thd]

############################################################
# wandb info (total_gpus used for group name)
############################################################
wandb_init_args:
  project: "test_convergence__recipes__${sanitize:${branch}}"
  group: "${model_type}__${task_cmd}__${total_gpus}__${sanitize:${gpu_type}}"
  job_type: "${recipe_subdir}"
  name: null

############################################################
# task commands
# shared across all products (if not explicitly overridden)
############################################################

# script overrides
# these should match the keys in the recipe's config file
model_tag: nvidia/esm2_t36_3B_UR50D
# task_cmd: train_fsdp2 # mfsdp
num_train_steps: 10_000
micro_batch_size: 16
num_warmup_steps: 20_000

# checkpoint controls
ckpt_dir: ""
save_checkpoints: false
save_final_model: false
resume_from_checkpoint: false
use_distributed_checkpoint_fsdp2: false
save_every_n_steps: 50

############################################################
# Each product is a different config to run, alongside
# config-specific arguments. Must have a w`andb_name`.
############################################################
products:
  - config: L1_3B
    task_cmd: train_fsdp2
    wandb_name: "${config}__${now:%Y%m%d-%H%M%S}__${gitsha:}"
  - config: L1_3B
    task_cmd: train_mfsdp
    wandb_name: "${config}__${now:%Y%m%d-%H%M%S}__${gitsha:}"
    micro_batch_size: 2
  - config: L1_3B
    task_cmd: train_ddp
    wandb_name: "${config}__${now:%Y%m%d-%H%M%S}__${gitsha:}"

############################################################
# run script
# This gets called right after `checkout_script` in the base config.
############################################################
run_script: |
  torchrun ${task_cmd}.py \
    --config-name ${config}.yaml \
    +wandb_init_args.mode=${wandb_init_args.mode} \
    wandb_init_args.project=${wandb_init_args.project} \
    +wandb_init_args.group=${wandb_init_args.group} \
    +wandb_init_args.job_type=${wandb_init_args.job_type} \
    wandb_init_args.name=${wandb_name} \
    num_train_steps=${num_train_steps} \
    dataset.micro_batch_size=${micro_batch_size} \
    lr_scheduler_kwargs.num_warmup_steps=${num_warmup_steps} \
    checkpoint.ckpt_dir=${ckpt_dir} \
    checkpoint.save_final_model=${save_final_model} \
    checkpoint.resume_from_checkpoint=${resume_from_checkpoint} \
    checkpoint.save_every_n_steps=${save_every_n_steps} \
    +checkpoint.save_checkpoints=${save_checkpoints} \
    +checkpoint.use_distributed_checkpoint_fsdp2=${use_distributed_checkpoint_fsdp2}
