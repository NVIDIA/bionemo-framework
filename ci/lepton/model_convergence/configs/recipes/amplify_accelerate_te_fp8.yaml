# @package _global_
defaults:
  - /base
  - _self_

resource_shape: gpu.2xh200

# Base configuration
recipe_subdir: amplify_accelerate_te_fp8
branch: jwilber/add-wandb-recipe-amplify
train_cmnd: train

wandb_init_args:
  project: "lepton_recipe_convergence_{branch}"
  group: "{recipe_subdir}_{resource_shape}"

stop_after_n_steps: 10

trainer:
  report_to: "wandb"

# Run one for each config
products:
  - model_name: amplify_120M_sanity
    config: L0_sanity
  - model_name: L1-350M-partial-conv
    config: L1_350M_partial_conv

train_script: |
  torchrun ${train_cmnd}.py \
    --config-name ${config}.yaml \
    trainer.report_to=${trainer.report_to} \
    wandb_init_args.mode=${wandb_init_args.mode} \
    wandb_init_args.project=${wandb_init_args.project} \
    +wandb_init_args.group=${wandb_init_args.group}
