# Lepton Job Configuration
job_name: evo2-finetune
resource_shape: gpu.1xh200
node_group_name: nv-int-multiteam-nebius-h200-01

container:
  image: nvcr.io/nvidia/clara/bionemo-framework:nightly
  registry_auth: lepton-nvidia-cvai-bnmo-trng

environment_variables:
  - name: WANDB_API_KEY
    value: MVLE_WANDB_API_KEY
  
mounts:
  - path: /BioNeMo
    mount_path: /data
    from: node-nfs:lepton-shared-fs
  - path: /BioNeMo/jwilber/bionemo-dev/bionemo-framework/sub-packages/bionemo-evo2
    mount_path: /workspace/bionemo2/sub-packages/bionemo-evo2
    from: node-nfs:lepton-shared-fs

script: |
  #!/bin/bash
  export DATA_PATH=/data/data/evo2
  
  wandb login $WANDB_API_KEY

  torchrun \
      --nnodes 1 \
      --nproc_per_node 1 \
      --node_rank 0 \
      --master_addr localhost \
      --master_port 29400 \
      /workspace/bionemo2/sub-packages/bionemo-evo2/src/bionemo/evo2/run/train.py \
      -d $DATA_PATH/training_data_config.yaml \
      --dataset-dir $DATA_PATH/preprocessed_data \
      --result-dir $DATA_PATH/pretraining_demo \
      --experiment-name evo2 \
      --model-size 1b \
      --devices 1 \
      --num-nodes 1 \
      --seq-length 8192 \
      --micro-batch-size 2 \
      --lr 0.000015 \
      --min-lr 0.0000149 \
      --warmup-steps 10 \
      --grad-acc-batches 4 \
      --max-steps 10 \
      --ckpt-dir $DATA_PATH/nemo2_evo2_1b_8k \
      --clip-grad 250 \
      --wd 0.001 \
      --attention-dropout 0.01 \
      --hidden-dropout 0.01 \
      --val-check-interval 5 \
      --num-layers 4 --hybrid-override-pattern SDH* --activation-checkpoint-recompute-num-layers 2 \
      --create-tensorboard-logger \
      --ckpt-async-save

