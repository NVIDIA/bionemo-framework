scope: partial-conv
time_limit: 14400
key_segments:
  # Modify keys to be renamed (str) or excluded (False) from run identifier. By default, all args under script_args are included.
  data_path: False
  val_check_interval: False
  lr: False
  cosine_rampup_frac: False
  cosine_hold_frac: False
  num_layers: False
  hidden_size: False
  ffn_hidden_size: False
  num_attention_heads: False
script_args:
  # All arguments referenced in the script string must be specified here.
  # Arguments not referenced in the script string must have the 'arg' field specified.
  # See jet/core/configs.py for the specification of the configuration class
  workspace: /workspace/bionemo2
  data_path: /data/cellxgene_scdl
  model: geneformer
  variant: train
  precision: [bf16-mixed]
  gpus: 8
  val_check_interval: 500
  acc_grad: 1
  products:
    - config_name: 10M
      nodes: 2
      batch_size: 32
      max_steps: 400000
      stop_steps: 30000
      lr: 0.001
      cosine_rampup_frac: 0.004331629559040111
      cosine_hold_frac: 0.021658147795200554
      num_layers: 6
      hidden_size: 256
      ffn_hidden_size: 512
      num_attention_heads: 4
    - config_name: 1B
      nodes: 4
      batch_size: 32
      max_steps: 123125
      stop_steps: 30000
      lr: 0.0004
      cosine_hold_frac: 0.05
      cosine_rampup_frac: 0.01
      num_layers: 48
      hidden_size: 1280
      ffn_hidden_size: 5120
      num_attention_heads: 20
script: |-
   WANDB_API_KEY=$BIONEMO_WANDB_API_KEY ${variant}_${model} \
    --data-dir ${data_path} \
    --experiment-name ${batch_size}bs_${nodes}node_${gpus}gpu_${max_steps}s_${precision}prec \
    --num-gpus ${gpus} \
    --save-last-checkpoint \
    --num-nodes ${nodes} \
    --val-check-interval ${val_check_interval} \
    --num-dataset-workers 8 \
    --num-steps ${max_steps} \
    --early-stop-on-step ${stop_steps} \
    --seq-length 2048 \
    --limit-val-batches 8 \
    --micro-batch-size ${batch_size} \
    --resume-if-exists \
    --log-every-n-steps 50 \
    --lr ${lr} \
    --hidden-size ${hidden_size} \
    --num-layers ${num_layers} \
    --ffn-hidden-size ${ffn_hidden_size} \
    --num-attention-heads ${num_attention_heads} \
    --create-tflops-callback \
    --create-tensorboard-logger \
    --result-dir=${tensorboard_dir} \
    --wandb-project ${wandb_project_name} \
    --wandb-job-type=${pipeline_label} \
    --wandb-group=${model}_${variant}_${config_name}__${target} \
    --cosine-rampup-frac {cosine_rampup_frac} \
    --cosine-hold-frac {cosine_hold_frac} \
    --accumulate-grad-batches ${acc_grad} \
    --precision ${precision} \
    --disable-checkpointing;
