scope: partial-conv
time_limit: 14400
key_segments:
  # Modify keys to be renamed (str) or excluded (False) from run identifier. By default, all args under script_args are included.
  train_data_path: False
  valid_data_path: False
  data_base_path: False
  num_workers: False
  limit_val_batches: False
  limit_test_batches: False
  val_check_interval: False
  dataset_class: False
  task_type: False
  config_class: False
  experiment_name: False
  workspace: False
  restore_from_checkpoint_path: False
script_args:
  # All arguments referenced in the script string must be specified here.
  # Arguments not referenced in the script string must have the 'arg' field specified.
  # See jet/core/configs.py for the specification of the configuration class
  workspace: /workspace/bionemo2
  data_base_path: /data/FLIP
  restore_from_checkpoint_path: /data/esm2_650M_nemo2
  nodes: [1]
  gpus: 8
  model: esm2
  variant: finetune
  config_name: 650M
  precision: [bf16-mixed]
  num_workers: 8
  limit_val_batches: 100 # original 1000, 100 is enough for validation and produce good enough curves
  limit_test_batches: 100
  task: seq_classification
  train_data_path: scl/train/x000.csv
  valid_data_path: scl/val/x000.csv
  task_type: classification
  config_class: ESM2FineTuneSeqConfig
  dataset_class: InMemorySingleValueDataset
  max_steps: 30000
  stop_steps: 3000
  experiment_name: seq-level-classification
  batch_size: 64
  val_check_interval: 100
script: |-
  WANDB_API_KEY=$BIONEMO_WANDB_API_KEY ${variant}_${model} \
    --train-data-path=${data_base_path}/${train_data_path} \
    --valid-data-path=${data_base_path}/${valid_data_path} \
    --restore-from-checkpoint-path=${restore_from_checkpoint_path} \
    --task-type=${task_type} \
    --config-class=${config_class} \
    --dataset-class=${dataset_class} \
    --num-steps=${max_steps} \
    --experiment-name=${experiment_name}_${batch_size}bs_${nodes}node_${gpus}gpu_${max_steps}s_${precision}prec \
    --lr=0.0005 \
    --result-dir=${tensorboard_dir} \
    --micro-batch-size=${batch_size} \
    --limit-val-batches=${limit_val_batches} \
    --limit-test-batches=${limit_test_batches} \
    --precision=${precision} \
    --label-column=scl_label \
    --num-gpus=${gpus} \
    --num-nodes=${nodes} \
    --accumulate-grad-batches=2 \
    --val-check-interval=${val_check_interval} \
    --num-dataset-workers=${num_workers} \
    --wandb-project=${wandb_project_name} \
    --wandb-group=${model}_${variant}_${config_name}_${task}_${target} \
    --create-tensorboard-logger \
    --encoder-frozen \
    --mlp-ft-dropout=0.25 \
    --mlp-hidden-size=256 \
    --mlp-target-size=10 \
    --disable-checkpointing \
    --early-stop-on-step=${stop_steps} \
    --create-tflops-callback;
tests:
  - logic_type: static
    logic_spec:
      exit_codes:
        - 0
      baselines:
        consumed_samples:
          operator: eq
          value: 3072000.0
        val_loss:
          operator: range
          max: 0.6678
          min: 0.5630
        reduced_train_loss:
          operator: range
          max: 0.1372
          min: 0.0993
        TFLOPS_per_GPU:
          operator: range
          max: 719.5984
          min: 694.8020
        val_acc:
          operator: range
          max: 0.8462
          min: 0.7833
