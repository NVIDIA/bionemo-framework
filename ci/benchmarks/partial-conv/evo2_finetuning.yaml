scope: partial-conv
time_limit: 14400
key_segments:
  # Modify keys to be renamed (str) or excluded (False) from run identifier. By default, all args under script_args are included.
  dataset_config: False
  dataset_dir: False
  data_base_path: False
  num_workers: False
  limit_val_batches: False
  val_check_interval: False
  experiment_name: False
  workspace: False
  restore_from_checkpoint_path: False
script_args:
  # All arguments referenced in the script string must be specified here.
  # Arguments not referenced in the script string must have the 'arg' field specified.
  # See jet/core/configs.py for the specification of the configuration class
  workspace: /workspace/bionemo2
  data_base_path: /data/evo2
  restore_from_checkpoint_path: /data/nemo2_evo2_1b_8k
  nodes: [1]
  model: evo2
  model_size: 1b
  precision: [bf16-mixed]
  num_workers: 8
  limit_val_batches: 20
  dataset_config: training_data_config.yaml
  dataset_dir: preprocessed_data
  val_check_interval: 50
  seq_length: 8192
  warmup_steps: 100
  activation_checkpoint_layers: 5
  products:
    - variant: finetune
      lora_enabled: false
      gpus: 8
      task: continued_pretraining
      max_steps: 3000
      stop_steps: 300
      experiment_name: dna-finetune
      batch_size: 8
      lr: 1e-5
      min_lr: 1e-6
      accumulate_grad_batches: 2
    - variant: lora_finetune
      lora_enabled: true
      gpus: 4
      task: lora_continued_pretraining
      max_steps: 2000
      stop_steps: 200
      experiment_name: dna-lora-finetune
      batch_size: 16
      lr: 5e-4
      min_lr: 5e-5
      accumulate_grad_batches: 1
script: |-
  WANDB_API_KEY=$BIONEMO_WANDB_API_KEY train_${model} \
    -d ${data_base_path}/${dataset_config} \
    --dataset-dir=${data_base_path}/${dataset_dir} \
    --ckpt-dir=${restore_from_checkpoint_path} \
    --finetune \
    ${ if lora_enabled then "--lora-finetune" else "" } \
    --model-size=${model_size} \
    --max-steps=${max_steps} \
    --experiment-name=${experiment_name}_${batch_size}bs_${nodes}node_${gpus}gpu_${max_steps}s_${precision}prec \
    --lr=${lr} \
    --min-lr=${min_lr} \
    --warmup-steps=${warmup_steps} \
    --result-dir=${tensorboard_dir} \
    --micro-batch-size=${batch_size} \
    --grad-acc-batches=${accumulate_grad_batches} \
    --limit-val-batches=${limit_val_batches} \
    --seq-length=${seq_length} \
    --devices=${gpus} \
    --num-nodes=${nodes} \
    --val-check-interval=${val_check_interval} \
    --wandb-project=${wandb_project_name} \
    --wandb-group=${model}_${variant}_${model_size}_${task}_${target} \
    --create-tensorboard-logger \
    --activation-checkpoint-recompute-num-layers=${activation_checkpoint_layers} \
    --disable-checkpointing \
    --early-stop-on-step=${stop_steps};