defaults:
  - defaults_amplify
  - _self_

model_tag: "nvidia/AMPLIFY_350M"

dataset:
  tokenizer_name: ${model_tag}
  max_seq_length: 512
  # TODO(BIONEMO-2783): Replace this with our ESM-2 parquet dataset when it's ready.
  train_load_dataset_kwargs:
    path: "chandar-lab/UR100P"
    split: "train"
    revision: "refs/convert/parquet"
    streaming: True
  eval_load_dataset_kwargs:
    path: "chandar-lab/UR100P"
    split: "test"
    revision: "refs/convert/parquet"
    streaming: True
  # Whether to truncate the eval dataset; HF Trainer will run the full eval dataset each eval step.
  # If set to an integer, the eval dataset will be truncated to that number of examples.
  truncate_eval_dataset: null

stop_after_n_steps: 20_000
trainer:
  run_name: "L1-350M-partial-conv"
  eval_steps: 1_000
  save_steps: 1_000
  logging_steps: 10
  report_to: "wandb"
  per_device_train_batch_size: 128
  per_device_eval_batch_size: 256
  dataloader_num_workers: 8
