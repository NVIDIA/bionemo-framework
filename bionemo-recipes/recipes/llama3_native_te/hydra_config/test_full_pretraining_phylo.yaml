# Test config for full pretraining dataset with phylogenetic tag masking
# Uses tiny model for quick validation of phylo masking logic
defaults:
  - defaults
  - _self_

model_tag: ./example_checkpoint  # 1B model with TE

# Override for genomic tokenizer
config_kwargs:
  vocab_size: 256
  eos_token_id: 0
  pad_token_id: 1
  bos_token_id: 2
  attn_input_format: "bshd"

num_train_steps: 100  # Quick test

dataset:
  sequence_column: "text"
  tokenizer_path: ${oc.env:PWD}/example_checkpoint
  micro_batch_size: 1
  num_workers: 2
  max_seq_length: 2048  # Shorter for faster testing
  stride: 200
  buffer_size: 10_000
  use_lazy_tokenization: false
  use_stateful_dataloader: false
  # Genomic masking for FULL pretraining dataset (has phylo tags!)
  uppercase_labels: false
  mask_degenerate_bases: true
  mask_phylo_tags: true  # Enable for full pretraining
  load_dataset_kwargs:
    path: "json"
    data_files: "/data/opengenome2/json/pretraining_or_both_phases/*/*.jsonl.gz"  # Full dataset (all splits)
    split: "train"
    streaming: true

adamw_kwargs:
  lr: 5e-4
  fused: true
  betas: [0.9, 0.98]
  eps: 1e-8
  weight_decay: 0.01

lr_scheduler_kwargs:
  num_warmup_steps: 10
  num_training_steps: 100

checkpoint:
  ckpt_dir: null
  save_final_model: false
  resume_from_checkpoint: false

logger:
  frequency: 10

wandb_init_args:
  name: "test-phylo-masking-${now:%Y%m%d_%H%M%S}"
  project: "llama3-genomic-debug"
  mode: "offline"

fp8_config:
  enabled: false
  fp8_recipe: transformer_engine.common.recipe.DelayedScaling
  fp8_format: "HYBRID"
  fp8_recipe_kwargs: {}
  fp8_model_init_kwargs:
    enabled: false

use_meta_device: false
use_torch_compile: false
