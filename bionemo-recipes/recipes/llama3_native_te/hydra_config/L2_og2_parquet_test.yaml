# TEST CONFIG for data ordering verification with parquet data
#
# Based on L2_og2_metagenome_7b_bshd.yaml but using pre-dumped parquet data
# from John's ShardedEdenDataset.
#
# Setup:
#   8 GPUs × 1 mbs × 8 grad_acc = 64 GBS
#   2 optimizer steps = 128 samples needed
#
# Usage:
#   # First, dump 128 samples:
#   python scripts/dump_sharded_eden_as_parquet.py \
#       --sequence-db-dir /data/bcr_eden/OG2_database_splits \
#       --window-db-path /data/bcr_eden/OG2_database_splits/og2__train__short.sqlite \
#       --output-dir /data/parquet_test \
#       --num-samples 128 \
#       --seed 42
#
#   # Then run training:
#   torchrun --nproc_per_node=8 train_fsdp2.py --config-name L2_og2_parquet_test

defaults:
  - defaults
  - _self_

# 7B model config (matching EdenConfig from bionemo-evo2)
config_name_or_path: meta-llama/Llama-3.1-8B
config_kwargs:
  vocab_size: 256  # CRITICAL: Must match nucleotide tokenizer vocab size!
  num_hidden_layers: 32
  hidden_size: 4096
  intermediate_size: 14336
  num_attention_heads: 32
  num_key_value_heads: 32
  max_position_embeddings: 8192
  initializer_range: 0.02
  attn_input_format: bshd
  rope_parameters:
    rope_type: "llama3"
    rope_theta: 500000
    factor: 1
    low_freq_factor: 1
    high_freq_factor: 4
    original_max_position_embeddings: 8192

# ============ CRITICAL FLAGS TO MATCH JOHN'S RUN ============
spike_no_more_embedding_init: true
skip_embedding_weight_decay: false  # John APPLIES weight decay to embeddings
use_weight_decay_grouping: true
use_megatron_scaled_init: true
use_megatron_loss: false
# ============================================================

use_sequence_packing: false

# 8 GPUs × 1 mbs × 8 grad_acc = 64 GBS
grad_acc_steps: 8

# Just 2 steps for testing
num_train_steps: 2

wandb:
  name: parquet_test_8gpu_7b
  project: llama3-data-ordering-test
  mode: disabled  # Disable wandb for testing

# Dataset config - using pre-dumped parquet from John's ShardedEdenDataset
dataset:
  tokenizer_name_or_path: ./tokenizers/nucleotide_fast_tokenizer
  micro_batch_size: 1
  num_workers: 1
  prefetch_factor: 2
  max_seq_length: 8192
  stride: 7992

  load_dataset_kwargs:
    path: parquet
    data_files: /data/parquet_test/*.parquet
    split: train
    streaming: false  # CRITICAL: map-style for DistributedSampler

  shuffle: false  # CRITICAL: preserve John's data order
  skip_tokenization: true  # Data is already tokenized
  use_stateful_dataloader: false
  mask_degenerate_bases: true
  uppercase_labels: false

  # Enable logging to verify batch composition
  log_sample_indices: true
  sample_log_dir: /data/parquet_sample_logs

# Optimizer (matching John's settings)
adamw_kwargs:
  lr: 3e-05
  fused: true
  betas: [0.9, 0.95]
  eps: 1e-8
  weight_decay: 0.1

lr_scheduler_kwargs:
  num_warmup_steps: 0
  num_decay_steps: 2
  min_lr_ratio: 0.02

# No checkpointing for test
checkpoint:
  ckpt_dir: null
  save_final_model: false
  resume_from_checkpoint: false
  save_every_n_steps: 999999
  async_save: false

logger:
  frequency: 1

profiler:
  enabled: false
