# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: LicenseRef-Apache2
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# 7B Llama3 on OpenGenome2 Metagenome - BF16 SANITY CHECK (FP32 master weights, NO FP8)
#
# Purpose: Verify that FP32 master weights + BF16 compute (without any FP8) produces
# a healthy training curve. This is the baseline to compare against FP8 + FP32 runs.
# If this converges well, it confirms that the FP8 recipe (block scaling) is the source
# of any convergence issues, not the master weight / mixed precision setup.
#
# Configuration:
#   - FP32 master weights (model init in FP32, optimizer states in FP32)
#   - BF16 compute via FSDP2 MixedPrecisionPolicy (param_dtype=bf16, reduce_dtype=fp32)
#   - NO FP8 at all (fp8_config.enabled=false)
#   - All other settings match the FP8 run for apples-to-apples comparison
#
# Usage:
#   torchrun --nproc_per_node=8 train_fsdp2.py --config-name L2_og2_metagenome_7b_thd_gqa_bf16_sanity ...

defaults:
  - L2_og2_metagenome_7b
  - _self_

# Override config_kwargs to enable GQA (matching FP8 configs)
config_kwargs:
  vocab_size: 256
  num_hidden_layers: 32
  hidden_size: 4096
  intermediate_size: 14336
  num_attention_heads: 32
  num_key_value_heads: 8  # GQA: 8 KV heads instead of 32 (MHA)
  max_position_embeddings: 8192
  initializer_range: 0.02
  attn_input_format: thd
  rope_theta: 500000
  rope_scaling:
    type: "llama3"
    factor: 1
    low_freq_factor: 1
    high_freq_factor: 4
    original_max_position_embeddings: 8192
  # NO fp8_first_last_bf16 needed — FP8 is completely disabled
  fp8_first_last_bf16: false

# ============ SANITY CHECK SETTINGS ============
# Match all non-FP8 settings from the FP8 run exactly

# Apply weight decay to embeddings (matches FP8 configs)
skip_embedding_weight_decay: false

# Use Megatron-style scaled init for proj/fc2
use_megatron_scaled_init: true

# Disable meta device init
use_meta_device: false

# FP32 master weights enabled — this is the key feature we're testing
# Model created in FP32, FSDP2 MixedPrecisionPolicy casts to BF16 for forward/backward
# Optimizer (AdamW) states stay in FP32 for precision
use_fp32_master_weights: true
# ================================================

# Override mbs — match the FP8 config
dataset:
  micro_batch_size: 1
  buffer_size: 500_000

# Gradient accumulation = 8 (for GBS = 1 * 8 * 6 * 8 = 384)
grad_acc_steps: 8

# FP8 COMPLETELY DISABLED — pure BF16 compute
fp8_config:
  enabled: false
  fp8_recipe: transformer_engine.common.recipe.DelayedScaling
  fp8_format: "HYBRID"
  fp8_recipe_kwargs: {}
  fp8_model_init_kwargs:
    enabled: false

# Enable validation logging
validation:
  enabled: true
  eval_interval: 500
  num_batches: 40
  data_path: /data/opengenome2/json/pretraining_or_both_phases/metagenomes/data_metagenomics_valid_chunk1.jsonl.gz
  micro_batch_size: 8
  max_seq_length: null
  stride: null

logger:
  frequency: 1

wandb:
  name: og2_7b_thd_gqa_bf16_fp32mw_sanity
  project: llama3-metagenome-7b
