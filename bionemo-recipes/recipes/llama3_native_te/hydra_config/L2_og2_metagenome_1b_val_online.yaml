# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: LicenseRef-Apache2
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# 1B Llama3 on OpenGenome2 Metagenome - ONLINE VERSION (loads from HuggingFace)
# Extends L2_og2_metagenome_1b but uses HuggingFace datasets instead of local paths
#
# Usage:
#   torchrun --nproc_per_node=8 train_fsdp2.py --config-name L2_og2_metagenome_1b_val_online ...

defaults:
  - L2_og2_metagenome_1b
  - _self_

# Use HuggingFace model as base (we override all architecture params anyway)
config_name_or_path: meta-llama/Llama-3.1-8B

# FP32 master weights for numerical stability (matches Megatron's main_params_dtype=torch.float32)
use_fp32_master_weights: false

# Override config_kwargs to 1B architecture with old-style RoPE format (transformers <5.0)
config_kwargs:
  vocab_size: 256
  num_hidden_layers: 24
  hidden_size: 2048
  intermediate_size: 5120
  num_attention_heads: 16
  num_key_value_heads: 16
  max_position_embeddings: 8192
  initializer_range: 0.02
  attn_input_format: thd
  # OLD-STYLE RoPE config for transformers <5.0
  rope_theta: 500000
  rope_scaling:
    type: "llama3"
    factor: 1
    low_freq_factor: 1
    high_freq_factor: 4
    original_max_position_embeddings: 8192

# Override dataset to load from HuggingFace instead of local paths
dataset:
  tokenizer_name_or_path: ./tokenizers/nucleotide_fast_tokenizer
  micro_batch_size: 2
  num_workers: 1
  max_seq_length: 8192
  stride: 7992
  buffer_size: 50000
  use_stateful_dataloader: false
  mask_degenerate_bases: true
  uppercase_labels: false
  load_dataset_kwargs:
    path: "arcinstitute/opengenome2"
    data_dir: "json/pretraining_or_both_phases/metagenomes"
    split: "train"
    streaming: true

# Enable validation logging from HuggingFace
validation:
  enabled: true
  eval_interval: 50
  num_batches: 40
  # Load validation from HuggingFace - use the validation split
  data_path: "hf://datasets/arcinstitute/opengenome2/json/pretraining_or_both_phases/metagenomes/data_metagenomics_valid_chunk1.jsonl.gz"
  micro_batch_size: null
  max_seq_length: null
  stride: null

# Settings from your wandb config
spike_no_more_embedding_init: false
skip_embedding_weight_decay: true
use_megatron_scaled_init: true
use_weight_decay_grouping: true

grad_acc_steps: 4
num_train_steps: 10000

checkpoint:
  ckpt_dir: null
  save_every_n_steps: 1000
  resume_from_checkpoint: false
  async_save: false
  save_final_model: true
  max_checkpoints: 5

fp8_config:
  enabled: false

logger:
  frequency: 1

wandb:
  name: og2-1b-online-hf
  project: llama3-metagenome-1b
