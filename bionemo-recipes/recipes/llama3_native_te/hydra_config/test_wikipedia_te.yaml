# Test config: Llama-3.2-1B with TE on Wikipedia (sanity check)
# Purpose: Verify TE training stability on standard text data
defaults:
  - defaults
  - _self_

# Use Meta's Llama-3.2-1B (standard architecture, TE layers)
model_tag: meta-llama/Llama-3.2-1B

# TE-specific config with explicit vocab override
config_kwargs:
  attn_input_format: "bshd"  # Use TE layers
  trust_remote_code: true  # Allow loading llama3_nv.py
  vocab_size: 128256  # Meta's text vocab (not 256 genomic!)
  tie_word_embeddings: false

num_train_steps: 150_000  # Full comparison run

dataset:
  sequence_column: "text"
  tokenizer_path: meta-llama/Llama-3.2-1B  # Use Meta's tokenizer
  micro_batch_size: 4
  num_workers: 2
  max_seq_length: 8192  # Match genomic configs for fair comparison
  stride: 200
  buffer_size: 10_000
  use_lazy_tokenization: false
  use_stateful_dataloader: false
  # Disable genomic masking for text data
  uppercase_labels: false
  mask_degenerate_bases: false  # MUST set to false (default is true!)
  load_dataset_kwargs:
    path: "wikimedia/wikipedia"
    name: "20231101.en"  # English Wikipedia
    split: "train"
    streaming: true

adamw_kwargs:
  lr: 3e-4
  fused: true
  betas: [0.9, 0.95]  # Standard for text
  eps: 1e-8
  weight_decay: 0.1

lr_scheduler_kwargs:
  num_warmup_steps: 2_000
  num_training_steps: 150_000

checkpoint:
  ckpt_dir: null
  save_final_model: false
  resume_from_checkpoint: false

logger:
  frequency: 50

wandb_init_args:
  name: "test-wikipedia-te-${now:%Y%m%d_%H%M%S}"
  project: "llama3-wikipedia-test"
  mode: "offline"
  tags:
    - wikipedia
    - te
    - sanity-check

fp8_config:
  enabled: false

use_meta_device: false
use_torch_compile: false
