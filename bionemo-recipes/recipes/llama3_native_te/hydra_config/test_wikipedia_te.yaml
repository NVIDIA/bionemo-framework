# Test config: Llama-3.2-1B with TE on Wikipedia (sanity check)
# Purpose: Verify TE training stability on standard text data
defaults:
  - defaults
  - _self_

# Use local TE config with Meta's Llama-3.2-1B architecture
model_tag: ./example_checkpoint  # Has TE layers (NVLlama)

# TE-specific config
config_kwargs:
  attn_input_format: "bshd"
  # Will load Meta's pretrained weights but use TE architecture

num_train_steps: 150_000  # Full comparison run

dataset:
  sequence_column: "text"
  tokenizer_path: meta-llama/Llama-3.2-1B  # Use Meta's tokenizer
  micro_batch_size: 4
  num_workers: 2
  max_seq_length: 8192  # Match genomic configs for fair comparison
  stride: 200
  buffer_size: 10_000
  use_lazy_tokenization: false
  use_stateful_dataloader: false
  # Standard collator (no genomic masking parameters on main branch)
  load_dataset_kwargs:
    path: "wikimedia/wikipedia"
    name: "20231101.en"  # English Wikipedia
    split: "train"
    streaming: true

adamw_kwargs:
  lr: 3e-4
  fused: true
  betas: [0.9, 0.95]  # Standard for text
  eps: 1e-8
  weight_decay: 0.1

lr_scheduler_kwargs:
  num_warmup_steps: 2_000
  num_training_steps: 150_000

checkpoint:
  ckpt_dir: null
  save_final_model: false
  resume_from_checkpoint: false

logger:
  frequency: 50

wandb_init_args:
  name: "test-wikipedia-te-${now:%Y%m%d_%H%M%S}"
  project: "llama3-wikipedia-test"
  mode: "offline"
  tags:
    - wikipedia
    - te
    - sanity-check

fp8_config:
  enabled: false

use_meta_device: false
use_torch_compile: false
