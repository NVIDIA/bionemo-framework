# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: LicenseRef-Apache2

# Configuration for training with ShardedEdenDataset directly (no parquet dumping)
# Uses standard PyTorch DataLoader with shuffle=True
# Reads directly from SQLite databases - same as John's Megatron code but without Megatron deps

defaults:
  - defaults
  - _self_

# 7B model config (matching EdenConfig from bionemo-evo2)
config_name_or_path: meta-llama/Llama-3.1-8B
config_kwargs:
  vocab_size: 256  # CRITICAL: Must match nucleotide tokenizer vocab size!
  num_hidden_layers: 32
  hidden_size: 4096
  intermediate_size: 14336
  num_attention_heads: 32
  num_key_value_heads: 32
  max_position_embeddings: 8192
  initializer_range: 0.02
  attn_input_format: bshd
  rope_theta: 500000
  rope_scaling:
    type: "llama3"
    factor: 1
    low_freq_factor: 1
    high_freq_factor: 4
    original_max_position_embeddings: 8192

# ============ CRITICAL FLAGS TO MATCH JOHN'S RUN ============
spike_no_more_embedding_init: true
skip_embedding_weight_decay: false  # Apply weight decay to embeddings
use_weight_decay_grouping: true
use_megatron_scaled_init: true  # Use Megatron-style scaled init for proj/fc2
use_megatron_loss: false
use_meta_device: false
use_fp32_master_weights: false  # Implementation not working, disabled
# ============================================================

use_sequence_packing: false  # BSHD format

# Enable ShardedEden dataset mode
use_sharded_eden: true

# Gradient accumulation
# 6 nodes × 8 GPUs × 1 MBS × 8 GA = 384 GBS (matches John's production)
grad_acc_steps: 8

# Training steps (matching John's Megatron run)
# At GBS=384, one epoch = 238M / 384 = ~620k steps
num_train_steps: 182314

wandb:
  name: og2_7b_sharded_eden_6node
  project: llama3-metagenome-7b
  mode: online

# ShardedEden dataset configuration - reads directly from SQLite
sharded_eden:
  tokenizer_name_or_path: ./tokenizers/nucleotide_fast_tokenizer
  sequence_db_dir: /data/bcr_eden/OG2_database_splits
  window_db_path: /data/bcr_eden/OG2_database_splits/og2__train__short.sqlite
  micro_batch_size: 1
  seq_length: 8192
  stride: 7992
  num_workers: 8  # Increased for production
  shuffle: true
  seed: 42
  rc_aug: false  # Set to true if you want reverse complement augmentation
  log_windows: true
  log_dir: /data/savithas/sharded_eden_logs

# Regular dataset config (needed for validation and PerfLogger serialization)
dataset:
  tokenizer_name_or_path: ./tokenizers/nucleotide_fast_tokenizer
  micro_batch_size: 1
  num_workers: 4
  prefetch_factor: 2
  max_seq_length: 8192
  stride: 7992
  shuffle: false
  skip_tokenization: false
  use_stateful_dataloader: false
  mask_degenerate_bases: true
  uppercase_labels: false
  # Placeholder for PerfLogger (not used when use_sharded_eden=true)
  load_dataset_kwargs:
    path: "unused"
    data_files: "unused"
    split: train
    streaming: false

# Optimizer (matching John's settings)
adamw_kwargs:
  lr: 3e-05
  fused: true
  betas: [0.9, 0.95]
  eps: 1e-8
  weight_decay: 0.1

# Learning rate scheduler (matching John's Megatron run)
# John's args: --lr 3e-05 --min-lr 6e-07 --warmup-steps 2500
lr_scheduler_kwargs:
  num_warmup_steps: 2500  # Match John's --warmup-steps
  num_decay_steps: 179814  # total_steps - warmup = 182314 - 2500
  min_lr_ratio: 0.02  # min_lr / lr = 6e-07 / 3e-05 = 0.02

# Validation settings (uses JSONL dataloader, NOT ShardedEden)
validation:
  enabled: true
  eval_interval: 500  # Run validation every 500 steps (matches John)
  num_batches: 40  # Number of batches per rank (matches John's --limit-val-batches 40)
  data_path: /data/opengenome2/json/pretraining_or_both_phases/metagenomes/data_metagenomics_valid_chunk1.jsonl.gz
  micro_batch_size: 8
  max_seq_length: null
  stride: null

# Checkpoint config
checkpoint:
  ckpt_dir: null  # Set via CLI: checkpoint.ckpt_dir=/path/to/checkpoints
  save_final_model: true
  resume_from_checkpoint: true
  save_every_n_steps: 5000  # Save every 5k steps
  async_save: true

logger:
  frequency: 1

profiler:
  enabled: false
