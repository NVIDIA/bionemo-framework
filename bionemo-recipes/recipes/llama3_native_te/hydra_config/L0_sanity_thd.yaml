# THD (sequence packing) training config for testing
defaults:
  - defaults
  - _self_

# Training config
model_tag: ./example_checkpoint  # Use tiny Llama config for testing
num_train_steps: 250

# Enable sequence packing (THD format)
use_sequence_packing: true

# We want this on in CI/CD to validate that the script runs successfully with torch.compile.
use_torch_compile: false  # Disable for faster startup during testing

dataset:
  tokenizer_path: ./example_checkpoint
  micro_batch_size: 2  # Pack ~2 sequences per batch
  num_workers: 1
  max_seq_length: 1024
  stride: 100
  buffer_size: 10_000
  use_stateful_dataloader: false
  sequence_column: "sequence"
  # Genomic masking options
  uppercase_labels: true  # Uppercase labels while keeping inputs mixed case
  mask_degenerate_bases: true  # Mask N, R, Y, @, # etc.
  mask_phylo_tags: false  # Disable for Milestone 1 (metagenomics)
  load_dataset_kwargs:
    path: "parquet"
    split: "train"
    data_files: "test_genomic_sequences.parquet"
    streaming: true  # REQUIRED for THD

# WandB config
wandb_init_args:
  name: "llama3_8B_genomic_sanity_thd"
  mode: "offline"
  project: null

# Learning rate scheduler config
lr_scheduler_kwargs:
  num_warmup_steps: 10
  num_training_steps: 250

checkpoint:
  ckpt_dir: null
  resume_from_checkpoint: false
  save_every_n_steps: 50
  save_final_model: false

logger:
  frequency: 1
