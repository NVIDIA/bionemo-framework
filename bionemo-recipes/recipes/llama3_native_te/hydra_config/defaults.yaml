# Training config
use_te: true # Whether to use TransformerEngine layers through NVLlamaForCausalLM (if false, use HF's LlamaForCausalLM)
config_name_or_path: ??? # E.g., meta-llama/Llama-3.2-1B or ./model_configs/meta-llama/Llama-3.2-1B

# Spike-No-More embedding initialization (https://arxiv.org/abs/2312.16903)
# When enabled, embeddings are initialized with std=1.0 instead of 0.02.
# This helps prevent loss spikes during training of large models.
# Note: This automatically ensures embeddings are NOT tied with output weights.
spike_no_more_embedding_init: false

# Skip weight decay on embedding layers (useful with spike_no_more_embedding_init)
# See https://arxiv.org/abs/2312.16903 - helps prevent embeddings from shrinking to zero
skip_embedding_weight_decay: false

# Use Megatron-style scaled initialization for residual output layers (proj, fc2)
# When true: proj and fc2 use std/sqrt(2*num_layers) (~0.0025 for 32 layers)
# When false: all layers use regular std (0.02) - this is TE's default
# Note: Megatron uses scaled init, but TE was designed with uniform init.
# If you see loss increasing after initial decrease, try setting this to false.
use_megatron_scaled_init: false

# Use weight decay grouping (skip decay on bias and 1D LayerNorm params)
# When true: bias and 1D params get weight_decay=0, 2D weights get weight_decay from config
# When false: all params get the same weight_decay (original behavior)
# Megatron uses grouping, but your baseline might not have used it.
use_weight_decay_grouping: true

config_kwargs: {}
# When spike_no_more_embedding_init is true, embedding_init_std=1.0 is added to config_kwargs automatically

num_train_steps: ???
grad_acc_steps: 1  # Gradient accumulation steps - effective batch = micro_batch_size * num_gpus * grad_acc_steps

# Use distributed gradient clipping for FSDP2 (all-reduce squared norms before clipping)
# When true: Computes true global gradient norm across all FSDP2 shards via all-reduce
# When false: Uses standard torch.nn.utils.clip_grad_norm_ (local shard only - may be incorrect for FSDP2)
# Set to true to match Megatron's global gradient norm computation.
use_distributed_grad_clip: true

# Loss scaling factor to match Megatron gradient magnitudes.
# FSDP2 averages gradients across ALL ranks, while Megatron with TP only averages across DP ranks.
# This causes FSDP gradients to be smaller by a factor of (world_size / dp_size).
# Example: 48 GPUs with TP=4 → FSDP averages across 48, Megatron DP averages across 12 → 4x diff
# Combined with grad_acc differences, you may need loss_scale of 4-16 to match gradient norms.
# Use this to tune gradient magnitudes to match a reference run.
loss_scale: 1.0

use_meta_device: true

# Whether to wrap the model in torch.compile. Note, this is currently not supported with mfsdp (BIONEMO-2977).
# We leave this off by default since we don't see much of a performance improvement with TE layers.
use_torch_compile: false

# Whether to use gradient checkpointing to trade compute for memory
use_gradient_checkpointing: false

use_sequence_packing: false

dataset:
  tokenizer_name_or_path: ??? # Set to the path of your tokenizer (e.g.,  meta-llama/Llama-3.1-8B or ./tokenizers/nucleotide_fast_tokenizer)
  micro_batch_size: 8
  num_workers: 1
  max_seq_length: 8192 # Window size for genomic sequences
  stride: 200 # Overlap for windowing
  buffer_size: 500_000 # Shuffle buffer size
  use_lazy_tokenization: true
  use_stateful_dataloader: false # Until https://github.com/pytorch/pytorch/pull/163102 is resolved with torchdata.
  # Pre-shift labels for causal LM to avoid cross-boundary predictions in packed sequences.
  # When True (Megatron-style): labels are shifted so the last token of each sequence predicts -100
  # instead of the next sequence's first token.
  # When False (default): use original HuggingFace behavior (global shift in ForCausalLMLoss).
  # Note: This only matters for heavily packed sequences. With mbs=1-2 and long sequences,
  # the difference is negligible (<0.01% of tokens affected).
  shift_labels_for_causal_lm: false
  load_dataset_kwargs:
    path: ???
    split: "train"
    streaming: True

# WandB config
wandb:
  name: ???
  project: null # Optional: set to your wandb project name

# TransformerEngine FP8 config. See
# https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html for more information on
# supported formats.
fp8_config:
  enabled: false
  fp8_recipe: transformer_engine.common.recipe.DelayedScaling
  fp8_format: "HYBRID"
  fp8_recipe_kwargs: {}
  fp8_model_init_kwargs:
    enabled: false # If this is set to true, fp8_config.enabled must also be set to true.

# Optimizer config
adamw_kwargs:
  lr: 3e-3
  fused: true
  betas: [0.9, 0.95]
  eps: 1e-5
  weight_decay: 0.1

# Learning rate scheduler config
# Supports Megatron-style cosine annealing with warmup + constant (plateau) + decay
lr_scheduler_kwargs:
  num_warmup_steps: 2_000
  num_constant_steps: 0  # Plateau at max_lr after warmup (set to 1024 to match Megatron runs)
  num_decay_steps: 498_000
  min_lr_ratio: 0.000001

# Checkpoint config
checkpoint:
  ckpt_dir: ???
  save_final_model: true
  resume_from_checkpoint: true
  save_every_n_steps: 50
  max_checkpoints: 5 # Keep only the latest 5 checkpoints
  async_save: true # Whether to save the checkpoint asynchronously, currently only supported with FSDP2.

logger:
  frequency: 100

profiler:
  enabled: false
  schedule:
    wait: 10
    warmup: 10
    active: 3
    repeat: 1
