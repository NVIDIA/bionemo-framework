# Training config
use_te: true # Whether to use TransformerEngine layers through NVLlamaForCausalLM (if false, use HF's LlamaForCausalLM)
config_name_or_path: ??? # E.g., meta-llama/Llama-3.2-1B or ./model_configs/meta-llama/Llama-3.2-1B

# Initialization flags (CRITICAL: these control the three key features)
spike_no_more_embedding_init: false  # Spike-No-More: use std=1.0 for embeddings (https://arxiv.org/abs/2312.16903)
use_megatron_scaled_init: false  # Megatron-style: use scaled init for proj/fc2 (std/sqrt(2*n_layers))

# Weight decay flags (CRITICAL: controls 1D param weight decay skip)
use_weight_decay_grouping: true  # Skip weight decay on bias and 1D params (LayerNorm/RMSNorm)
skip_embedding_weight_decay: false  # Skip weight decay on embedding layers

# Loss computation
use_megatron_loss: false  # Use Megatron-style per-token loss reduction (for packed sequences)

config_kwargs:
  fp8_first_last_bf16: false  # Keep first/last transformer layers in bf16 for FP8 stability
# When spike_no_more_embedding_init is true, embedding_init_std=1.0 is added to config_kwargs automatically

num_train_steps: ???
grad_acc_steps: 1 # Gradient accumulation steps - effective batch = micro_batch_size * num_gpus * grad_acc_steps

# Random seed for reproducibility (same on all ranks for FSDP2/DTensor)
seed: 42

use_meta_device: true

# Whether to wrap the model in torch.compile. Note, this is currently not supported with mfsdp (BIONEMO-2977).
# We leave this off by default since we don't see much of a performance improvement with TE layers.
use_torch_compile: false

use_sequence_packing: false

# ---- Data source selection ----
# Set use_sharded_eden to true to use pre-computed SQLite window databases
# (same data format as John's Evo2 training).  When false, the standard
# HuggingFace streaming dataset is used.
use_sharded_eden: false

dataset:
  tokenizer_name_or_path: ??? # Set to the path of your tokenizer (e.g.,  meta-llama/Llama-3.1-8B or ./tokenizers/nucleotide_fast_tokenizer)
  micro_batch_size: 8
  num_workers: 4
  max_seq_length: 8192 # Window size for genomic sequences
  stride: 200 # Overlap for windowing
  buffer_size: 50_000 # Shuffle buffer size for sequence/window shuffling
  shuffle_sequences: true # Shuffle raw sequences before windowing (fast, recommended)
  shuffle_windows: false # Shuffle windows after tokenization (uses buffer_size, can be slow/memory-heavy)
  # Interleaved shuffle: pseudo-global window coverage without massive memory cost
  interleaved_shuffle: false # Use interleaved shuffling (overrides shuffle_windows)
  interleave_chunks: 10 # Number of stream positions to interleave from
  interleave_skip: 50_000 # Windows to skip between chunk starting positions (10 chunks Ã— 50k = 500k coverage)
  use_stateful_dataloader: false # Until https://github.com/pytorch/pytorch/pull/163102 is resolved with torchdata.
  pad_sequences_to_be_divisible_by: null
  load_dataset_kwargs:
    path: ???
    split: "train"
    streaming: True

# ---- Sharded Eden data config (used only when use_sharded_eden=true) ----
sharded_eden:
  sequence_db_dir: null  # Directory containing per-sample SQLite databases
  train_window_db: null  # Path to the pre-computed training window database
  val_window_db: null    # Path to the pre-computed validation window database
  test_window_db: null   # Path to the pre-computed test window database
  stride: 7992           # Window stride in bases (must match window DB metadata)
  rc_aug: false          # Whether to apply reverse complement augmentation

# WandB config
wandb:
  name: ???
  project: null # Optional: set to your wandb project name

# TransformerEngine FP8 config. See
# https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html for more information on
# supported formats.
fp8_config:
  enabled: false
  fp8_recipe: transformer_engine.common.recipe.DelayedScaling
  fp8_format: "HYBRID"
  fp8_recipe_kwargs: {}
  fp8_model_init_kwargs:
    enabled: false # If this is set to true, fp8_config.enabled must also be set to true.

# FP32 Master Weights
# When enabled, model is initialized in FP32 with MixedPrecisionPolicy (param_dtype=bf16, reduce_dtype=fp32).
# FP8 layers use FP8 via TE's fp8_autocast, non-FP8 ops run in BF16.
use_fp32_master_weights: false

# Optimizer config
adamw_kwargs:
  lr: 3e-3
  fused: true
  betas: [0.9, 0.95]
  eps: 1e-5
  weight_decay: 0.1

# Learning rate scheduler config
lr_scheduler_kwargs:
  num_warmup_steps: 2_000
  num_decay_steps: 498_000
  min_lr_ratio: 0.000001

# Checkpoint config
checkpoint:
  ckpt_dir: ???
  save_final_model: true
  resume_from_checkpoint: true
  save_every_n_steps: 50
  max_checkpoints: 5 # Keep only the latest 5 checkpoints
  async_save: true # Whether to save the checkpoint asynchronously, currently only supported with FSDP2.

logger:
  frequency: 100

# Validation config
validation:
  enabled: false  # Set to true to run periodic validation
  eval_interval: 500  # Run validation every N steps
  num_batches: 10  # Number of validation batches to evaluate
  data_path: null  # Path to validation data

fp8_stats_config:
  enabled: false
  fp8_stats_file: ./fp8_debugging_stats.yaml
  fp8_log_dir: ./log_fp8_stats

profiler:
  enabled: false
  schedule:
    wait: 10
    warmup: 10
    active: 3
    repeat: 1
