# Training config
use_te: true # Whether to use TransformerEngine layers through NVLlamaForCausalLM (if false, use HF's LlamaForCausalLM)
config_name_or_path: ??? # E.g., meta-llama/Llama-3.2-1B or ./model_configs/meta-llama/Llama-3.2-1B

# Random seed for reproducibility (John uses 42)
# Each rank gets seed + rank for per-rank uniqueness
seed: 42

# Spike-No-More embedding initialization (https://arxiv.org/abs/2312.16903)
# When enabled, embeddings are initialized with std=1.0 instead of 0.02.
# This helps prevent loss spikes during training of large models.
# Note: This automatically ensures embeddings are NOT tied with output weights.
spike_no_more_embedding_init: false

# Skip weight decay on embedding layers (useful with spike_no_more_embedding_init)
# See https://arxiv.org/abs/2312.16903 - helps prevent embeddings from shrinking to zero
skip_embedding_weight_decay: false

# Use Megatron-style scaled initialization for residual output layers (proj, fc2)
# When true: proj and fc2 use std/sqrt(2*num_layers) (~0.0025 for 32 layers)
# When false: all layers use regular std (0.02) - this is TE's default
# Note: Megatron uses scaled init, but TE was designed with uniform init.
# If you see loss increasing after initial decrease, try setting this to false.
use_megatron_scaled_init: false

# Use weight decay grouping (skip decay on bias and 1D LayerNorm params)
# When true: bias and 1D params get weight_decay=0, 2D weights get weight_decay from config
# When false: all params get the same weight_decay (original behavior)
# Megatron uses grouping, but your baseline might not have used it.
use_weight_decay_grouping: true

# ============ LOSS CALCULATION ============
# Use Megatron-style per-token loss reduction (matches --no-calculate-per-token-loss behavior)
# When true: loss = sum(per_token_losses) / total_valid_tokens across all microbatches
# When false: use HuggingFace-style reduction='mean' (default PyTorch CrossEntropyLoss)
# This affects how losses are averaged with gradient accumulation and can cause 0.03-0.07 loss difference.
# Set to true to match John's Megatron runs more closely.
use_megatron_loss_reduction: false

# ============ LOGGING ============
# Log Token Embedding Variance (TEV) statistics to wandb
# TEV tracks how embedding variance changes during training (matches John's tev_mean/tev_sd metrics)
# Useful for monitoring Spike-No-More initialization and embedding stability.
log_tev: true

# Log initialization statistics after model init (useful for debugging init differences)
# Logs mean/std of key layers (embed_tokens, lm_head, proj, fc2) at step 0
log_init_stats: true

config_kwargs: {}
# When spike_no_more_embedding_init is true, embedding_init_std=1.0 is added to config_kwargs automatically

num_train_steps: ???
grad_acc_steps: 1  # Gradient accumulation steps - effective batch = micro_batch_size * num_gpus * grad_acc_steps

use_meta_device: true

# Whether to wrap the model in torch.compile. Note, this is currently not supported with mfsdp (BIONEMO-2977).
# We leave this off by default since we don't see much of a performance improvement with TE layers.
use_torch_compile: false

# Whether to use gradient checkpointing to trade compute for memory
use_gradient_checkpointing: false

use_sequence_packing: false

dataset:
  tokenizer_name_or_path: ??? # Set to the path of your tokenizer (e.g.,  meta-llama/Llama-3.1-8B or ./tokenizers/nucleotide_fast_tokenizer)
  micro_batch_size: 8
  num_workers: 1
  max_seq_length: 8192 # Window size for genomic sequences
  stride: 200 # Overlap for windowing
  buffer_size: 500_000 # Shuffle buffer size
  use_lazy_tokenization: true
  use_stateful_dataloader: false # Until https://github.com/pytorch/pytorch/pull/163102 is resolved with torchdata.
  # Pre-shift labels for causal LM to avoid cross-boundary predictions in packed sequences.
  # When True (Megatron-style): labels are shifted so the last token of each sequence predicts -100
  # instead of the next sequence's first token.
  # When False (default): use original HuggingFace behavior (global shift in ForCausalLMLoss).
  # Note: This only matters for heavily packed sequences. With mbs=1-2 and long sequences,
  # the difference is negligible (<0.01% of tokens affected).
  shift_labels_for_causal_lm: false
  load_dataset_kwargs:
    path: ???
    split: "train"
    streaming: True

# WandB config
wandb:
  name: ???
  project: null # Optional: set to your wandb project name

# TransformerEngine FP8 config. See
# https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html for more information on
# supported formats.
fp8_config:
  enabled: false
  fp8_recipe: transformer_engine.common.recipe.DelayedScaling
  fp8_format: "HYBRID"
  fp8_recipe_kwargs: {}
  fp8_model_init_kwargs:
    enabled: false # If this is set to true, fp8_config.enabled must also be set to true.

# Optimizer config
adamw_kwargs:
  lr: 3e-3
  fused: true
  betas: [0.9, 0.95]
  eps: 1e-5
  weight_decay: 0.1

# Learning rate scheduler config
# Supports Megatron-style cosine annealing with warmup + constant (plateau) + decay
lr_scheduler_kwargs:
  num_warmup_steps: 2_000
  num_constant_steps: 0  # Plateau at max_lr after warmup (set to 1024 to match Megatron runs)
  num_decay_steps: 498_000
  min_lr_ratio: 0.01  # min_lr = max_lr * min_lr_ratio (e.g., 0.02 for min_lr=6e-7 with lr=3e-5)

# Checkpoint config
checkpoint:
  ckpt_dir: ???
  save_final_model: true
  resume_from_checkpoint: true
  save_every_n_steps: 50
  max_checkpoints: 5 # Keep only the latest 5 checkpoints
  async_save: true # Whether to save the checkpoint asynchronously, currently only supported with FSDP2.

logger:
  frequency: 1  # Log every step for debugging (set higher for long runs)

# Validation config (for tracking val loss during training like John's WandB curves)
validation:
  enabled: false  # Set to true to run periodic validation
  eval_interval: 500  # Run validation every N steps
  num_batches: 40  # Number of batches to evaluate (matches John's --limit-val-batches 40)
  data_path: null  # Path to validation data (e.g., /data/opengenome2/.../data_metagenomics_valid_chunk1.jsonl.gz)
  # Optional overrides (defaults to training config values if null)
  micro_batch_size: null  # Override validation batch size
  max_seq_length: null  # Override validation sequence length
  stride: null  # Override validation stride

profiler:
  enabled: false
  schedule:
    wait: 10
    warmup: 10
    active: 3
    repeat: 1
