# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: LicenseRef-Apache2
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Configuration for debugging with pre-dumped tensors from Megatron
# This guarantees EXACT data matching with John's training
#
# Setup:
#   1. Run John's training with --dump-batches --dump-batches-dir /data/savithas/megatron_batches
#   2. Use this config to consume the dumped tensors
#
# Usage:
#   torchrun --nproc_per_node=8 train_fsdp2.py --config-name L2_og2_tensor_debug

defaults:
  - L2_og2_metagenome_7b
  - _self_

# Override config_kwargs for BSHD format (not THD packing)
config_kwargs:
  vocab_size: 256
  num_hidden_layers: 32
  hidden_size: 4096
  intermediate_size: 14336
  num_attention_heads: 32
  num_key_value_heads: 8  # GQA: 8 KV heads (matching John's training)
  max_position_embeddings: 8192
  initializer_range: 0.02
  attn_input_format: bshd  # BSHD format for tensor dataset
  rope_theta: 500000
  rope_scaling:
    type: "llama3"
    factor: 1
    low_freq_factor: 1
    high_freq_factor: 4
    original_max_position_embeddings: 8192

# ============ UPDATED SETTINGS ============
skip_embedding_weight_decay: false
use_megatron_scaled_init: true
use_meta_device: false
use_fp32_master_weights: false
# ==========================================

# Disable sequence packing (BSHD format)
use_sequence_packing: false

# ============ TENSOR DATASET MODE ============
use_tensor_dataset: true
tensor_dir: /data/savithas/megatron_batches  # Directory with dumped .pt files
# =============================================

# Dataset config (MBS=1 for tensor dataset)
dataset:
  micro_batch_size: 1
  num_workers: 4

# Gradient accumulation
# For 1 node: 8 GPUs × 1 MBS × 8 GA = 64 GBS
grad_acc_steps: 8

# Training steps
num_train_steps: 10000

# Enable validation logging (matches John's val/loss curve)
validation:
  enabled: true
  eval_interval: 1000
  num_batches: 40
  data_path: /data/opengenome2/json/pretraining_or_both_phases/metagenomes/data_metagenomics_valid_chunk1.jsonl.gz
  micro_batch_size: 8
  max_seq_length: null
  stride: null

logger:
  frequency: 1

wandb:
  name: og2_7b_tensor_debug
  project: llama3-metagenome-7b

# Checkpointing
checkpoint:
  save_every_n_steps: 1000
  save_final_model: true
  resume_from_checkpoint: true
  ckpt_dir: /data/savithas/tensor_debug_checkpoints
  async_save: true
