# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: LicenseRef-Apache2
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# 7B Llama3 on OpenGenome2 Metagenome WITH VALIDATION + 2x LEARNING RATE
# Extends L2_og2_metagenome_7b_val with 2x LR (6e-05 instead of 3e-05)
#
# Hypothesis: Higher LR may help close early training gap with John's run
#
# Usage:
#   torchrun --nproc_per_node=8 train_fsdp2.py --config-name L2_og2_metagenome_7b_val_2x_lr ...

defaults:
  - L2_og2_metagenome_7b_val
  - _self_

# ============ 2x LEARNING RATE ============
adamw_kwargs:
  lr: 6e-05  # 2x John's LR (was 3e-05)
  fused: true
  betas: [0.9, 0.95]
  eps: 1e-8
  weight_decay: 0.1

# LR scheduler - 2x peak LR, decay to same min_lr as John (6e-07)
# min_lr_ratio = 6e-07 / 6e-05 = 0.01
lr_scheduler_kwargs:
  num_warmup_steps: 2500
  num_decay_steps: 179814  # 182314 - 2500
  min_lr_ratio: 0.01  # Decay to John's min_lr (6e-07)

wandb:
  name: og2_7b_val_2x_lr_6node
  project: llama3-metagenome-7b
