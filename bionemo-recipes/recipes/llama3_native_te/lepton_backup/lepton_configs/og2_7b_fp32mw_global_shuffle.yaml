# OpenGenome2 Metagenome 7B - THD + BF16 + FP32 master weights + Globally Shuffled Shards
# PARQUET_SPLIT DATASET: pre-chunked 8190bp windows at /data/opengenome2/parquet_split/
# Standard windowed tokenization (each 8190bp sequence = 1 window)
# 8 workers, 10k buffer
# Fresh run from scratch
# GBS = 1 * 8 * 6 * 8 = 384
defaults:
  - _self_

job_name: "og2-7b-fp32mw-global-shuffle"
node_group: "yo-bom-lepton-001"
resource_shape: "gpu.8xh100-sxm"

num_nodes: 6
gpus_per_node: 8
num_train_steps: 182314
micro_batch_size: 1
grad_acc_steps: 8

dataset_path: "/data/opengenome2/parquet_split"
data_dir: ""
num_workers: 8
buffer_size: 10000

shuffle_windows: false
shuffle_sequences: true

repo_root: "/data/savithas/bionemo-framework"
code_path: "/data/savithas/bionemo-framework/bionemo-recipes/recipes/llama3_native_te"
train_script: "train_fsdp2.py"
hydra_config: "L2_og2_metagenome_7b_thd_gqa_global_shuffle"

git_branch: "savitha/hf-model-globally-shuffled-shards"

validation_enabled: false

spike_no_more_embedding_init: true
skip_embedding_weight_decay: false
use_megatron_scaled_init: true
use_weight_decay_grouping: true
use_meta_device: false

fp8_enabled: false
use_fp32_master_weights: true

logger_frequency: 10

checkpoint_dir: "/data/savithas/checkpoints/og2-7b-fp32mw-global-shuffle"  # pragma: allowlist secret
save_every_n_steps: 5000
async_save: false

wandb_project: "llama3-metagenome-7b"
wandb_name: "og2-7b-fp32mw-global-shuffle"
wandb_secret: "wandb.savithas"  # pragma: allowlist secret

hf_secret: "HUGGING_FACE_HUB_TOKEN.savithas"  # pragma: allowlist secret

exclude_nodes:
  - node-ip-10-50-80-195
  - node-ip-10-50-81-231
  - nvidia-lepton093
  - nvidia-lepton007

container:
  image: "nvcr.io/nvidia/pytorch:25.11-py3"
  registry_auth: "lepton-nvidia-cvai-bnmo-trng"
