# DIAGNOSTIC: HF Window Shuffle — 1 worker, 50k buffer
# 2000 steps, 6 nodes, THD + GQA, BF16 + FP32 master weights, NO FP8
# 1 worker, 50k buffer, window shuffle ON, sequence shuffle OFF
# Branch: savitha/dataloader-debug-diagnostics
defaults:
  - _self_

job_name: "og2-7b-diag-hf-ws-w1-b50k"
node_group: "yo-bom-lepton-001"
resource_shape: "gpu.8xh100-sxm"

num_nodes: 6
gpus_per_node: 8
num_train_steps: 10000
micro_batch_size: 1
grad_acc_steps: 8

# HF streaming parquet dataset — MUST be full path, not just "parquet"
dataset_path: "/data/opengenome2/parquet"
data_dir: ""
num_workers: 1
buffer_size: 50000

# Window shuffle ONLY
shuffle_windows: true
shuffle_sequences: false

# Code and paths
repo_root: "/data/savithas/bionemo-framework"
code_path: "/data/savithas/bionemo-framework/bionemo-recipes/recipes/llama3_native_te"
train_script: "train_fsdp2.py"
hydra_config: "L2_og2_metagenome_7b_diag_hf_ws_w1_b50k"

# Git branch
git_branch: "savitha/dataloader-debug-diagnostics"

# Validation enabled
validation_enabled: false
validation_interval: 500

# Init settings
spike_no_more_embedding_init: true
skip_embedding_weight_decay: false
use_megatron_scaled_init: true
use_weight_decay_grouping: true
use_meta_device: false

# FP8 DISABLED
fp8_enabled: false

# FP32 Master Weights
use_fp32_master_weights: true

# Log every step
logger_frequency: 1

# Checkpointing
checkpoint_dir: "/data/savithas/checkpoints/og2-7b-diag-hf-ws-w1-b50k"  # pragma: allowlist secret
save_every_n_steps: 5000
async_save: false
resume_from_checkpoint: true

# WandB
wandb_project: "llama3-metagenome-7b"
wandb_name: "og2-7b-diag-hf-ws-w1-b50k"
wandb_secret: "wandb.savithas"  # pragma: allowlist secret

# HuggingFace token
hf_secret: "HUGGING_FACE_HUB_TOKEN.savithas"  # pragma: allowlist secret

# Nodes to exclude
exclude_nodes:
  - node-ip-10-50-80-195
  - node-ip-10-50-81-231
  - nvidia-lepton093
  - nvidia-lepton007

# Container
container:
  image: "nvcr.io/nvidia/pytorch:25.11-py3"
  registry_auth: "lepton-nvidia-cvai-bnmo-trng"
