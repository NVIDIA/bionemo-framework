# Pilot configuration for LLAMA3 genomic training (WIP testing)
defaults:
  - defaults

# Slightly larger model for realistic testing (but still memory-safe)
model_layers: 8   # Double the sanity test layers (still 1/4 of full model)
hidden_size: 2048 # Keep reduced hidden size for memory safety

dataset:
  seq_length: 8192  # Full sequence length to test RoPE
  batch_size: 1     # Conservative for memory
  num_workers: 4    # More realistic data loading
  stride: 7992      # EVO2-style: 8192 - 200 = 7992 (200bp overlap)

# More realistic training run
num_train_steps: 200  # Enough to see learning trends
micro_batch_size: 1   # Conservative for memory

# Learning rate scheduler
lr_scheduler_kwargs:
  num_warmup_steps: 20  # 10% warmup
  num_training_steps: ${num_train_steps}

# Optimizer (slightly more conservative)
adamw_kwargs:
  lr: 2.0e-4  # Reduced from 4e-4 for stability
  fused: true
  betas: [0.9, 0.98]
  eps: 1.0e-8
  weight_decay: 0.1

# Wandb config for pilot testing
wandb_init_args:
  project: "llama3-genomic-pretraining"  
  name: "l1-pilot-8layers-8k-context"
  notes: "Pilot test: 8-layer LLAMA3 with 8K context, ASCII tokenization, dynamic RoPE"
  tags: ["llama3", "genomic", "pilot", "ascii-tokenizer", "dynamic-rope", "wip"]
  mode: "online"
