defaults:
  - defaults
  - _self_

# Training config
model_tag: nvidia/esm2_t6_8M_UR50D
num_train_steps: 250

dataset:
  micro_batch_size: 1
  max_seq_length: 512 # if you have CP=2

# WandB config
wandb_init_args:
  name: "esm2_t6_8M_UR50D_mfsdp_sanity"
  mode: "offline"

# Learning rate scheduler config
lr_scheduler_kwargs:
  num_warmup_steps: 100


checkpoint:
  ckpt_dir: "checkpoints/esm2_t6_8M_UR50D_sanity"
  resume_from_checkpoint: false
  use_distributed_checkpoint_fsdp2: true
  save_every_n_steps: 100000

context_parallel: 2
data_parallel: 1