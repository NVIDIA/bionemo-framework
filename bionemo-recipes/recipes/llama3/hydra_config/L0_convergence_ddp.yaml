# @package _global_

# Convergence test configuration for DDP with tiny Llama model (~10M params)
# Tests that the model can overfit on a small 200MB dataset
# Single GPU version

defaults:
  - defaults
  - _self_

# Use tiny Llama config for fast convergence testing
model_tag: /workspaces/bionemo-framework/bionemo-recipes/recipes/llama3/tiny_llama_config

# Training steps - enough to see convergence on small dataset
num_train_steps: 1000

# Dataset configuration - use 2MB subset
dataset:
  tokenizer_path: /workspaces/bionemo-framework/bionemo-recipes/models/llama3/nucleotide_fast_tokenizer
  micro_batch_size: 1  # Conservative for single GPU
  num_workers: 2
  max_seq_length: 8192  # Full Llama3 context length
  stride: 400  # 400bp overlap for 8K context
  buffer_size: 10_000  # Smaller buffer for faster iteration
  use_lazy_tokenization: true
  load_dataset_kwargs:
    path: "parquet"
    data_files: "/workspaces/bionemo-framework/data/genomic_sequences_2mb.parquet"
    split: "train"
    streaming: true  # Use streaming to avoid loading entire dataset into memory

# Optimizer - higher LR for faster convergence on small model
adamw_kwargs:
  lr: 5e-4  # Higher than default for faster convergence
  fused: true
  betas: [0.9, 0.98]
  eps: 1e-8
  weight_decay: 0.01

# Learning rate scheduler
lr_scheduler_kwargs:
  num_warmup_steps: 100  # Quick warmup (10% of training)
  num_training_steps: 1000

# Checkpoint configuration - disabled for fast convergence testing
checkpoint:
  ckpt_dir: null  # No checkpoints
  save_final_model: false  # Don't save final model
  resume_from_checkpoint: false  # Start fresh for convergence test
  save_every_n_steps: null  # No intermediate checkpoints

# Logging - frequent logging to track convergence
logger:
  frequency: 10  # Log every 10 steps

# WandB configuration
wandb_init_args:
  project: "llama3-genomic-convergence"
  name: "tiny-llama-ddp-convergence-test"
  mode: "online"  # Online mode for real-time dashboard
  tags:
    - convergence-test
    - ddp
    - tiny-model
    - 10M-params
    - single-gpu
    - 8192-context

# Meta device and torch compile
use_meta_device: false
use_torch_compile: false  # Disable for debugging

# FP8 configuration - disabled for convergence testing
fp8_config:
  enabled: false
  fp8_recipe: transformer_engine.common.recipe.DelayedScaling
  fp8_format: "HYBRID"
  fp8_recipe_kwargs: {}
  fp8_model_init_kwargs:
    enabled: false

