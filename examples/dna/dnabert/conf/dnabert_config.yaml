defaults:
  - dnabert_base_config
trainer:
  devices: 1
  val_check_interval: 100
  max_steps: 100000 # consumed_samples = global_step * micro_batch_size * data_parallel_size * accumulate_grad_batches
model:
  num_layers: 12
  micro_batch_size: 4
  pre_process: True # add embedding
  post_process: True # add pooler
  bert_binary_head: False # BERT binary head
  tokenizer:
    type: 'kmer'
    k: 3
    model: ${oc.env:BIONEMO_HOME}/tokenizers/dna/dnabert/vocab/dnabert${.k}.model
    vocab_file: ${oc.env:BIONEMO_HOME}/tokenizers/dna/dnabert/vocab/dnabert${.k}.vocab
  data:
    dataset_path: ${oc.env:BIONEMO_HOME}/data/GRCh38.p13 # contains train / val / test folders
    dataset: # inclusive range of data files to load or can load a single file, e.g. x000.csv
      train: chr[1..19].fna.gz.chunked.fa
      test: chr21.fna.gz.chunked.fa
      val: chr20.fna.gz.chunked.fa
    micro_batch_size: ${model.micro_batch_size}
    num_workers: 24
    dataloader_type: single
    dataset_format: fasta
