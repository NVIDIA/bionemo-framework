defaults:
  - pretrain_base_canonicalized  # override the model.data section of pretrain_base_equivariant
  - _self_

# Large model
trainer:
  devices: 8
  num_nodes: 8

model: 
  name: MolMIM-large
  
  # model architecture
  encoder:
    num_layers: 8
    num_attention_heads: 16
    hidden_size: 1024
