defaults:
  - pretrain_base

# Extra small model with augmentation of the encoder SMILES and masking of decoder tokens
trainer:
  devices: 1
  num_nodes: 1

model:
  name: xsmall_span_aug
  # model architecture
  num_layers: 2
  hidden_size: 256
  num_attention_heads: 4
  dwnstr_task_validation:
    enabled: True
    dataset:
      class: SingleValuePredictionCallback
      task_type: regression
      infer_target: bionemo.model.molecule.megamolbart.infer.MegaMolBARTInference
      max_seq_length: ${model.seq_length}
      emb_batch_size: 128
      batch_size: 128
      num_epochs: 10
      shuffle: True
      num_workers: 8
      dataset_path: /data/physchem/SAMPL
      dataset:
        train: x000
        test: x000
      sequence_column: 'smiles'
      target_column: 'expt'
      random_seed: 1234
      optim:
          name: adam
          lr: 0.0001
          betas:
            - 0.9
            - 0.999
          eps: 1e-8
          weight_decay: 0.01
          sched:
            name: WarmupAnnealing
            min_lr: 0.00001
            last_epoch: -1
            warmup_ratio: 0.01
            max_steps: 1000