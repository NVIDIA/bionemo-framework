name: esm1nv_sec_str
defaults:
  - pretrain_small
do_training: True # set to false if data preprocessing steps must be completed
do_testing: True # set to true to run evaluation on test data after training
restore_from_path: /model/protein/esm1nv/esm1nv.nemo 
target: bionemo.model.protein.esm1nv.ESM1nvModel
infer_target: bionemo.model.protein.esm1nv.infer.ESM1nvInference

trainer:
  devices: 2 # number of GPUs or CPUs
  num_nodes: 1 
  max_epochs: 10 # # use max_steps instead with NeMo Megatron model
  max_steps: 1000 # consumed_samples = global_step * micro_batch_size * data_parallel_size * accumulate_grad_batches
  val_check_interval: 20
  limit_val_batches: 1000 # number of batches in validation step, use fraction for fraction of data
  limit_test_batches: 1000 # number of batches in test step, use fraction for fraction of data

exp_manager:
  wandb_logger_kwargs:
    project: ${name}_finetuning
    name: ${name}_finetuning_encoder_frozen_${model.encoder_frozen}

model:
  encoder_frozen: True
  post_process: False
  micro_batch_size: 128 # NOTE: adjust to occupy ~ 90% of GPU memory
  global_batch_size: null
  tensor_model_parallel_size: 1  # model parallelism
  
  data:
    emb_batch_size: ${model.micro_batch_size}
    train_ds: 
      data_file: /data/netsurfp_2.0/train/x000.csv 
    validation_ds: 
      data_file: /data/netsurfp_2.0/val/x000.csv  
    test_ds: 
      data_file: /data/netsurfp_2.0/test/new_pisces.csv
    num_workers: 0
  
  finetuning_optim:
    name: adam
    lr: 0.0001
    betas:
      - 0.9
      - 0.999
    eps: 1e-8
    weight_decay: 0.01
    sched:
      name: WarmupAnnealing
      min_lr: 0.00001
      last_epoch: -1
      warmup_steps: 100
      max_steps: 1000
