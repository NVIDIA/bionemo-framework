##
## A faithful reproduction of the ESM-2 architecture and training parameters originally developed by META-ai
## using NVIDIA NeMo building blocks. Original model https://github.com/facebookresearch/esm
##

defaults:
  - base_config
restore_from_path: null # used when starting from a .nemo file

name: ESM2nv-8M

trainer:
  max_steps: 500000
  gradient_clip_val: 0

use_cpu_initialization: False

model:
  precision: 16
  # ESM2-specific parameters
  seq_length: 1024
  num_layers: 6
  hidden_size: 320
  ffn_hidden_size: 1280 # Transformer FFN hidden size. Usually 4 * hidden_size.
  num_attention_heads: 20
  megatron_legacy: True
  position_embedding_type: rope # ESM2 uses relative positional encoding 'ROPE' to extrapolate to longer sequences unseen during training
  hidden_dropout: 0 # ESM2 removes dropout from hidden layers and attention
  embedding_use_attention_mask: True # ESM2 uses attention masking on the embeddings
  embedding_token_dropout: True # ESM2 rescales embeddings based on masked tokens
  mask_token_id: ${.tokenizer.mask_id} # Needed for token dropout rescaling
  attention_dropout: 0.0 # ESM2 does not use attention dropout
  use_esm_attention: True # Use specific attention modifications for ESM2
  normalize_attention_scores: False # ESM2 does not use normalized attention scores
  tensor_model_parallel_size: 1 # model parallelism
  pipeline_model_parallel_size: 1 # model parallelism
  esm_gelu: True # ESM2 uses custom gelu in the ML layer
  bias_gelu_fusion: False
  use_pt_layernorm: True # Use pytorch implementation of layernorm instead of fused nemo layernorm. Important for equivalency of results with ESM2.
  use_pt_mlp_out: True # Use pytorch implementation of attention output mlp instead of the nemo version. Important for equivalency of results with ESM2.

  optim:
    lr: 4e-4
    weight_decay: 0.01
    betas:
      - 0.9
      - 0.98
    sched:
      name: CosineAnnealing
      warmup_steps: 2000
      constant_steps: 50000
      min_lr: 4e-5

  # These parameters are similar to pretrain_ESM1b_small
  tokenizer: # Use ESM2 tokenizers from HF
    library: 'huggingface'
    type: 'BertWordPieceLowerCase'
    model_name: "facebook/esm2_t6_8M_UR50D"
    mask_id: 32
    model: null
    vocab_file: null
    merge_file: null
  data:
    dataset_path: /data/uniref2022_05 # parent directory for data, contains train / val / test folders. Needs to be writeable for index creation.
    dataset: # inclusive range of data files to load x[000..049] or can a single file, e.g. x000
      train: x[000..049]
      test: x[000..049]
      val: x[000..049]
    micro_batch_size: 32
    global_batch_size: null
    num_workers: 10
    data_impl_kwargs:
      csv_mmap:
        data_col: 3 # 0-based
    modify_percent: 0.1 # Percentage of characters in a protein sequence to modify. (Modification means replacing with another amino acid or with a mask token)
    perturb_percent: 0.5 # Of the modify_percent, what percentage of characters are to be replaced with another amino acid.
  dwnstr_task_validation:
    enabled: True
