defaults:
  - base_config
  - _self_

do_preprocess: false
do_training: true
do_validation: false

model:
  stage: initial_training
  seed: 44
  micro_batch_size: 1
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  resume_from_checkpoint: null
  num_train_iters: 80000
  metrics:
    # available metrics are: lddt_ca, drmsd_ca, alignment_rmsd, gdt_ts, gdt_ha
    - lddt_ca

  optim:
    name: adam
    lr: 1e-3
    eps: 1e-6
    sched:
      name: AlphaFoldLRScheduler
      base_lr: 0.
      max_lr: 0.001
      warmup_no_steps: 1000
      start_decay_after_n_steps: 50000
      decay_every_n_steps: 50000
      decay_factor: 0.05

  data:
    dataset_path: ${oc.env:BIONEMO_HOME}/data/
    dataset_variant: processed
    realign_when_required: True
    prepare:
      # Downloading-only will create the following structure:

      # data
      # ├── open_protein_set
      # │   └── original  # 1.1 TiB (~1.3M files)
      # │       ├── LICENSE
      # │       ├── duplicate_pdb_chains.txt
      # │       ├── pdb/
      # │       └── uniclust30/
      # └── pdb_mmcif
      #     └── original  # +55 GiB (+200k files)
      #         ├── clusters-by-entity-40.txt
      #         ├── obsolete.dat
      #         └── raw/

      # After processing, the structure is going to look like this:

      # data
      # ├── open_protein_set
      # │   ├── original  # unchanged
      # │   └── processed  # 1.1 TiB (33 files)
      # │       └── pdb_alignments/
      # │       └── uniclust30_alignments/
      # │       └── uniclust30_targets/
      # └── pdb_mmcif
      #     ├── original  # unchanged
      #     └── processed  # +15 GiB (~1k files)
      #         ├── chains.csv
      #         ├── dicts/
      #         ├── dicts_preprocessing_logs.csv
      #         └── obsolete.dat


      # Please refer to PDBMMCIFPreprocess class of
      # bionemo.data.preprocess.protein.pdb_mmcif
      pdb_mmcif:
        mmcif_ref: rsync.rcsb.org::ftp_data/structures/divided/mmCIF/
        pdb_clusters_by_entity_ref: https://cdn.rcsb.org/resources/sequence/clusters/clusters-by-entity-40.txt
        pdb_obolete_ref: ftp://ftp.wwpdb.org/pub/pdb/data/status/obsolete.dat
        force: false

      # Control preparation stages of PDB mmcifs
      pdb_mmcif_actions:
        download: true # dataset will be downloaded inside model.data.dataset_path
        preprocess: true

      # Please refer to OpenProteinSetPreprocess class of
      # bionemo.data.preprocess.protein.open_protein_set
      open_protein_set:
        num_shards: 10
        force: false

      # Control preparation stages of OpenProteinSet
      open_protein_set_actions:
        download: true
        preprocess_pdb_alignments: true
        preprocess_uniclust30_alignments: true
        preprocess_uniclust30_targets: true

      create_sample: False

      sample:
        num_shards: 3
        output_root_path: null
        sample_variant: processed_sample
        source_variant: ${model.data.dataset_variant}
        force: False

      sample_pdb_chain_ids: null # list of pdb ids that are part of training data and should be included in training sample


    use_only_pdb_chain_ids: null # Optional list of pdb chain ids for intersection with train and val datasets.
    filter_by_alignments: true

  train_ds:
    train_max_pdb_release_date: "2021-09-16"
    num_workers: 14
    realign_when_required: ${model.data.realign_when_required}
    threading_enabled: False # TODO: explain

  validation_ds:
    val_min_cameo_submission_date: "2021-09-17"
    val_max_cameo_submission_date: "2021-12-11"
    num_workers: 2
    val_max_sequence_length: 700
    realign_when_required: ${model.data.realign_when_required}

  # List of optimisations to be applied to OpenFold (initial-training only).
  # The following optimisations are available:
  #      - mha_fused_gemm (Fused HEMM in Multi-Headed-Attention)
  #      - mha_triton (Tritonimplementation of  Multi-Headed-Attention)
  #      - layernorm_triton  (takes precedence before layernorm_inductor if both enabled)
  #      - layernorm_inductor
  #      - inductor_global (jitting turned on globally for all modules)
  #      - dataloader_pq (DataLoader with a priority queue)
  optimisations: []

trainer:
  # the effective batch size should be 128
  devices: 1
  accelerator : gpu
  precision: 32 # has been only trained with 32 precision
  num_nodes: 1
  max_steps : 80_000 # pre-training default as in the paper
  max_epochs: 1 # step-based training
  val_check_interval: 200
  logger: False
  enable_checkpointing: False # checkpointing is done by NeMo exp_manager
  gradient_clip_val: 0.1
  use_distributed_sampler: False

exp_manager:
  name: initial-training
  exp_dir: ${oc.env:BIONEMO_HOME}/results/nemo_experiments/${.name}/${.wandb_logger_kwargs.name}
  explicit_log_dir: ${.exp_dir}
  version: base
  resume_if_exists: True
  resume_ignore_no_checkpoint: True
  checkpoint_callback_params:
    always_save_nemo: True
    save_top_k : 2
    monitor: val_lddt_ca
    mode: max
    # filename 
    #   - As of 2024-03-16, the variable multisessionstep is managed within
    #   the AlphaFold class, since behavior changed as a result of NeMo upgrade,
    #   and the value of step written to checkpoint filename did not reflect
    #   progress through the training dataset
    filename: 'openfold--{multisessionstep}--{step}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}'
  create_wandb_logger: False
  wandb_logger_kwargs:
    offline: False # set to True if there are issues uploading to WandB during training
    project: ${name}-initial-training
    name: ${name}-initial-training
    group: ${name}
    notes: "date: ${now:%y%m%d-%H%M%S}"
    job_type: Localhost_nodes_${trainer.num_nodes}_gpus_${trainer.devices}
    tags:
      - ${name}


  ema:
    enable: True