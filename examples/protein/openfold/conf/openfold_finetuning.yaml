defaults:
  - base_config
  - _self_

do_training: true

# User must provide a checkpoint which will be fine-tuned. restore_from_path is intended to load .nemo checkpoint
# Alternatively, user can provide `torch_restore` which points to .pt checkpoint.
restore_from_path: null

model:
  stage: finetuning
  precision: tf32
  seed: 44
  micro_batch_size: 1
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  resume_from_checkpoint: null
  num_steps_in_one_epoch: 12000
  metrics:
  - lddt_ca

  # override from basic config
  train_sequence_crop_size: 384
  max_msa_clusters: 508
  max_extra_msa: 5120

  loss_config:
    experimentally_resolved_loss_config:
      weight: 0.01
    violation_loss_config:
      weight: 1.0

  extra_msa_stack_config:
    chunk_size_msa_att: 1024
    chunk_size_opm: 128

  optim:
    name: fused_adam
    lr: 5e-4
    eps: 1e-6
    sched:
      name: AlphaFoldLRScheduler
      base_lr: 0.
      max_lr: 0.0005
      warmup_no_steps: 1
      start_decay_after_n_steps: 8000
      decay_every_n_steps: 8000
      decay_factor: 0.95

  data:
    dataset_path: ${oc.env:BIONEMO_HOME}/examples/tests/test_data/openfold_data
    dataset_variant: processed
    realign_when_required: True
    use_only_pdb_chain_ids: null # Optional list of pdb chain ids for intersection with train and val datasets.
    filter_by_alignments: True # Whether to filter out mmcif chains with no alignments.

    prepare:
      create_sample: False
      sample:
        num_shards: 3
        output_root_path: null
        sample_variant: processed_sample
        source_variant: ${model.data.dataset_variant}
        force: False

      # use default lists of ids if not provided
      sample_pdb_chain_ids: null # list of pdb ids that are part of training data and should be included in training sample
      sample_cameo_chain_ids: null  # list of pdb ids that are part of validation data and should be included in validation sample
      sample_uniclust30_ids: null # list of uniclust30 ids that are part of fine-tuning data and should be included in training sample

  train_ds:
    train_max_pdb_release_date: "2021-09-16"
    num_workers: 14
    realign_when_required: ${model.data.realign_when_required}

  validation_ds:
    val_min_cameo_submission_date: "2021-09-17"
    val_max_cameo_submission_date: "2021-12-11"
    num_workers: 2
    val_max_sequence_length: 700
    realign_when_required: ${model.data.realign_when_required}

  # Stop the application when close to the slurm-job-end-time
  #   - When active, this functionality operates within the model class methods.
  #   - To activate this functionality, uncomment the two lines below, where
  #   the format for the time is HH:MM:SS
  #
  # do_stop_when_close_to_slurm_job_end: True
  # buffer_before_slurm_job_end_time: 00:05:00

trainer:
  # the effective batch size should be 128
  devices: 1
  accelerator : gpu
  precision: 32 # default precision
  num_nodes: 1
  max_steps : 12000
  max_epochs: 1 # step-based training
  val_check_interval: 100
  logger: False
  enable_checkpointing: False # checkpointing is done by NeMo exp_manager
  gradient_clip_val: 0.1
  use_distributed_sampler: False

exp_manager:
  name: fine-tuning
  exp_dir: ${oc.env:BIONEMO_HOME}/results/nemo_experiments/${.name}/${.wandb_logger_kwargs.name}
  explicit_log_dir: ${.exp_dir}
  version: base
  resume_if_exists: True
  resume_ignore_no_checkpoint: True
  checkpoint_callback_params:
    always_save_nemo: True
    save_top_k : 2
    monitor: val_lddt_ca
    mode: max
    # filename
    #   - As of 2024-03-16, the variable multisessionstep is managed within
    #   the AlphaFold class, since behavior changed as a result of NeMo upgrade,
    #   and the value of step written to checkpoint filename did not reflect
    #   progress through the training dataset
    filename: 'openfold--{multisessionstep}--{step}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}'
  create_wandb_logger: False
  wandb_logger_kwargs:
    offline: False # set to True if there are issues uploading to WandB during training
    project: ${name}-fine-tuning
    name: ${name}-fine-tuning
    group: ${name}
    notes: "date: ${now:%y%m%d-%H%M%S}"
    job_type: Localhost_nodes_${trainer.num_nodes}_gpus_${trainer.devices}
    tags:
      - ${name}


  ema:
    enable: True
