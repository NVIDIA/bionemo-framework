{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa0f8fb1",
   "metadata": {},
   "source": [
    "# Inference Sample for ESM2 - Logit Sampling Scan\n",
    "\n",
    "## Modified for logit/proba extraction\n",
    "Adrian Lange, A-Alpha Bio\n",
    "\n",
    "---------------\n",
    "\n",
    "SPDX-FileCopyrightText: Copyright (c) <year> NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "SPDX-License-Identifier: LicenseRef-NvidiaProprietary\n",
    "\n",
    "NVIDIA CORPORATION, its affiliates and licensors retain all intellectual property and proprietary rights in and to this material, related documentation and any modifications thereto. Any use, reproduction, disclosure or distribution of this material and related documentation without an express license agreement from NVIDIA CORPORATION or its affiliates is strictly prohibited.\n",
    "\n",
    "### Prerequisite\n",
    "\n",
    "- Linux OS\n",
    "- Pascal, Volta, Turing, or an NVIDIA Ampere architecture-based GPU.\n",
    "- NVIDIA Driver\n",
    "- Docker\n",
    "\n",
    "#### Import\n",
    "\n",
    "Components for inferencing are part of the BioNeMo ESM source code. This notebook demonstrates the use of these components.\n",
    "\n",
    "__`ESMInferenceWrapper`__ implements __`seq_to_embedding`__ function to obtain encoder embeddings for the input protein sequence in text format. \n",
    "\n",
    "\n",
    "To run this notebook, you should launch the gRPC client in your terminal beforehand. \n",
    "\n",
    "The following command is an example of launching the gRPC inference client using the esm2-650M checkpoints:\n",
    "\n",
    "```python3 -m bionemo.model.protein.esm1nv.grpc.service --model esm2nv_650M```\n",
    "\n",
    "Note that gRPC limits request size to 4MB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0e839b6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca006c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# For an optional visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a1a80ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model configuration at: /workspace/bionemo/my_workspace/esm2nv_logits/conf\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "try:\n",
    "    BIONEMO_HOME: Path = Path(os.environ['BIONEMO_HOME']).absolute()\n",
    "except KeyError:\n",
    "    print(\"Must have BIONEMO_HOME set in the environment! See docs for instructions.\")\n",
    "    raise\n",
    "\n",
    "config_path = BIONEMO_HOME / \"my_workspace\" / \"esm2nv_logits\" / \"conf\"\n",
    "print(f\"Using model configuration at: {config_path}\")\n",
    "assert config_path.is_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea31005a",
   "metadata": {},
   "source": [
    "### Setup and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4300adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = [\n",
    "    'MSLKRKNIALIPAAGIGVRFGADKPKQYVEIGSKTVLEHVL', \n",
    "    'MIQSQINRNIRLDLADAILLSKAKKDLSFAEIADGTGLA',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2903b8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rdkit:Enabling RDKit 2023.09.1 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "from bionemo.triton.utils import load_model_config\n",
    "\n",
    "cfg = load_model_config(config_path, config_name=\"infer.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcf459cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-04-15 20:50:43 utils:426] pytorch DDP is not initialized. Initializing with pytorch-lightening...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-04-15 20:50:43 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "`CUDAAccelerator` can not run on your system since the accelerator is not available. The following accelerator(s) is available and can be passed into `accelerator` argument of `Trainer`: ['cpu'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbionemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtriton\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model_for_inference\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbionemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotein\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mesm1nv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ESM1nvInference\n\u001b[0;32m----> 4\u001b[0m inferer \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_for_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minteractive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(inferer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inferer, ESM1nvInference)\n",
      "File \u001b[0;32m/workspace/bionemo/bionemo/triton/utils.py:183\u001b[0m, in \u001b[0;36mload_model_for_inference\u001b[0;34m(cfg, interactive)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting configuration to have an `infer_target` attribute. Invalid config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    181\u001b[0m infer_class \u001b[38;5;241m=\u001b[39m import_class_by_path(cfg\u001b[38;5;241m.\u001b[39minfer_target)\n\u001b[0;32m--> 183\u001b[0m \u001b[43minitialize_distributed_parallel_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor_model_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_model_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_model_parallel_split_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43minteractive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minteractive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m model \u001b[38;5;241m=\u001b[39m infer_class(cfg, interactive\u001b[38;5;241m=\u001b[39minteractive)\n\u001b[1;32m    192\u001b[0m model\u001b[38;5;241m.\u001b[39mfreeze()\n",
      "File \u001b[0;32m/workspace/bionemo/bionemo/model/utils.py:427\u001b[0m, in \u001b[0;36minitialize_distributed_parallel_state\u001b[0;34m(local_rank, tensor_model_parallel_size, pipeline_model_parallel_size, pipeline_model_parallel_split_rank, interactive)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mis_initialized():\n\u001b[1;32m    426\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch DDP is not initialized. Initializing with pytorch-lightening...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 427\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mddp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minteractive\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    430\u001b[0m         trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(_dummy, trainer\u001b[38;5;241m=\u001b[39mtrainer)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/argparse.py:348\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(env_variables\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py:420\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, logger, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, inference_mode)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# init connectors\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector \u001b[38;5;241m=\u001b[39m DataConnector(\u001b[38;5;28mself\u001b[39m, multiple_trainloader_mode)\n\u001b[0;32m--> 420\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_connector \u001b[38;5;241m=\u001b[39m \u001b[43mAcceleratorConnector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_processes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtpu_cores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpu_cores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mipus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mipus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync_batchnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_batchnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbenchmark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplace_sampler_ddp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace_sampler_ddp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_select_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_select_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamp_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamp_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamp_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamp_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplugins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplugins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector \u001b[38;5;241m=\u001b[39m LoggerConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector \u001b[38;5;241m=\u001b[39m CallbackConnector(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:204\u001b[0m, in \u001b[0;36mAcceleratorConnector.__init__\u001b[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, amp_type, amp_level, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, auto_select_gpus, num_processes, tpu_cores, ipus, gpus)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_choose_gpu_accelerator_backend()\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_parallel_devices_and_init_accelerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# 3. Instantiate ClusterEnvironment\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_environment: ClusterEnvironment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_choose_and_init_cluster_environment()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:565\u001b[0m, in \u001b[0;36mAcceleratorConnector._set_parallel_devices_and_init_accelerator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m accelerator_cls\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m    560\u001b[0m     available_accelerator \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    561\u001b[0m         acc_str\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m acc_str \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_types\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m AcceleratorRegistry[acc_str][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccelerator\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mis_available()\n\u001b[1;32m    564\u001b[0m     ]\n\u001b[0;32m--> 565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccelerator_cls\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` can not run on your system\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m since the accelerator is not available. The following accelerator(s)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is available and can be passed into `accelerator` argument of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `Trainer`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_accelerator\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m     )\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_devices_flag_if_auto_passed()\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_devices_flag \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gpus \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gpus\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: `CUDAAccelerator` can not run on your system since the accelerator is not available. The following accelerator(s) is available and can be passed into `accelerator` argument of `Trainer`: ['cpu']."
     ]
    }
   ],
   "source": [
    "from bionemo.triton.utils import load_model_for_inference\n",
    "from bionemo.model.protein.esm1nv.infer import ESM1nvInference\n",
    "\n",
    "inferer = load_model_for_inference(cfg, interactive=True)\n",
    "\n",
    "print(f\"Loaded a {type(inferer)}\")\n",
    "assert isinstance(inferer, ESM1nvInference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9d9e42",
   "metadata": {},
   "source": [
    "### Turn off post_process\n",
    "\n",
    "After loading, we switch off post processing so that the inferer still returns hidden states like we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ce79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferer.model.model.post_process = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ca48be",
   "metadata": {},
   "source": [
    "### Sequence to Hidden States\n",
    "\n",
    "__`seq_to_hiddens`__ queries the model to fetch the encoder hiddens states for the input protein sequence. `pad_mask` is returned with `hidden_states` and contains padding information  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73be827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states, pad_masks = inferer.seq_to_hiddens(seqs)\n",
    "print(f\"{hidden_states.shape=}\")\n",
    "print(f\"{pad_masks.shape=}\")\n",
    "assert tuple(hidden_states.shape) == (2, 43, 2560)  # ESM2nv has 2560 dimensions\n",
    "assert tuple(pad_masks.shape) == (2, 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ed347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check pad_masks\n",
    "pad_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7bb4cb",
   "metadata": {},
   "source": [
    "### Language model head\n",
    "\n",
    "Helpers for working with the ESM2nv BERT LM head.\n",
    "\n",
    "The tokenizer has classpath: `nemo.collections.common.tokenizers.huggingface.auto_tokenizer.AutoTokenizer`\n",
    "\n",
    "The source code for this should be findable on the BioNeMo Docker image at:\n",
    "```\n",
    "/usr/local/lib/python3.10/dist-packages/nemo/collections/common/tokenizers/huggingface/auto_tokenizer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08df7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check we have the lm_head\n",
    "inferer.model.model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0596ce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check vocabulary\n",
    "inferer.tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a329cde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_states_to_logits(hidden_states):\n",
    "    \"\"\"Take hidden_states from inferer.seq_to_hiddens(seqs) and apply the LM head.\n",
    "    \n",
    "    hidden_states.shape: <n_batch, n_max_seq_length_for_batch, 2560 hidden state dimensions>\n",
    "    \n",
    "    The logit shape has a fixed length of 128 tokens, however, ESM2nv only uses a vocabulary of 33.\n",
    "    So, we pare down to only the relevant vocabulary classes.\n",
    "    \"\"\"\n",
    "    with autocast(enabled=inferer.model.enable_autocast):\n",
    "        lm_out = inferer.model.model.lm_head(hidden_states, inferer.model.model.word_embeddings_weight())\n",
    "    logits = lm_out[:, :, :inferer.tokenizer.vocab_size]\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b254bd7a",
   "metadata": {},
   "source": [
    "### Example 1a: Proba heatmap\n",
    "\n",
    "1. Get logits for a batch of seqs\n",
    "2. Pick a seq of interest\n",
    "3. Drop padding mask, which includes dropping SOS and EOS tokens too\n",
    "4. Apply softmax to convert logits to probabilities\n",
    "5. Make a fun heatmp viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8d5bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_logits = hidden_states_to_logits(hidden_states)\n",
    "\n",
    "i_seq_of_interest = 1\n",
    "logits = batch_logits[i_seq_of_interest]\n",
    "logits = logits[pad_masks[i_seq_of_interest]]\n",
    "\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a88ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "plt.matshow(probas.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a30f93",
   "metadata": {},
   "source": [
    "### Example 1b: Argmax\n",
    "\n",
    "1. Translate the logits/probas back into sequence space via argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7fc200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argmax and compare prediction/reconstruction to true/input\n",
    "pred_idx_list = np.argmax(probas, axis=-1).tolist()\n",
    "true_idx_list = inferer.tokenizer.text_to_ids(seqs[i_seq_of_interest])\n",
    "\n",
    "pred_seq = inferer.tokenizer.ids_to_text(pred_idx_list).replace(\" \", \"\")\n",
    "true_seq = inferer.tokenizer.ids_to_text(true_idx_list).replace(\" \", \"\")\n",
    "\n",
    "display(pred_seq)\n",
    "display(\n",
    "    \"\".join(\n",
    "        [\".\" if a == b else \"|\" for a, b in zip(pred_seq, true_seq)]\n",
    "    )\n",
    ")\n",
    "display(true_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ac7e07",
   "metadata": {},
   "source": [
    "I guess it does pretty perfect for Nvidia's little peptide examples! Easy when there's no masking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d331458d",
   "metadata": {},
   "source": [
    "### Example 1c: Compute log-proba of the sequence\n",
    "\n",
    "Given the reconstruction may not always be perfect, but we do know the exact input, we pluck out the probas of the input tokens.\n",
    "\n",
    "1. Extract the input probas\n",
    "2. Log transform\n",
    "3. Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d7dbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = np.arange(len(true_idx_list))\n",
    "cols = np.asarray(true_idx_list)\n",
    "seq_log_proba = np.log(probas[rows, cols]).sum()\n",
    "seq_log_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8338c6",
   "metadata": {},
   "source": [
    "### Example 2: Mask some individual positions\n",
    "\n",
    "1. Make some masks\n",
    "2. Run the whole shebang to get probas of the masked token(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c2b1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK_TOKEN = \"<mask>\"\n",
    "\n",
    "seqs = [\n",
    "    \"MIQSQINRNIRLDLADAILLSKAKKDLSFAEIADGTGLA\",  # original unmasked sequence\n",
    "    \"MIQ\" + MASK_TOKEN + \"QINRNIRLDLADAILLSKAKKDLSFAEIADGTGLA\",\n",
    "    \"MIQSQINRNIRLDLADAILLSKAKKDLSFAEIADGT\" + MASK_TOKEN + \"LA\"\n",
    "]\n",
    "\n",
    "hidden_states, pad_masks = inferer.seq_to_hiddens(seqs)\n",
    "batch_logits = hidden_states_to_logits(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af988340",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_seq_of_interest = 1\n",
    "logits = batch_logits[i_seq_of_interest]\n",
    "logits = logits[pad_masks[i_seq_of_interest]]\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "probas[3]  # position of mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0471f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_sort = sorted(enumerate(probas[3]), key=lambda x: x[1], reverse=True)\n",
    "tmp = [(inferer.tokenizer.vocab[i], i, p) for i, p in tmp_sort]\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156a639b",
   "metadata": {},
   "source": [
    "Hmm, ranks the original amino acid (S) at 6-th most probable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50525d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_seq_of_interest = 2\n",
    "logits = batch_logits[i_seq_of_interest]\n",
    "logits = logits[pad_masks[i_seq_of_interest]]\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "probas[-3]  # position of mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9dadbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_sort = sorted(enumerate(probas[-3]), key=lambda x: x[1], reverse=True)\n",
    "tmp = [(inferer.tokenizer.vocab[i], i, p) for i, p in tmp_sort]\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e4de67",
   "metadata": {},
   "source": [
    "Seems to like the original amino acid (G) here as most probable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c2193d",
   "metadata": {},
   "source": [
    "### Example 3: In-paint a bunch of masked tokens all in one go\n",
    "\n",
    "1. Mask a short contiguous part of a seq\n",
    "2. Unmask/in-paint all at once by taking the argmax\n",
    "3. Unmask/in-paint by sampling probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573931ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK_TOKEN = \"<mask>\"\n",
    "\n",
    "before = \"M\"\n",
    "n_contig = 8\n",
    "contig = \"\".join([MASK_TOKEN] * n_contig)\n",
    "after = \"IRLDLADAILLSKAKKDLSFAEIADGTGLA\"\n",
    "\n",
    "seqs = [\n",
    "    \"MIQSQINRNIRLDLADAILLSKAKKDLSFAEIADGTGLA\",  # original unmasked sequence\n",
    "    before + contig + after\n",
    "]\n",
    "\n",
    "hidden_states, pad_masks = inferer.seq_to_hiddens(seqs)\n",
    "batch_logits = hidden_states_to_logits(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf07e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_seq_of_interest = 1\n",
    "logits = batch_logits[i_seq_of_interest]\n",
    "logits = logits[pad_masks[i_seq_of_interest]]\n",
    "probas = torch.softmax(logits, dim=-1).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f8b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# argmax in-paint: only taken over the masked contig\n",
    "\n",
    "pred_idx_list = np.argmax(probas[len(before):len(before) + n_contig, :], axis=-1).tolist()\n",
    "in_paint = inferer.tokenizer.ids_to_text(pred_idx_list).replace(\" \", \"\")\n",
    "pred_seq = before + in_paint + after\n",
    "\n",
    "orig_seq = seqs[0]\n",
    "\n",
    "display(pred_seq)\n",
    "display(\n",
    "    \"\".join(\n",
    "        [\".\" if a == b else \"|\" for a, b in zip(pred_seq, orig_seq)]\n",
    "    )\n",
    ")\n",
    "display(orig_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575bc340",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = []\n",
    "for p in probas[len(before):len(before) + n_contig, :]:\n",
    "    i = np.random.choice(np.arange(len(p)), p=p)\n",
    "    foo.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea504bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample in-paint: only taken over the masked contig\n",
    "\n",
    "np.random.seed(88888)\n",
    "\n",
    "pred_idx_list = []\n",
    "for p in probas[len(before):len(before) + n_contig, :]:\n",
    "    i = np.random.choice(np.arange(len(p)), p=p)\n",
    "    pred_idx_list.append(i)\n",
    "\n",
    "in_paint = inferer.tokenizer.ids_to_text(pred_idx_list).replace(\" \", \"\")\n",
    "pred_seq = before + in_paint + after\n",
    "\n",
    "orig_seq = seqs[0]\n",
    "\n",
    "display(pred_seq)\n",
    "display(\n",
    "    \"\".join(\n",
    "        [\".\" if a == b else \"|\" for a, b in zip(pred_seq, orig_seq)]\n",
    "    )\n",
    ")\n",
    "display(orig_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c83f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
