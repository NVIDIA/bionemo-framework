defaults:
  - base_config

name: prott5nv_model
do_training: True # set to false if data preprocessing steps must be completed
do_testing: False # set to true to run evaluation on test data after training, requires test_dataset section
restore_from_path: null # used when starting from a .nemo file

trainer:
  devices: 1
  num_nodes: 1
  accelerator: gpu
  precision: 16
  max_steps: 30000 # consumed_samples = global_step * micro_batch_size * data_parallel_size * accumulate_grad_batches
  val_check_interval: 50 
  limit_val_batches: 1000
  accumulate_grad_batches: 1

model:
  global_batch_size: 4
  micro_batch_size: 4
  data:
    data_url: 'https://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref50/uniref50.fasta.gz' 
    dataset_path: ${oc.env:PROJECT_MOUNT}/examples/tests/test_data/protein # parent directory for data, contains train / val / test folders
    dataset: # inclusive range of data files to load or can load a single file, e.g. x000.csv
      train: x000
      test: x000
      val: x000
    micro_batch_size: ${model.micro_batch_size}
    data_prefix: null
    data_col: 3 # 0-based
    data_sep: ','
    header_lines: 1
    num_workers: 24
  num_layers: 6
  
exp_manager:
  exp_dir: /tmp/nemo_experiments/prott5nv_pretrain
  create_wandb_logger: False
  create_tensorboard_logger: False
  wandb_logger_kwargs:
    offline: True
  resume_if_exists: False
