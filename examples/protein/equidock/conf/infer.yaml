do_training: False
do_testing: False
seed: 1204518

data:
  data_name: db5 # Train first over this dataset, [db5, dips] for now only train on db5
  num_workers: 0 # Should left 0 to avoid creating cuda content on a child thread
  pin_memory: True
  micro_batch_size: ${model.micro_batch_size}
  world_size: ${multiply:${trainer.devices}, ${trainer.num_nodes}}
  cache_path: ${oc.env:BIONEMO_HOME}/data/ # Folder from where to load/restore cached dataset
  data_dir: ${oc.env:BIONEMO_HOME}/tests/equidock_test_data/test_sets_pdb/${data.data_name}_test_random_transformed/random_transformed
  graph_cutoff: 30.0
  graph_max_neighbor: 10
  split: 0
  pocket_cutoff: 8.0
  translation_interval: 5
  n_jobs: 0

model:
  name: EquiDock_${data.data_name}_inference
  seed: 42
  target: bionemo.model.protein.equidock.equidock_model.EquiDock
  restore_from_path: ${oc.env:BIONEMO_HOME}/models/protein/equidock/equidock_${data.data_name}.nemo # used when starting from a .nemo file
  micro_batch_size: 32
  resume_from_checkpoint: null
  debug: False
  iegmn_n_lays: 5
  graph_nodes: residues
  rot_model: kb_att
  noise_decay_rate: 0.0
  noise_initial: 0.0
  use_edge_features_in_gmn: True
  use_mean_node_features: True
  residue_emb_dim: 64
  iegmn_lay_hid_dim: 64
  input_edge_feats_dim: null
  dropout: 0.0
  nonlin: lkyrelu # ['lkyrelu', 'swish']
  cross_msgs: True
  layer_norm: LN # ['LN', 'BN', default]
  layer_norm_coors: '0'
  final_h_layer_norm: '0'
  use_dist_in_layers: True
  skip_weight_h: 0.5
  x_connection_init: 0.0
  leakyrelu_neg_slope: 0.01
  shared_layers: True
  num_att_heads: 50
  fine_tune: false
  pocket_ot_loss_weight: 1.0
  intersection_loss_weight: 10.0
  intersection_sigma: 25.0
  intersection_surface_ct: 10.0
  divide_coors_dist: false
  graph_residue_loc_is_alphaC: True # needed for preprocessing
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1

  optim:
    name: adam # fused optimizers used by model
    lr: 0.0002 # max is scaled by scheduler
    betas:
      - 0.9
      - 0.999
    eps: 1e-8
    # weight_decay: 0.0001
    # sched:
    #   name: WarmupAnnealing
    #   min_lr: 0.00001
    #   last_epoch: -1
    #   warmup_steps: 10

  train_ds:
    micro_batch_size: ${model.micro_batch_size}
    pin_memory: ${data.pin_memory}
    seed: ${seed}
    data_dir: ${data.data_dir}
    cache_path: ${data.cache_path}
    num_workers: ${data.num_workers}
    shuffle: True
    data_fraction: 1.0
    drop_last: False
    data_name: ${data.data_name}
    graph_cutoff: ${data.graph_cutoff}
    graph_max_neighbor: ${data.graph_max_neighbor}
    split: ${data.split}
    pocket_cutoff: ${data.pocket_cutoff}
    translation_interval: ${data.translation_interval}
    graph_nodes: ${model.graph_nodes}
    n_jobs: ${data.n_jobs}

  validation_ds:
    micro_batch_size: ${model.micro_batch_size}
    pin_memory: ${data.pin_memory}
    seed: ${seed}
    data_dir: ${data.data_dir}
    cache_path: ${data.cache_path}
    num_workers: ${data.num_workers}
    shuffle: False
    data_fraction: null
    drop_last: False
    data_name: ${data.data_name}
    graph_cutoff: ${data.graph_cutoff}
    graph_max_neighbor: ${data.graph_max_neighbor}
    split: ${data.split}
    pocket_cutoff: ${data.pocket_cutoff}
    translation_interval: ${data.translation_interval}
    graph_nodes: ${model.graph_nodes}
    n_jobs: ${data.n_jobs}

  test_ds:
    micro_batch_size: ${model.micro_batch_size}
    pin_memory: ${data.pin_memory}
    seed: ${seed}
    data_dir: ${data.data_dir}
    cache_path: ${data.cache_path}
    num_workers: ${data.num_workers}
    shuffle: False
    data_fraction: null
    drop_last: False
    data_name: ${data.data_name}
    graph_cutoff: ${data.graph_cutoff}
    graph_max_neighbor: ${data.graph_max_neighbor}
    split: ${data.split}
    pocket_cutoff: ${data.pocket_cutoff}
    translation_interval: ${data.translation_interval}
    graph_nodes: ${model.graph_nodes}
    n_jobs: ${data.n_jobs}


trainer:
  devices: 2
  num_nodes: 1
  precision: 32 # To activate AMP set to 16 or 'bf16'; otherwise will be float32
  accelerator: gpu # gpu or cpu
  max_epochs: 1000 # set to null when using max_steps instead with NeMo model
  max_steps: -1
  log_every_n_steps: 1 # number of iterations between logging
  val_check_interval: 1.0 # set to integer when using steps to determine frequency of validation, use fraction with epochs
  num_sanity_val_steps: 1.0 # set to 0 or small number to test validation before training
  limit_val_batches: 1.0 # number of batches in validation step, use fraction for fraction of data
  limit_test_batches: 1.0 # number of batches in test step, use fraction for fraction of data
  limit_train_batches: 1.0
  gradient_clip_val: 100.0
  logger: False # logger is provided by NeMo exp_manager
  enable_checkpointing: False # checkpointing is done by NeMo exp_manager
  reload_dataloaders_every_n_epochs: 10 # Set to a non-negative integer to reload dataloaders every n epochs. Default: ``0``.
  accumulate_grad_batches: 1


exp_manager:
  name: ${model.name}_${trainer.devices}_${data.micro_batch_size}
  # checkpoint reloading and saving
  resume_if_exists: True # autmatically resume if checkpoint exists
  resume_ignore_no_checkpoint: True # leave as True, will start new training if resume_if_exists is True but no checkpoint exists
  create_checkpoint_callback: True # leave as True, use exp_manger for checkpoints

  checkpoint_callback_params:
    save_top_k: 1 # number of checkpoints to save
    monitor: val_loss # use loss to select best checkpoints
    mode: min # use min or max of monitored metric to select best checkpoints
    save_last: True # always save last checkpoint
    always_save_nemo: True # not implemented for model parallel, additionally save NeMo-style checkpoint during validation, set to False if checkpoint saving is time consuming
    filename: '${model.name}--{val_loss:.4f}-{step}' # -{consumed_samples}'
    save_best_model: True
    save_nemo_on_train_end: True

  # ********************************************************* #
  # Early stopping callback works with the latest nemo
  # create_early_stopping_callback:
  # early_stopping_callback_params:
  #   monitor: val_loss
  #   mode: 'min'
  #   patience: 50
  #   verbose: True

  # EMA
  # ema: # Exponential Moving Average; is picked up by exp_manager()
  #   enable: False # Creates EMA callback in exp_manager
  #   decay: 0.999 # (ema_rate) The exponential decay used when calculating the moving average. Has to be between 0-1.

  # logging
  exp_dir: ${oc.env:BIONEMO_HOME}/results/nemo_experiments/${.name}/  #${.wandb_logger_kwargs.name}
  explicit_log_dir: ${.exp_dir}
  create_tensorboard_logger: False
  create_wandb_logger: True
  wandb_logger_kwargs:
    project: ${model.name}_training
    name: ${model.name}_${trainer.devices}_${data.micro_batch_size}
    group: ${model.name}
    job_type: Localhost_nodes_${trainer.num_nodes}_gpus_${trainer.devices}
    notes: "date: ${now:%y%m%d-%H%M%S}"
    tags:
      -${model.name}
      -${data.data_name}
    offline: True # set to True if there are issues uploading to WandB during training
