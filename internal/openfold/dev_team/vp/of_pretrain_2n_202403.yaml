# @package _global_

# application-specific variables
#    vp --config-name examples/slurm/hello
#
# ~2000 steps in 4h
# ~250 steps in 30min
#
#
vp_run_tag_in: vp20240426T0928zP
results_subdir_in: vp_out_20240421
prod_level_in: dev
project_in: openfold-apr-por-initial-training
total_nodes_in: 2
ntasks_per_node_in: 8
val_check_interval_in: 4
num_steps_in_one_epoch_in: 20
max_steps_in: 100
max_epochs_in: 2
warmup_for_perf_logger_in: NA
walltime_in: 00:20:00
walltime_tag: 20m
do_stop_when_close_to_slurm_job_end_in: True
buffer_before_slurm_job_end_time_in: 00:05:00
do_open_loop_train_metrics_in: False
log_global_rank_0_only_in: True
resumable_in: True

# List of optimisations to be applied to OpenFold (initial-training only).
# The following optimisations are available:
#      - mha_fused_gemm (Fused HEMM in Multi-Headed-Attention)
#      - mha_triton (Tritonimplementation of  Multi-Headed-Attention)
#      - layernorm_triton  (takes precedence before layernorm_inductor if both enabled)
#      - layernorm_inductor
#      - inductor_global (jitting turned on globally for all modules)
#      - dataloader_pq (DataLoader with a priority queue)

opt_tag_in: "O"
opt_list_in: "[]"
precision_in: 32

#opt_tag_in: "A"
#opt_list_in: "[mha_fused_gemm,mha_triton,layernorm_triton,layernorm_inductor,inductor_global,dataloader_pq]"
#precision_in: 16


# ---- docker image path ----
# gitlab-master.nvidia.com: git-12051.nvidia.com (10.120.180.120)
# nvcr.io:52.9.33.149
#
docker_repo: nvcr.io/nvidian/cvai_bnmo_trng/bionemo
docker_tag: br_cd-3173_debug_dataloader_pq_20240426T0909
#docker_tag: br_cd-2888_fix_epoch_def_20240417T1431
#docker_tag: br_cd-2888_fix_epoch_def_20240412T2115
#docker_tag: br_cd-2888_fix_epoch_def_20240416T1151
#docker_tag: cycletrue_br_cd-2888_apr_por_head_20240415T1317
#docker_tag: cyclefalse_br_cd-2888_apr_por_head_20240415T1323
#docker_tag: br_cd-2888_apr_por_head_20240414T2230

#docker_tag: br_cd-2858_validation_metric_debug_step_from_filename_20240403T1552

defaults:
 - /cluster: ord2  
 - _self_

cluster:
 setup:
  echo 'of_pretrain_2n_202403.yaml - setup placeholder';
 cleanup:
  echo 'of_pretrain_2n_202403.yaml - cleanup placeholder';
 results_dir: /lustre/fsw/portfolios/healthcareeng/users/broland/${results_subdir_in}/${vp_run_tag_in}_${prod_level_in}_${docker_tag}_opt${opt_tag_in}_pr${precision_in}_n${total_nodes_in}_t${ntasks_per_node_in}_v${val_check_interval_in}_s${num_steps_in_one_epoch_in}_e${max_epochs_in}_m${max_steps_in}_w${walltime_tag}_wu${warmup_for_perf_logger_in}_res${resumable_in}
 docker_image: ${docker_repo}:${docker_tag}
 account: healthcareeng_bionemo
 partition: polar,polar2,polar3
 walltime: ${walltime_in}
 total_nodes: ${total_nodes_in}
 ntasks_per_node: ${ntasks_per_node_in}
 datasets:
    /data : /lustre/fsw/portfolios/convai/users/broland/openfold_from_tgrzegorzek_20240228
 project: openfold
 resumable: ${resumable_in}
 pre_script: [set -x, set -e]


run:
 command: >
  echo 'of_pretrain_2n_202403.yaml - before date';
  date;
  lscpu;
  nvidia-smi;
  echo 'of_pretrain_2n_202403.yaml - before install_third_party.sh';
  cd /workspace/bionemo;
  ./examples/protein/openfold/scripts/install_third_party.sh;
  echo 'of_pretrain_2n_202403.yaml - before train.py';  
  export HYDRA_FULL_ERROR=1;
  cd /workspace/bionemo;
  python examples/protein/openfold/train.py \\
    ++model.data.dataset_path=/data \\
    ++model.optimisations=${opt_list_in} \\
    ++model.num_steps_in_one_epoch=${num_steps_in_one_epoch_in} \\
    ++model.do_open_loop_train_metrics=${do_open_loop_train_metrics_in} \\
    ++model.do_stop_when_close_to_slurm_job_end=${do_stop_when_close_to_slurm_job_end_in} \\
    ++model.buffer_before_slurm_job_end_time=${buffer_before_slurm_job_end_time_in} \\
    ++trainer.num_nodes=${total_nodes_in} \\
    ++trainer.devices=${ntasks_per_node_in} \\
    ++trainer.precision=${precision_in} \\
    ++trainer.val_check_interval=${val_check_interval_in} \\
    ++trainer.max_steps=${max_steps_in} \\
    ++trainer.max_epochs=${max_epochs_in} \\
    ++exp_manager.exp_dir=/result \\
    ++exp_manager.log_global_rank_0_only=${log_global_rank_0_only_in} \\
    ++exp_manager.wandb_logger_kwargs.name=${vp_run_tag_in}_${prod_level_in}_${docker_tag}_opt${opt_tag_in}_pr${precision_in}_n${total_nodes_in}_t${ntasks_per_node_in}_v${val_check_interval_in}_s${num_steps_in_one_epoch_in}_e${max_epochs_in}_m${max_steps_in}_w${walltime_tag}_wu${warmup_for_perf_logger_in}_res${resumable_in} \\
    ++exp_manager.wandb_logger_kwargs.project=${project_in} \\
    ++exp_manager.create_wandb_logger=True;

  echo 'of_pretrain_2n_202403.yaml - after train.py';
  date;

 name: bionemo-openfold-initial-training-${prod_level_in}-${total_nodes_in}-${ntasks_per_node_in}


