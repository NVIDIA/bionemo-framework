type: recipe
format_version: 1
maintainers: [dorotat]
logger: dllogger
labels:
  origin: bionemo
  workload_ref: ""
key_segments:
  domain: domain
  dwnstr_task: dwnstrtask
  config_name: config
  warmup: warmup
  config_path: False
  default_overwrites: False
  extra_overwrites: False
  max_steps: max_steps
  val_check_interval: val_check
launchers:
  type:slurm:
    ntasks_per_node: '{gpus}'
  name:dgxa100_dracooci:
    mounts:
      /workspace/bionemo/models: /lustre/fsw/portfolios/convai/projects/convai_bionemo_training/jet/models
      /workspace/bionemo/examples/tests/test_data/cellxgene_2023-12-15_small: /lustre/fsw/portfolios/convai/projects/convai_bionemo_training/jet/data/test_data/cellxgene_2023-12-15_small
spec:
  build: bionemo
  scope: perf-train
  platforms: [linux/amd64]
  BIONEMO_HOME: "/workspace/bionemo"
  MODEL_PATH: "/workspace/bionemo/models"
  NGC_CLI_FORMAT_TYPE: ascii
  NGC_CLI_ORG: nvidian
  NGC_CLI_TEAM: clara-lifesciences
  config_path: "examples/tests/conf/"
  warmup: 0
  val_check_interval: 100
  max_steps: 500
  default_overwrites: "++exp_manager.create_wandb_logger=False ++exp_manager.create_tensorboard_logger=False ++exp_manager.exp_dir=/tmp/nemo_experiments/ ++exp_manager.create_checkpoint_callback=False ++exp_manager.resume_if_exists=False"
  extra_overwrites: ""
  script: |-
    cd {BIONEMO_HOME}
    export NGC_CLI_ORG={NGC_CLI_ORG}
    export NGC_CLI_TEAM={NGC_CLI_TEAM}
    export NGC_CLI_FORMAT_TYPE={NGC_CLI_FORMAT_TYPE}
    if [ "$SLURM_LOCALID" = "0" ]; then
      if [ "{model}" = "esm2nv" ]; then
        unzip examples/tests/test_data/uniref202104_esm2_qc_test200_val200.zip -d examples/tests/test_data/;
      fi
      touch .download-finished;
    else
      until [ -f .download-finished ]; do
        sleep 5;
      done
    fi
    python examples/{domain}/{model}/{variant}.py --config-path {BIONEMO_HOME}/{config_path} --config-name {config_name} \
    trainer.num_nodes={nodes} trainer.devices={gpus} trainer.precision={precision} model.micro_batch_size={batch_size} \
    trainer.max_steps={max_steps} trainer.val_check_interval={val_check_interval} \
    ++model.dwnstr_task_validation.enabled={dwnstr_task} \
    {default_overwrites} {extra_overwrites} hydra.searchpath=[file://{BIONEMO_HOME}/examples/{domain}/{model}/conf] \
    ++create_dllogger_callbacks=True ++create_trainer_metric_callback=True \
    ++dllogger_callbacks_kwargs.use_existing_dllogger=True ++dllogger_callbacks_kwargs.warmup={warmup} \
    ++dllogger_callbacks_kwargs.json_file={dllogger_file} ++trainer_metric_callback_kwargs.log_path={assets_dir} ++logs_dir={logs_dir}
  time_limit: 3600
  artifacts: {}
  metrics:
    throughput_train:
      goal: maximize
      tags: [primary]
    latency_train_mean:
      goal: minimize
      tags: [secondary]
    train_loss:
      goal: minimize
      tags: [secondary]
products:
  - nodes: [1]
    gpus: [8]
    precision: [32, bf16-mixed]
    products:
      - domain: [molecule]
        batch_size: [2]  # Small batch size because otherwise we run out of test validation samples for parallelism.
        products:
          - model: [megamolbart]
            products:
              - variant: [pretrain]
                dwnstr_task: [False, True]
                config_name: [megamolbart_test]
              - variant: [downstream_retro]
                dwnstr_task: [False]
                config_name: [megamolbart_downstream_retro_test]
              - variant: [downstream_physchem]
                dwnstr_task: [False]
                config_name: [megamolbart_physchem_test]
                extra_overwrites: ["++trainer.check_val_every_n_epoch=null"]
          - model: [molmim]
            products:
              - variant: [pretrain]
                products:
                  - config_name: [molmim_pretrain_xsmall_test]
                    dwnstr_task: [False, True]
                    extra_overwrites: ["++trainer.limit_train_batches=null"]
                  - config_name: [molmim_pretrain_continue_small_test]
                    dwnstr_task: [False]
                    extra_overwrites: ["++trainer.limit_train_batches=null"]
      - domain: [protein]
        dwnstr_task: [False, True]
        products:
          - model: [esm1nv]
            batch_size: [32]
            variant: [pretrain]
            config_name: [esm1nv_test]
          - model: [esm2nv]
            batch_size: [2]
            variant: [pretrain]
            config_name: [esm2nv_8M_test]
          - model: [prott5nv]
            batch_size: [12]
            variant: [pretrain]
            config_name: [prott5nv_test]
      - domain: [singlecell]
        dwnstr_task: [False]
        products:
          - model: [geneformer]
            batch_size: [8]
            variant: [pretrain]
            config_name: [geneformer_pretrain_test]
      - domain: [dna]
        dwnstr_task: [False]
        products:
          - model: [dnabert]
            batch_size: [2]
            variant: [pretrain]
            config_name: [dnabert_test]
