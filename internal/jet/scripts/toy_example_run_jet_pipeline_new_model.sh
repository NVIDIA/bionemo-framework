#!/bin/bash
#
# SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
#
# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
# property and proprietary rights in and to this material, related
# documentation and any modifications thereto. Any use, reproduction,
# disclosure or distribution of this material and related documentation
# without an express license agreement from NVIDIA CORPORATION or
# its affiliates is strictly prohibited.

####################### USAGE #############################################
### Requires installed JET API https://gitlab-master.nvidia.com/dl/jet/api but can be run outside docker
### To setup jet command line for the first time, add --setup_jet_api flag to the below command

TIMESTAMP="$(date +%s)"
JET_WORKLOADS_DIR="$(dirname "$PWD")"
JET_WORKLOADS_PATH="${JET_WORKLOADS_DIR}/${TIMESTAMP}"
JET_REF_EPH="ephemeral/bionemo/temp_${TIMESTAMP}"

echo $JET_WORKLOADS_PATH
echo $JET_REF_EPH

# For more info about arguments, check docs in internal/jet/run_jet_downstream_pipeline.py and internal/Readme
python internal/jet/run_jet_downstream_pipeline.py --jet_ref_eph ${JET_REF_EPH}  --jet_repo_path ${JET_WORKLOADS_PATH} \
  --model megamolbart --config_path examples/molecule/megamolbart/conf --config_name pretrain_xsmall_span_aug \
  --script_path examples/molecule/megamolbart --variant pretrain --nodes 1 --gpus 1 8 --batch_size 4 8 \
  --precision 16-mixed 32 --extra_overwrites "trainer.max_steps=200 ++trainer.check_val_every_n_epoch=null ++model.data.dataset_path=\$BIONEMO_HOME/examples/tests/test_data/molecule ++model.data.dataset.train=x000 ++model.data.dataset.val=x000 ++model.data.dataset.test=x000 ++model.dwnstr_task_validation.enabled=False"

#In the console one should see 8 tests generated by the above command
# CONSOLE:
#  key
#  ----------------------------------------------------------------------------------------------------------------
#  recipe/megamolbart_pretrain_bionemo_perf-train_16_nodes-1_gpus-1_bs-4_config-pretrain-xsmall-span-aug_warmup-100
#  recipe/megamolbart_pretrain_bionemo_perf-train_16_nodes-1_gpus-1_bs-8_config-pretrain-xsmall-span-aug_warmup-100
#  recipe/megamolbart_pretrain_bionemo_perf-train_16_nodes-1_gpus-8_bs-4_config-pretrain-xsmall-span-aug_warmup-100
#  recipe/megamolbart_pretrain_bionemo_perf-train_16_nodes-1_gpus-8_bs-8_config-pretrain-xsmall-span-aug_warmup-100
#  recipe/megamolbart_pretrain_bionemo_perf-train_32_nodes-1_gpus-1_bs-4_config-pretrain-xsmall-span-aug_warmup-100
#  recipe/megamolbart_pretrain_bionemo_perf-train_32_nodes-1_gpus-1_bs-8_config-pretrain-xsmall-span-aug_warmup-100
#  recipe/megamolbart_pretrain_bionemo_perf-train_32_nodes-1_gpus-8_bs-4_config-pretrain-xsmall-span-aug_warmup-100
#  recipe/megamolbart_pretrain_bionemo_perf-train_32_nodes-1_gpus-8_bs-8_config-pretrain-xsmall-span-aug_warmup-100

rm -rf $JET_WORKLOADS_PATH

# You can visit the pipline-id url link to JET CI (printed to the console) to track progress
# of your pipeline and check when it is finished.
# CONSOLE:
#  Created pipeline #9469458
#  https://gitlab-master.nvidia.com/dl/jet/ci/-/pipelines/9469458
#
# You can print or save the results by internal/jet/get_results_from_jet.py
# (check the scripts with implemented methods for details of the arguments)

# python internal/jet/get_results_from_jet.py --jet_workloads_ref "${JET_REF_EPH}" --print_script

# alternatively, you can provide pipeline id that is printed to the console after execution of the above command

# python internal/jet/get_results_from_jet.pyy --pipeline_id <PIPELINE_ID> --print_script
