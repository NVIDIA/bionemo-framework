# Build image, note the -devel tag which gives us nvcc and other build-time tools. In this image we'll install uv and
# create a python virtual environment containing all our dependencies and an installed version of our bionemo library.
# (Later, we can split that final step out into a separate image for an easier devcontainer setup.)
FROM nvcr.io/nvidia/cuda:12.1.0-cudnn8-devel-ubuntu22.04 AS pre-build

# Build-time dependencies. We install and link against the ubuntu-provided version of python rather than uv, since it's
# easier to get a consistent version of python in the runtime image without needing to install UV there as well.
RUN <<EOT
apt-get update -qy
apt-get install -qyy \
    -o APT::Install-Recommends=false \
    -o APT::Install-Suggests=false \
    build-essential \
    ca-certificates \
    curl \
    software-properties-common \
    git \
    ninja-build \
    cmake \
    openmpi-bin \
    libopenmpi-dev \

add-apt-repository ppa:deadsnakes/ppa
apt-get update -qy
apt-get install -qyy \
    python3.10 \
    python3.10-dev
EOT

COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv

# - Silence uv complaining about not being able to use hard links,
# - tell uv to byte-compile packages for faster application startups,
# - prevent uv from accidentally downloading isolated Python builds,
# - pick a Python,
# - and finally declare `/venv` as the target for `uv sync`.
ENV UV_LINK_MODE=copy \
    UV_COMPILE_BYTECODE=1 \
    UV_PYTHON=python3.10 \
    UV_PYTHON_DOWNLOADS=never \
    UV_PROJECT_ENVIRONMENT=/venv

COPY uv.lock /_lock/
COPY pyproject.toml /_lock/

# If we don't use a `package=` option, we only install top-level dependencies, i.e., we don't install everything we'll
# need recursively. Hopefully https://github.com/astral-sh/uv/issues/6935 will lead to a cleaner way of installing all
# non-package dependencies before having to copy in all our source code. Once we copy in our source code, we're
# essentially giving up on these dependency installs being cached between builds.
#
# We have to do this in two steps in order to ensure that `flash-attn` (and later, any other packages with build-time
# requirements) are installed after we have created a base environment with torch & numpy.
RUN --mount=type=cache,target=/root/.cache <<EOT
cd /_lock
uv sync \
    --frozen \
    --no-install-workspace \
    --no-dev -v \
    --package bionemo-meta

uv sync \
    --frozen \
    --no-install-workspace \
    --no-dev -v \
    --package bionemo-meta \
    --extra build
EOT

# TODO: figure out if & how we can include apex and transformer-engine in the uv build system. `uv pip install` works,
# but including them in the uv.lock file (with these build-time arguments) doesn't seem to work.
RUN --mount=type=cache,target=/root/.cache <<EOT
uv pip install --python=$UV_PROJECT_ENVIRONMENT --no-deps \
    git+https://github.com/NVIDIA/apex.git@810ffae374a2b9cb4b5c5e28eaeca7d7998fca0c \
    -v --no-build-isolation \
    --config-settings "\
    --build-option=--cpp_ext \
    --cuda_ext \
    --fast_layer_norm \
    --distributed_adam \
    --deprecated_fused_adam \
    --group_norm \
    "
EOT

# Transformer engine before 1.9 seems to have issues installing directly from the git url.
# RUN --mount=type=cache,target=/root/.cache <<EOT
# NVTE_FRAMEWORK=pytorch NVTE_WITH_USERBUFFERS=1 MPI_HOME=/usr/lib/x86_64-linux-gnu/openmpi/ \
# uv pip install --python=$UV_PROJECT_ENVIRONMENT --no-deps \
#     git+https://github.com/NVIDIA/TransformerEngine.git@v1.9 \
#     -v --no-build-isolation
# EOT
# Transformer Engine pre-1.7.0. 1.7 standardizes the meaning of bits in the attention mask to match
ARG TE_COMMIT=7d576ed25266a17a7b651f2c12e8498f67e0baea
RUN --mount=type=cache,target=/root/.cache <<EOT
cd /tmp/te
git clone https://github.com/NVIDIA/TransformerEngine.git
cd TransformerEngine
git fetch origin ${TE_COMMIT}
git checkout FETCH_HEAD
git submodule init && git submodule update
NVTE_FRAMEWORK=pytorch NVTE_WITH_USERBUFFERS=1 MPI_HOME=/usr/local/mpi \
uv pip --python=$UV_PROJECT_ENVIRONMENT --no-deps install  .
rm -rf /tmp/te
EOT

FROM pre-build AS build
# Just copy over the 3rdparty/ and sub-packages/ directories to limit the radius of file changes that trigger a cache
# miss. Then we install these (in a non-editable way) into the virtual environment without dependencies (since these
# were installed above).
COPY 3rdparty /src/3rdparty/
COPY sub-packages /src/sub-packages/

# Mount the local .git directory in /src/ so that setuptools-scm can use it to determine the version number.
# COPY . /src
# RUN <<EOT
RUN --mount=type=bind,source=./.git,target=/src/.git <<EOT
uv pip install --python=$UV_PROJECT_ENVIRONMENT --no-deps /src/3rdparty/* /src/sub-packages/bionemo-*
EOT


FROM nvcr.io/nvidia/cuda:12.1.0-cudnn8-runtime-ubuntu22.04 AS pre-release
# Note the -release tag, which gives us a smaller image without the build-time tools.

SHELL ["/bin/bash", "-xc"]

# Make sure the eventual virtualenv is in the runtim path.
ENV PATH=/venv/bin:$PATH \
    TORCH_ALLOW_TF32_CUBLAS_OVERRIDE=1 \
    CUDNN_V8_API_ENABLED=1

# Runtime dependencies only. We need python-dev and build-essential for torch.compile.
RUN <<EOT
apt-get update -qy
apt-get install -qyy \
    -o APT::Install-Recommends=false \
    -o APT::Install-Suggests=false \
    gosu \
    build-essential \
    software-properties-common \
    openmpi-bin

add-apt-repository ppa:deadsnakes/ppa
apt-get update -qy
apt-get install -qyy \
    python3.10 \
    python3.10-dev

apt-get clean
rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*
EOT

# Create a non-root user. We'll also want to do this in the devcontainer setup.
ARG USERNAME=bionemo
ARG USER_UID=1000
ARG USER_GID=$USER_UID
RUN <<EOT
groupadd --gid $USER_GID $USERNAME
useradd --uid $USER_UID --gid $USER_GID -m $USERNAME -d /home/bionemo -s /bin/bash
EOT

WORKDIR /home/bionemo

# Note that we don't use a USER command to switch to the non-root user, and instead use this entrypoint script that
# mocks the user's UID and GID and runs the provided command as `bionemo`.
COPY ci/docker/entrypoint.sh /usr/local/bin/
ENTRYPOINT ["entrypoint.sh"]

FROM pre-release AS dev
# In the development container, we don't want the virtual environment with our dependencies installed, since we want to
# let uv install them as editable packages from the workspace. We'll also need UV and the associated environment
# variables set as well.

RUN <<EOT
apt-get update -qy
apt-get install -qyy \
    git \
    bash-completion \

apt-get clean
rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*
EOT

COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv

ENV UV_LINK_MODE=copy \
    UV_COMPILE_BYTECODE=1 \
    UV_PYTHON=python3.10 \
    UV_PYTHON_DOWNLOADS=never \
    UV_PROJECT_ENVIRONMENT=/venv

COPY --from=pre-build --chown=$USERNAME:$USERNAME --chmod=777 /venv /venv

# This should be the final target in the dockerfile so that it's the default build target.
FROM pre-release AS release
# Copy the virtual environment from the build image; this is how we get the build libraries above into the runtime
# container.
COPY --from=build --chown=$USERNAME:$USERNAME /venv /venv
